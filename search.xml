<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【特征提取+分类模型】4种常见的NLP实践思路</title>
    <url>/2020/07/28/%E3%80%90%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96+%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E3%80%914%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84NLP%E5%AE%9E%E8%B7%B5%E6%80%9D%E8%B7%AF/</url>
    <content><![CDATA[<p>越来越多的人选择参加算法赛事，为了提升项目实践能力，同时也希望能拿到好的成绩增加履历的丰富度。期望如此美好，现实却是：看完赛题，一点思路都木有。那么，当我们拿到一个算法赛题后，如何破题，如何找到可能的解题思路呢。</p>
<p>本文针对NLP项目给出了4种常见的解题思路，其中包含1种基于机器学习的思路和3种基于深度学习的思路。</p>
<h1 id="一、数据及背景"><a href="#一、数据及背景" class="headerlink" title="一、数据及背景"></a><strong>一、数据及背景</strong></h1><p><a href="https://tianchi.aliyun.com/competition/entrance/531810/information%EF%BC%88%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP%E8%B5%9B%E4%BA%8B%EF%BC%89">https://tianchi.aliyun.com/competition/entrance/531810/information（阿里天池-零基础入门NLP赛事）</a></p>
<h1 id="二、数据下载及分析"><a href="#二、数据下载及分析" class="headerlink" title="二、数据下载及分析"></a><strong>二、数据下载及分析</strong></h1><h2 id="2-1-获取数据"><a href="#2-1-获取数据" class="headerlink" title="2.1 获取数据"></a>2.1 <strong>获取数据</strong></h2><p>我们直接打开数据下载地址，看到的是这样一个页面：</p>
<p><img src="G:\blog\public\2020\07\28\【特征提取+分类模型】4种常见的NLP实践思路\image\1.png" alt="1"></p>
<p>接着三步走：注册报名下载数据，查看数据前五行可以看到我们获得的数据如下：</p>
<img src="G:\blog\public\2020\07\28\【特征提取+分类模型】4种常见的NLP实践思路\image\2.png" alt="2" style="zoom: 80%;" />

<p>其中左边的label是数据集文本对应的标签，而右边的text则是编码后的文本，文本对应的标签列举如下：</p>
<img src="G:\blog\public\2020\07\28\【特征提取+分类模型】4种常见的NLP实践思路\image\3.png" alt="3" style="zoom: 67%;" />

<p>根据官方描述：赛题以匿名处理后的新闻数据为赛题数据，数据集报名后可见并可下载。赛题数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据。</p>
<p>赛题数据由以下几个部分构成：训练集20w条样本，测试集A包括5w条样本，测试集B包括5w条样本。为了预防选手人工标注测试集的情况，我们将比赛数据的文本按照字符级别进行了匿名处理。</p>
<p>同时我们还应该注意到官网有给出结果评价指标，我们也需要根据这个评价指标衡量我们的验证集数据误差：</p>
<p><img src="C:\Users\Melody\AppData\Roaming\Typora\typora-user-images\image-20201202235654877.png" alt="image-20201202235654877"></p>
<p>既然该拿到的我们都拿到了，我们接下来就开始构思我们都应该使用哪些思路来完成我们的预测。</p>
<h2 id="2-2-常见思路"><a href="#2-2-常见思路" class="headerlink" title="2.2 常见思路"></a><strong>2.2 常见思路</strong></h2><p>赛题本质是一个文本分类问题，需要根据每句的字符进行分类。但赛题给出的数据是匿名化的，不能直接使用中文分词等操作，这个是赛题的难点。</p>
<p>因此本次赛题的难点是需要对匿名字符进行建模，进而完成文本分类的过程。由于文本数据是一种典型的非结构化数据，因此可能涉及到**<code>特征提取</code><strong>和</strong><code>分类模型</code>**两个部分。为了减低参赛难度，我们提供了一些解题思路供大家参考：</p>
<ul>
<li><strong>思路1</strong>：TF-IDF + 机器学习分类器：直接使用TF-IDF对文本提取特征，并使用分类器进行分类。在分类器的选择上，可以使用SVM、LR、或者XGBoost。</li>
<li><strong>思路2</strong>：FastText：FastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建出分类器。</li>
<li><strong>思路3</strong>：WordVec + 深度学习分类器：WordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRNN或者BiLSTM。</li>
<li><strong>思路4</strong>：Bert词向量：Bert是高配款的词向量，具有强大的建模学习能力。</li>
</ul>
<h1 id="三、基于机器学习的文本分类"><a href="#三、基于机器学习的文本分类" class="headerlink" title="三、基于机器学习的文本分类"></a><strong>三、基于机器学习的文本分类</strong></h1><h2 id="3-1-TF-IDF-机器学习分类器-思路1"><a href="#3-1-TF-IDF-机器学习分类器-思路1" class="headerlink" title="3.1 TF-IDF+机器学习分类器(思路1)"></a><strong>3.1 TF-IDF+机器学习分类器(思路1)</strong></h2><h3 id="3-1-1-什么是TF-IDF？"><a href="#3-1-1-什么是TF-IDF？" class="headerlink" title="3.1.1. 什么是TF-IDF？"></a><strong>3.1.1. 什么是TF-IDF？</strong></h3><p>TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。</p>
<p>TF-IDF有两层意思，一层是”词频”（Term Frequency，缩写为TF），另一层是”逆文档频率”（Inverse Document Frequency，缩写为IDF）。</p>
<p>当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。</p>
<h3 id="3-2-2-TF-IDF算法步骤"><a href="#3-2-2-TF-IDF算法步骤" class="headerlink" title="3.2.2. TF-IDF算法步骤"></a><strong>3.2.2. TF-IDF算法步骤</strong></h3><p><strong>第一步</strong>，计算词频：</p>
<img src="C:\Users\Melody\AppData\Roaming\Typora\typora-user-images\image-20201202235831618.png" alt="image-20201202235831618" style="zoom:50%;" />

<p>考虑到文章有长短之分，为了便于不同文章的比较，进行”词频”标准化:</p>
<img src="C:\Users\Melody\AppData\Roaming\Typora\typora-user-images\image-20201202235847123.png" alt="image-20201202235847123" style="zoom: 50%;" />

<p><strong>第二步</strong>，计算逆文档频率：</p>
<p>这时，需要一个语料库（corpus），用来模拟语言的使用环境。</p>
<img src="C:\Users\Melody\AppData\Roaming\Typora\typora-user-images\image-20201202235904403.png" alt="image-20201202235904403" style="zoom:50%;" />

<p>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。</p>
<p><strong>第三步</strong>，计算TF-IDF：</p>
<img src="https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsHl2PYInTor9uj1UkVTPP4qvibj0FmA3M3IWFRTsQmco54gwYspPo7a4SmSkr9TSkk1TEbPzMysBOQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:50%;" />

<p>可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。</p>
<h3 id="3-3-3-优缺点"><a href="#3-3-3-优缺点" class="headerlink" title="3.3.3. 优缺点"></a><strong>3.3.3. 优缺点</strong></h3><p>TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。</p>
<h1 id="四、基于深度学习的文本分类"><a href="#四、基于深度学习的文本分类" class="headerlink" title="四、基于深度学习的文本分类"></a>四、基于深度学习的文本分类</h1><h2 id="4-1-FastText-思路2"><a href="#4-1-FastText-思路2" class="headerlink" title="4.1 FastText(思路2)"></a><strong>4.1 FastText(思路2)</strong></h2><h3 id="4-1-1-FastText的核心思想"><a href="#4-1-1-FastText的核心思想" class="headerlink" title="4.1.1 FastText的核心思想"></a><strong>4.1.1 FastText的核心思想</strong></h3><p>将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级N-gram特征的引入以及分层Softmax分类。</p>
<h3 id="4-1-2-字符级N-gram特征"><a href="#4-1-2-字符级N-gram特征" class="headerlink" title="4.1.2 字符级N-gram特征"></a><strong>4.1.2 字符级N-gram特征</strong></h3><p>N-gram是基于语言模型的算法，基本思想是将文本内容按照子节顺序进行大小为N的窗口滑动操作，最终形成窗口为N的字节片段序列。举个例子：</p>
<blockquote>
<p>我来到达观数据参观</p>
</blockquote>
<blockquote>
<p>相应的bigram特征为：我来 来到 到达 达观 观数 数据 据参 参观</p>
</blockquote>
<blockquote>
<p>相应的trigram特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观</p>
</blockquote>
<p>注意一点：n-gram中的gram根据粒度不同，有不同的含义。它可以是字粒度，也可以是词粒度的。上面所举的例子属于字粒度的n-gram，词粒度的n-gram看下面例子：</p>
<blockquote>
<p>我 来到 达观数据 参观</p>
</blockquote>
<blockquote>
<p>相应的bigram特征为：我/来到 来到/达观数据 达观数据/参观</p>
</blockquote>
<blockquote>
<p>相应的trigram特征为：我/来到/达观数据 来到/达观数据/参观</p>
</blockquote>
<p>n-gram产生的特征只是作为文本特征的候选集，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征。</p>
<h3 id="4-1-3-分层Softmax分类"><a href="#4-1-3-分层Softmax分类" class="headerlink" title="4.1.3 分层Softmax分类"></a><strong>4.1.3 分层Softmax分类</strong></h3><p>softmax函数常在神经网络输出层充当激活函数，目的就是将输出层的值归一化到0-1区间，将神经元输出构造成概率分布，主要就是起到将神经元输出值进行归一化的作用。</p>
<p>下图是一个分层Softmax示例：</p>
<img src="C:\Users\Melody\AppData\Roaming\Typora\typora-user-images\image-20201203000728656.png" alt="image-20201203000728656" style="zoom:67%;" />

<p>通过分层的Softmax，计算复杂度一下从|K|降低到log|K|。</p>
<h2 id="4-2-Word2Vec-深度学习分类器（思路3）"><a href="#4-2-Word2Vec-深度学习分类器（思路3）" class="headerlink" title="4.2 Word2Vec+深度学习分类器（思路3）"></a><strong>4.2 Word2Vec+深度学习分类器（思路3）</strong></h2><h3 id="4-2-1-Word2Vec"><a href="#4-2-1-Word2Vec" class="headerlink" title="4.2.1 Word2Vec"></a><strong>4.2.1 Word2Vec</strong></h3><p>Word2vec，是一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系，该向量为神经网络之隐藏层。【百度百科】</p>
<p>Word2vec 是 Word Embedding 的方法之一。他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。</p>
<h3 id="4-2-2-优化方法"><a href="#4-2-2-优化方法" class="headerlink" title="4.2.2 优化方法"></a><strong>4.2.2 优化方法</strong></h3><p>为了提高速度，Word2vec 经常采用 2 种加速方式：</p>
<ul>
<li>Negative Sample（负采样）</li>
<li>Hierarchical Softmax</li>
</ul>
<h3 id="4-2-3-优缺点"><a href="#4-2-3-优缺点" class="headerlink" title="4.2.3 优缺点"></a><strong>4.2.3 优缺点</strong></h3><p><strong>优点：</strong></p>
<ul>
<li>由于 Word2vec 会考虑上下文，跟之前的 Embedding 方法相比，效果要更好（但不如 18 年之后的方法）</li>
<li>比之前的 Embedding方 法维度更少，所以速度更快</li>
<li>通用性很强，可以用在各种 NLP 任务中</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>由于词和向量是一对一的关系，所以多义词的问题无法解决。</li>
<li>Word2vec 是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化</li>
</ul>
<h2 id="4-3-Bert词向量-思路4"><a href="#4-3-Bert词向量-思路4" class="headerlink" title="4.3 Bert词向量(思路4)"></a><strong>4.3 Bert词向量(思路4)</strong></h2><p>BERT（Bidirectional Encoder Representations from Transformers）词向量模型，2018年10月在《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》这篇论文中被Google提出，在11种不同nlp测试中创出最佳成绩，包括将glue基准推至80.4%（绝对改进7.6%），multinli准确度达到86.7% （绝对改进率5.6%）等。</p>
<h3 id="4-1-1-特征"><a href="#4-1-1-特征" class="headerlink" title="4.1.1 特征"></a><strong>4.1.1 特征</strong></h3><p>1、通过联合调节所有层中的左右上下文来预训练深度双向表示</p>
<p>2、the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many systems with task-specific architectures</p>
<p>3、所需计算量非常大。Jacob 说：「OpenAI 的 Transformer 有 12 层、768 个隐藏单元，他们使用 8 块 P100 在 8 亿词量的数据集上训练 40 个 Epoch 需要一个月，而 BERT-Large 模型有 24 层、2014 个隐藏单元，它们在有 33 亿词量的数据集上需要训练 40 个 Epoch，因此在 8 块 P100 上可能需要 1 年？16 Cloud TPU 已经是非常大的计算力了。</p>
<p>4、预训练的BERT表示可以通过一个额外的输出层进行微调，适用于广泛任务的state-of-the-art模型的构建，比如问答任务和语言推理，无需针对具体任务做大幅架构修改。</p>
<p>5、一词多义问题</p>
<h1 id="五、学习和实践路径参考资料"><a href="#五、学习和实践路径参考资料" class="headerlink" title="五、学习和实践路径参考资料"></a>五、学习和实践路径参考资料</h1><p>1、复旦大学邱锡鹏组实验室新生一般完成的五个NLP练习上手实验（NLP四大类任务：分类、序列标注、文本匹配、文本生成，都需要完整实现一遍）。<br><a href="https://www.zhihu.com/question/324189960/answer/682130580?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=952466020582064128">https://www.zhihu.com/question/324189960/answer/682130580?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=952466020582064128</a><br>2自动化所宗成庆研究员：读懂NLP的过去与现在（梳理的非常好，把各个概念之间的关系和NLP的发展都梳理清楚了）<br><a href="https://mp.weixin.qq.com/s/xgySwq2m-mHT7XG1zZGpzw">https://mp.weixin.qq.com/s/xgySwq2m-mHT7XG1zZGpzw</a><br>3、中文自然语言处理入门实战<br><a href="https://mp.weixin.qq.com/s/5z7Xy4NL-buUkpBmv4iIpw">https://mp.weixin.qq.com/s/5z7Xy4NL-buUkpBmv4iIpw</a><br>4、自然语言处理全家福：纵览当前NLP中的任务、数据、模型与论文<br><a href="https://mp.weixin.qq.com/s/sQ903WNSR4v367t78_VG1Q">https://mp.weixin.qq.com/s/sQ903WNSR4v367t78_VG1Q</a><br>5、中文信息处理发展报告（综述由中文信息学会统筹，国内各大NLP专家撰写，非常适合入门了解NLP）<br><a href="http://cips-upload.bj.bcebos.com/cips2016.pdf">http://cips-upload.bj.bcebos.com/cips2016.pdf</a><br>6、Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]<br><a href="https://ieeexplore.ieee.org/document/6786458">https://ieeexplore.ieee.org/document/6786458</a><br>7、Natural Language Processing: A Review<br><a href="https://www.researchgate.net/publication/309210149_Natural_Language_Processing_A_Review">https://www.researchgate.net/publication/309210149_Natural_Language_Processing_A_Review</a><br>8、A Review of the Neural History of Natural Language Processing<br><a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/">http://ruder.io/a-review-of-the-recent-history-of-nlp/</a><br>9、邓力和刘洋大神合著的图书《Deep Learning in Natural Language Processing》<br>10、宗成庆研究员所著《统计自然语言处理》（经典之作）<br>11、Steven Bird所著《Python自然语言处理》（快速掌握python开发nlp技术的各种能力）<br>12、机器学习 → 推荐周志华教授所著的《机器学习》<br>13、深度学习 → 推荐Ian Goodfellow等人合著的《Deep Learning》<br>14、开发框架 → 首推Pytorch，推荐陈云的《深度学习框架Pytorch入门与实践》或者廖星宇的《深度学习入门之Pytorch》<br>15、Tensorflow学习 → 推荐黄文坚、唐源的《Tensorflow实战》</p>
]]></content>
      <categories>
        <category>【NLP】</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>天池推荐系统入门赛——数据分析</title>
    <url>/2020/12/02/%E5%A4%A9%E6%B1%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%85%A5%E9%97%A8%E8%B5%9B%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>﻿# 数据分析</p>
<p>数据分析的价值主要在于熟悉了解整个数据集的基本情况包括每个文件里有哪些数据，具体的文件中的每个字段表示什么实际含义，以及数据集中特征之间的相关性，在推荐场景下主要就是分析用户本身的基本属性，文章基本属性，以及用户和文章交互的一些分布，这些都有利于后面的召回策略的选择，以及特征工程。</p>
<p><strong>建议：当特征工程和模型调参已经很难继续上分了，可以回来在重新从新的角度去分析这些数据，或许可以找到上分的灵感</strong></p>
<h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入相关包</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.rc(<span class="string">&#x27;font&#x27;</span>, family=<span class="string">&#x27;SimHei&#x27;</span>, size=<span class="number">13</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os,gc,re,warnings,sys</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">path = <span class="string">&#x27;./data_raw/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#####train</span></span><br><span class="line">trn_click = pd.read_csv(path+<span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">item_df = pd.read_csv(path+<span class="string">&#x27;articles.csv&#x27;</span>)</span><br><span class="line">item_df = item_df.rename(columns=&#123;<span class="string">&#x27;article_id&#x27;</span>: <span class="string">&#x27;click_article_id&#x27;</span>&#125;)  <span class="comment">#重命名，方便后续match</span></span><br><span class="line">item_emb_df = pd.read_csv(path+<span class="string">&#x27;articles_emb.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#####test</span></span><br><span class="line">tst_click = pd.read_csv(path+<span class="string">&#x27;testA_click_log.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>计算用户点击rank和点击次数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对每个用户的点击时间戳进行排序</span></span><br><span class="line">trn_click[<span class="string">&#x27;rank&#x27;</span>] = trn_click.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;click_timestamp&#x27;</span>].rank(ascending=<span class="literal">False</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">tst_click[<span class="string">&#x27;rank&#x27;</span>] = tst_click.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;click_timestamp&#x27;</span>].rank(ascending=<span class="literal">False</span>).astype(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算用户点击文章的次数，并添加新的一列count</span></span><br><span class="line">trn_click[<span class="string">&#x27;click_cnts&#x27;</span>] = trn_click.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;click_timestamp&#x27;</span>].transform(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">tst_click[<span class="string">&#x27;click_cnts&#x27;</span>] = tst_click.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;click_timestamp&#x27;</span>].transform(<span class="string">&#x27;count&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="数据浏览"><a href="#数据浏览" class="headerlink" title="数据浏览"></a>数据浏览</h2><h3 id="用户点击日志文件-训练集"><a href="#用户点击日志文件-训练集" class="headerlink" title="用户点击日志文件_训练集"></a>用户点击日志文件_训练集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_click = trn_click.merge(item_df, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&#x27;click_article_id&#x27;</span>])</span><br><span class="line">trn_click.head()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119112706647.png" alt="image-20201119112706647"></p>
<p><strong>train_click_log.csv文件数据中每个字段的含义</strong> </p>
<ol>
<li>user_id: 用户的唯一标识</li>
<li>click_article_id: 用户点击的文章唯一标识</li>
<li>click_timestamp: 用户点击文章时的时间戳</li>
<li>click_environment: 用户点击文章的环境</li>
<li>click_deviceGroup: 用户点击文章的设备组</li>
<li>click_os: 用户点击文章时的操作系统</li>
<li>click_country: 用户点击文章时的所在的国家</li>
<li>click_region: 用户点击文章时所在的区域</li>
<li>click_referrer_type: 用户点击文章时，文章的来源</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用户点击日志信息</span></span><br><span class="line">trn_click.info()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119112622939.png" alt="image-20201119112622939"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_click.describe()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119112649376.png" alt="image-20201119112649376"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练集中的用户数量为20w</span></span><br><span class="line">trn_click.user_id.nunique()</span><br></pre></td></tr></table></figure>


<pre><code>200000</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_click.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_article_id&#x27;</span>].count().<span class="built_in">min</span>()  <span class="comment"># 训练集里面每个用户至少点击了两篇文章</span></span><br></pre></td></tr></table></figure>


<pre><code>2</code></pre>
<p><strong>画直方图大体看一下基本的属性分布</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">20</span>))</span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>, <span class="string">&#x27;click_environment&#x27;</span>, <span class="string">&#x27;click_deviceGroup&#x27;</span>, <span class="string">&#x27;click_os&#x27;</span>, <span class="string">&#x27;click_country&#x27;</span>, </span><br><span class="line">            <span class="string">&#x27;click_region&#x27;</span>, <span class="string">&#x27;click_referrer_type&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>, <span class="string">&#x27;click_cnts&#x27;</span>]:</span><br><span class="line">    plot_envs = plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i)</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    v = trn_click[col].value_counts().reset_index()[:<span class="number">10</span>]</span><br><span class="line">    fig = sns.barplot(x=v[<span class="string">&#x27;index&#x27;</span>], y=v[col])</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> fig.get_xticklabels():</span><br><span class="line">        item.set_rotation(<span class="number">90</span>)</span><br><span class="line">    plt.title(col)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/20201118000820300.png" alt="在这里插入图片描述"></p>
<p><strong>从点击时间clik_timestamp来看，分布较为平均，可不做特殊处理。由于时间戳是13位的，后续将时间格式转换成10位方便计算。</strong></p>
<p><strong>从点击环境click_environment来看，仅有1922次（占0.1%）点击环境为1；仅有24617次（占2.3%）点击环境为2；剩余（占97.6%）点击环境为4。</strong></p>
<p><strong>从点击设备组click_deviceGroup来看，设备1占大部分（60.4%），设备3占36%。</strong></p>
<h3 id="测试集用户点击日志"><a href="#测试集用户点击日志" class="headerlink" title="测试集用户点击日志"></a>测试集用户点击日志</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tst_click = tst_click.merge(item_df, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&#x27;click_article_id&#x27;</span>])</span><br><span class="line">tst_click.head()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119112952261.png" alt="image-20201119112952261"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tst_click.describe()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113015529.png" alt="image-20201119113015529"></p>
<p><strong>我们可以看出训练集和测试集的用户是完全不一样的</strong></p>
<p><strong>训练集的用户ID由0 ~ 199999，而测试集A的用户ID由200000 ~ 249999。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试集中的用户数量为5w</span></span><br><span class="line">tst_click.user_id.nunique()</span><br></pre></td></tr></table></figure>


<pre><code>50000</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tst_click.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_article_id&#x27;</span>].count().<span class="built_in">min</span>() <span class="comment"># 注意测试集里面有只点击过一次文章的用户</span></span><br></pre></td></tr></table></figure>


<pre><code>1</code></pre>
<h3 id="新闻文章信息数据表"><a href="#新闻文章信息数据表" class="headerlink" title="新闻文章信息数据表"></a>新闻文章信息数据表</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#新闻文章数据集浏览</span></span><br><span class="line">item_df.head().append(item_df.tail())</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113118388.png" alt="image-20201119113118388"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_df[<span class="string">&#x27;words_count&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113147240.png" alt="image-20201119113147240"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(item_df[<span class="string">&#x27;category_id&#x27;</span>].nunique())     <span class="comment"># 461个文章主题</span></span><br><span class="line">item_df[<span class="string">&#x27;category_id&#x27;</span>].hist()</span><br></pre></td></tr></table></figure>

<p><img src="C:\Users\ruyiluo\AppData\Roaming\Typora\typora-user-images\image-20201119113223601.png" alt="image-20201119113223601"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_df.shape       <span class="comment"># 364047篇文章</span></span><br></pre></td></tr></table></figure>


<pre><code>(364047, 4)</code></pre>
<h3 id="新闻文章embedding向量表示"><a href="#新闻文章embedding向量表示" class="headerlink" title="新闻文章embedding向量表示"></a>新闻文章embedding向量表示</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_emb_df.head()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113253455.png" alt="image-20201119113253455"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_emb_df.shape</span><br></pre></td></tr></table></figure>


<pre><code>(364047, 251)</code></pre>
<h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><h3 id="用户重复点击"><a href="#用户重复点击" class="headerlink" title="用户重复点击"></a>用户重复点击</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#####merge</span></span><br><span class="line">user_click_merge = trn_click.append(tst_click)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用户重复点击</span></span><br><span class="line">user_click_count = user_click_merge.groupby([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>])[<span class="string">&#x27;click_timestamp&#x27;</span>].agg(&#123;<span class="string">&#x27;count&#x27;</span>&#125;).reset_index()</span><br><span class="line">user_click_count[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113334727.png" alt="image-20201119113334727"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_click_count[user_click_count[<span class="string">&#x27;count&#x27;</span>]&gt;<span class="number">7</span>]</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113351807.png" alt="image-20201119113351807"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_click_count[<span class="string">&#x27;count&#x27;</span>].unique()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113429769.png" alt="image-20201119113429769"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#用户点击新闻次数</span></span><br><span class="line">user_click_count.loc[:,<span class="string">&#x27;count&#x27;</span>].value_counts() </span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113414785.png" alt="image-20201119113414785"></p>
<p><strong>可以看出：有1605541（约占99.2%）的用户未重复阅读过文章，仅有极少数用户重复点击过某篇文章。 这个也可以单独制作成特征</strong></p>
<h3 id="用户点击环境变化分析"><a href="#用户点击环境变化分析" class="headerlink" title="用户点击环境变化分析"></a>用户点击环境变化分析</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_envs</span>(<span class="params">df, cols, r, c</span>):</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        plt.subplot(r, c, i)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        v = df[col].value_counts().reset_index()</span><br><span class="line">        fig = sns.barplot(x=v[<span class="string">&#x27;index&#x27;</span>], y=v[col])</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> fig.get_xticklabels():</span><br><span class="line">            item.set_rotation(<span class="number">90</span>)</span><br><span class="line">        plt.title(col)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分析用户点击环境变化是否明显，这里随机采样10个用户分析这些用户的点击环境分布</span></span><br><span class="line">sample_user_ids = np.random.choice(tst_click[<span class="string">&#x27;user_id&#x27;</span>].unique(), size=<span class="number">5</span>, replace=<span class="literal">False</span>)</span><br><span class="line">sample_users = user_click_merge[user_click_merge[<span class="string">&#x27;user_id&#x27;</span>].isin(sample_user_ids)]</span><br><span class="line">cols = [<span class="string">&#x27;click_environment&#x27;</span>,<span class="string">&#x27;click_deviceGroup&#x27;</span>, <span class="string">&#x27;click_os&#x27;</span>, <span class="string">&#x27;click_country&#x27;</span>, <span class="string">&#x27;click_region&#x27;</span>,<span class="string">&#x27;click_referrer_type&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> _, user_df <span class="keyword">in</span> sample_users.groupby(<span class="string">&#x27;user_id&#x27;</span>):</span><br><span class="line">    plot_envs(user_df, cols, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113624424.png" alt="image-20201119113624424"></p>
<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113637746.png" alt="image-20201119113637746"></p>
<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113652132.png" alt="image-20201119113652132"></p>
<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113702034.png" alt="image-20201119113702034"></p>
<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113714135.png" alt="image-20201119113714135"></p>
<p><strong>可以看出绝大多数数的用户的点击环境是比较固定的。思路：可以基于这些环境的统计特征来代表该用户本身的属性</strong></p>
<h3 id="用户点击新闻数量的分布"><a href="#用户点击新闻数量的分布" class="headerlink" title="用户点击新闻数量的分布"></a>用户点击新闻数量的分布</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_click_item_count = <span class="built_in">sorted</span>(user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_article_id&#x27;</span>].count(), reverse=<span class="literal">True</span>)</span><br><span class="line">plt.plot(user_click_item_count)</span><br></pre></td></tr></table></figure>


<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113759490.png" alt="image-20201119113759490"></p>
<p><strong>可以根据用户的点击文章次数看出用户的活跃度</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#点击次数在前50的用户</span></span><br><span class="line">plt.plot(user_click_item_count[:<span class="number">50</span>])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113825586.png" alt="image-20201119113825586"></p>
<p><strong>点击次数排前50的用户的点击次数都在100次以上。思路：我们可以定义点击次数大于等于100次的用户为活跃用户，这是一种简单的处理思路， 判断用户活跃度，更加全面的是再结合上点击时间，后面我们会基于点击次数和点击时间两个方面来判断用户活跃度。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#点击次数排名在[25000:50000]之间</span></span><br><span class="line">plt.plot(user_click_item_count[<span class="number">25000</span>:<span class="number">50000</span>])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113844946.png" alt="image-20201119113844946"></p>
<p><strong>可以看出点击次数小于等于两次的用户非常的多，这些用户可以认为是非活跃用户</strong></p>
<h3 id="新闻点击次数分析"><a href="#新闻点击次数分析" class="headerlink" title="新闻点击次数分析"></a>新闻点击次数分析</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_click_count = <span class="built_in">sorted</span>(user_click_merge.groupby(<span class="string">&#x27;click_article_id&#x27;</span>)[<span class="string">&#x27;user_id&#x27;</span>].count(), reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(item_click_count)</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113912912.png" alt="image-20201119113912912"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(item_click_count[:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113930745.png" alt="image-20201119113930745"></p>
<p><strong>可以看出点击次数最多的前100篇新闻，点击次数大于1000次</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(item_click_count[:<span class="number">20</span>])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119113958254.png" alt="image-20201119113958254"></p>
<p><strong>点击次数最多的前20篇新闻，点击次数大于2500。思路：可以定义这些新闻为热门新闻， 这个也是简单的处理方式，后面我们也是根据点击次数和时间进行文章热度的一个划分。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(item_click_count[<span class="number">3500</span>:])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114017762.png" alt="image-20201119114017762"></p>
<p><strong>可以发现很多新闻只被点击过一两次。思路：可以定义这些新闻是冷门新闻。</strong></p>
<h3 id="新闻共现频次：两篇新闻连续出现的次数"><a href="#新闻共现频次：两篇新闻连续出现的次数" class="headerlink" title="新闻共现频次：两篇新闻连续出现的次数"></a>新闻共现频次：两篇新闻连续出现的次数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tmp = user_click_merge.sort_values(<span class="string">&#x27;click_timestamp&#x27;</span>)</span><br><span class="line">tmp[<span class="string">&#x27;next_item&#x27;</span>] = tmp.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;click_article_id&#x27;</span>].transform(<span class="keyword">lambda</span> x:x.shift(-<span class="number">1</span>))</span><br><span class="line">union_item = tmp.groupby([<span class="string">&#x27;click_article_id&#x27;</span>,<span class="string">&#x27;next_item&#x27;</span>])[<span class="string">&#x27;click_timestamp&#x27;</span>].agg(&#123;<span class="string">&#x27;count&#x27;</span>&#125;).reset_index().sort_values(<span class="string">&#x27;count&#x27;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">union_item[[<span class="string">&#x27;count&#x27;</span>]].describe()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114044351.png" alt="image-20201119114044351"></p>
<p><strong>由统计数据可以看出，平均共现次数2.88，最高为1687。</strong></p>
<p><strong>说明用户看的新闻，相关性是比较强的。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#画个图直观地看一看</span></span><br><span class="line">x = union_item[<span class="string">&#x27;click_article_id&#x27;</span>]</span><br><span class="line">y = union_item[<span class="string">&#x27;count&#x27;</span>]</span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114106223.png" alt="image-20201119114106223"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(union_item[<span class="string">&#x27;count&#x27;</span>].values[<span class="number">40000</span>:])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114122557.png" alt="image-20201119114122557"></p>
<p><strong>大概有70000个pair至少共现一次。</strong></p>
<h3 id="新闻文章信息"><a href="#新闻文章信息" class="headerlink" title="新闻文章信息"></a>新闻文章信息</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不同类型的新闻出现的次数</span></span><br><span class="line">plt.plot(user_click_merge[<span class="string">&#x27;category_id&#x27;</span>].value_counts().values)</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114144058.png" alt="image-20201119114144058"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#出现次数比较少的新闻类型, 有些新闻类型，基本上就出现过几次</span></span><br><span class="line">plt.plot(user_click_merge[<span class="string">&#x27;category_id&#x27;</span>].value_counts().values[<span class="number">150</span>:])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114201764.png" alt="image-20201119114201764"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#新闻字数的描述性统计</span></span><br><span class="line">user_click_merge[<span class="string">&#x27;words_count&#x27;</span>].describe()</span><br></pre></td></tr></table></figure>

<p><img src="C:\Users\ruyiluo\AppData\Roaming\Typora\typora-user-images\image-20201119114216116.png" alt="image-20201119114216116"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(user_click_merge[<span class="string">&#x27;words_count&#x27;</span>].values)</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114241194.png" alt="image-20201119114241194"></p>
<h3 id="用户点击的新闻类型的偏好"><a href="#用户点击的新闻类型的偏好" class="headerlink" title="用户点击的新闻类型的偏好"></a>用户点击的新闻类型的偏好</h3><p>此特征可以用于度量用户的兴趣是否广泛。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(<span class="built_in">sorted</span>(user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;category_id&#x27;</span>].nunique(), reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>


<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114300286.png" alt="image-20201119114300286"></p>
<p><strong>从上图中可以看出有一小部分用户阅读类型是极其广泛的，大部分人都处在20个新闻类型以下。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;category_id&#x27;</span>].nunique().reset_index().describe()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114318523.png" alt="image-20201119114318523"></p>
<h3 id="用户查看文章的长度的分布"><a href="#用户查看文章的长度的分布" class="headerlink" title="用户查看文章的长度的分布"></a>用户查看文章的长度的分布</h3><p>通过统计不同用户点击新闻的平均字数，这个可以反映用户是对长文更感兴趣还是对短文更感兴趣。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(<span class="built_in">sorted</span>(user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;words_count&#x27;</span>].mean(), reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>


<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114337448.png" alt="image-20201119114337448"></p>
<p><strong>从上图中可以发现有一小部分人看的文章平均词数非常高，也有一小部分人看的平均文章次数非常低。</strong></p>
<p><strong>大多数人偏好于阅读字数在200-400字之间的新闻。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#挑出大多数人的区间仔细看看</span></span><br><span class="line">plt.plot(<span class="built_in">sorted</span>(user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;words_count&#x27;</span>].mean(), reverse=<span class="literal">True</span>)[<span class="number">1000</span>:<span class="number">45000</span>])</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114355195.png" alt="image-20201119114355195"></p>
<p><strong>可以发现大多数人都是看250字以下的文章</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#更加详细的参数</span></span><br><span class="line">user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;words_count&#x27;</span>].mean().reset_index().describe()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114418911.png" alt="image-20201119114418911"></p>
<h3 id="用户点击新闻的时间分析"><a href="#用户点击新闻的时间分析" class="headerlink" title="用户点击新闻的时间分析"></a>用户点击新闻的时间分析</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#为了更好的可视化，这里把时间进行归一化操作</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">mm = MinMaxScaler()</span><br><span class="line">user_click_merge[<span class="string">&#x27;click_timestamp&#x27;</span>] = mm.fit_transform(user_click_merge[[<span class="string">&#x27;click_timestamp&#x27;</span>]])</span><br><span class="line">user_click_merge[<span class="string">&#x27;created_at_ts&#x27;</span>] = mm.fit_transform(user_click_merge[[<span class="string">&#x27;created_at_ts&#x27;</span>]])</span><br><span class="line"></span><br><span class="line">user_click_merge = user_click_merge.sort_values(<span class="string">&#x27;click_timestamp&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_click_merge.head()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114447904.png" alt="image-20201119114447904"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_diff_time_func</span>(<span class="params">df, col</span>):</span></span><br><span class="line">    df = pd.DataFrame(df, columns=&#123;col&#125;)</span><br><span class="line">    df[<span class="string">&#x27;time_shift1&#x27;</span>] = df[col].shift(<span class="number">1</span>).fillna(<span class="number">0</span>)</span><br><span class="line">    df[<span class="string">&#x27;diff_time&#x27;</span>] = <span class="built_in">abs</span>(df[col] - df[<span class="string">&#x27;time_shift1&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> df[<span class="string">&#x27;diff_time&#x27;</span>].mean()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 点击时间差的平均值</span></span><br><span class="line">mean_diff_click_time = user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_timestamp&#x27;</span>, <span class="string">&#x27;created_at_ts&#x27;</span>].apply(<span class="keyword">lambda</span> x: mean_diff_time_func(x, <span class="string">&#x27;click_timestamp&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(<span class="built_in">sorted</span>(mean_diff_click_time.values, reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119114505086.png" alt="image-20201119114505086"></p>
<p><strong>从上图可以发现不同用户点击文章的时间差是有差异的。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 前后点击文章的创建时间差的平均值</span></span><br><span class="line">mean_diff_created_time = user_click_merge.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_timestamp&#x27;</span>, <span class="string">&#x27;created_at_ts&#x27;</span>].apply(<span class="keyword">lambda</span> x: mean_diff_time_func(x, <span class="string">&#x27;created_at_ts&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(<span class="built_in">sorted</span>(mean_diff_created_time.values, reverse=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119122227666.png" alt="image-20201119122227666"></p>
<p><strong>从图中可以发现用户先后点击文章，文章的创建时间也是有差异的</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户前后点击文章的相似性分布</span></span><br><span class="line">item_idx_2_rawid_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_emb_df[<span class="string">&#x27;article_id&#x27;</span>], item_emb_df.index))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> item_emb_df[<span class="string">&#x27;article_id&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_emb_np = np.ascontiguousarray(item_emb_df.values, dtype=np.float32)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机选择5个用户，查看这些用户前后查看文章的相似性</span></span><br><span class="line">sub_user_ids = np.random.choice(user_click_merge.user_id.unique(), size=<span class="number">15</span>, replace=<span class="literal">False</span>)</span><br><span class="line">sub_user_info = user_click_merge[user_click_merge[<span class="string">&#x27;user_id&#x27;</span>].isin(sub_user_ids)]</span><br><span class="line"></span><br><span class="line">sub_user_info.head()</span><br></pre></td></tr></table></figure>

<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119122251274.png" alt="image-20201119122251274"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_sim_list</span>(<span class="params">df</span>):</span></span><br><span class="line">    sim_list = []</span><br><span class="line">    item_list = df[<span class="string">&#x27;click_article_id&#x27;</span>].values</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(item_list)-<span class="number">1</span>):</span><br><span class="line">        emb1 = item_emb_np[item_idx_2_rawid_dict[item_list[i]]]</span><br><span class="line">        emb2 = item_emb_np[item_idx_2_rawid_dict[item_list[i+<span class="number">1</span>]]]</span><br><span class="line">        sim_list.append(np.dot(emb1,emb2)/(np.linalg.norm(emb1)*(np.linalg.norm(emb2))))</span><br><span class="line">    sim_list.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> sim_list</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> _, user_df <span class="keyword">in</span> sub_user_info.groupby(<span class="string">&#x27;user_id&#x27;</span>):</span><br><span class="line">    item_sim_list = get_item_sim_list(user_df)</span><br><span class="line">    plt.plot(item_sim_list)</span><br></pre></td></tr></table></figure>


<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119122310969.png" alt="image-20201119122310969"></p>
<p><strong>从图中可以看出有些用户前后看的商品的相似度波动比较大，有些波动比较小，也是有一定的区分度的。</strong></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过数据分析的过程， 我们目前可以得到以下几点重要的信息， 这个对于我们进行后面的特征制作和分析非常有帮助：</p>
<ol>
<li>训练集和测试集的用户id没有重复，也就是测试集里面的用户没有模型是没有见过的</li>
<li>训练集中用户最少的点击文章数是2， 而测试集里面用户最少的点击文章数是1</li>
<li>用户对于文章存在重复点击的情况， 但这个都存在于训练集里面</li>
<li>同一用户的点击环境存在不唯一的情况，后面做这部分特征的时候可以采用统计特征</li>
<li>用户点击文章的次数有很大的区分度，后面可以根据这个制作衡量用户活跃度的特征</li>
<li>文章被用户点击的次数也有很大的区分度，后面可以根据这个制作衡量文章热度的特征</li>
<li>用户看的新闻，相关性是比较强的，所以往往我们判断用户是否对某篇文章感兴趣的时候， 在很大程度上会和他历史点击过的文章有关</li>
<li>用户点击的文章字数有比较大的区别， 这个可以反映用户对于文章字数的区别</li>
<li>用户点击过的文章主题也有很大的区别， 这个可以反映用户的主题偏好<br>10.不同用户点击文章的时间差也会有所区别， 这个可以反映用户对于文章时效性的偏好</li>
</ol>
<p>所以根据上面的一些分析，可以更好的帮助我们后面做好特征工程， 充分挖掘数据的隐含信息。</p>
]]></content>
  </entry>
  <entry>
    <title>天池推荐系统入门赛——赛题理解+Baseline</title>
    <url>/2020/12/02/%E5%A4%A9%E6%B1%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%85%A5%E9%97%A8%E8%B5%9B%E2%80%94%E2%80%94%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3+Baseline/</url>
    <content><![CDATA[<p>﻿# 赛题理解</p>
<p>赛题理解是切入一道赛题的基础，会影响后续特征工程和模型构建等各种工作，也影响着后续发展工作的方向，正确了解赛题背后的思想以及赛题业务逻辑的清晰，有利于花费更少时间构建更为有效的特征模型， 在各种比赛中， 赛题理解都是极其重要且必须走好的第一步， 今天我们就从赛题的理解出发， 首先了解一下这次赛题的概况和数据，从中分析赛题以及大致的处理方式， 其次我们了解模型评测的指标，最后对赛题的理解整理一些经验。</p>
<h2 id="赛题简介"><a href="#赛题简介" class="headerlink" title="赛题简介"></a>赛题简介</h2><p>此次比赛是新闻推荐场景下的用户行为预测挑战赛， 该赛题是以新闻APP中的新闻推荐为背景， 目的是<strong>要求我们根据用户历史浏览点击新闻文章的数据信息预测用户未来的点击行为， 即用户的最后一次点击的新闻文章</strong>， 这道赛题的设计初衷是引导大家了解推荐系统中的一些业务背景， 解决实际问题。 </p>
<h2 id="数据概况"><a href="#数据概况" class="headerlink" title="数据概况"></a>数据概况</h2><p>该数据来自某新闻APP平台的用户交互数据，包括30万用户，近300万次点击，共36万多篇不同的新闻文章，同时每篇新闻文章有对应的embedding向量表示。为了保证比赛的公平性，从中抽取20万用户的点击日志数据作为训练集，5万用户的点击日志数据作为测试集A，5万用户的点击日志数据作为测试集B。具体数据表和参数， 大家可以参考赛题说明。下面说一下拿到这样的数据如何进行理解， 来有效的开展下一步的工作。</p>
<h2 id="评价方式理解"><a href="#评价方式理解" class="headerlink" title="评价方式理解"></a>评价方式理解</h2><p>理解评价方式， 我们需要结合着最后的提交文件来看， 根据sample.submit.csv， 我们最后提交的格式是针对每个用户， 我们都会给出五篇文章的推荐结果，按照点击概率从前往后排序。 而真实的每个用户最后一次点击的文章只会有一篇的真实答案， 所以我们就看我们推荐的这五篇里面是否有命中真实答案的。比如对于user1来说， 我们的提交会是：</p>
<blockquote>
<p>user1, article1, article2, article3, article4, article5.</p>
</blockquote>
<p>评价指标的公式如下：<br>$$<br>score(user) = \sum_{k=1}^5 \frac{s(user, k)}{k}<br>$$</p>
<p>假如article1就是真实的用户点击文章，也就是article1命中， 则s(user1,1)=1, s(user1,2-4)都是0， 如果article2是用户点击的文章， 则s(user,2)=1/2,s(user,1,3,4,5)都是0。也就是score(user)=命中第几条的倒数。如果都没中， 则score(user1)=0。 这个是合理的， 因为我们希望的就是命中的结果尽量靠前， 而此时分数正好比较高。</p>
<h2 id="赛题理解"><a href="#赛题理解" class="headerlink" title="赛题理解"></a>赛题理解</h2><p>根据赛题简介，我们首先要明确我们此次比赛的目标： 根据用户历史浏览点击新闻的数据信息预测用户最后一次点击的新闻文章。从这个目标上看， 会发现此次比赛和我们之前遇到的普通的结构化比赛不太一样， 主要有两点：</p>
<ul>
<li>首先是目标上， 要预测最后一次点击的新闻文章，也就是我们给用户推荐的是新闻文章， 并不是像之前那种预测一个数或者预测数据哪一类那样的问题</li>
<li>数据上， 通过给出的数据我们会发现， 这种数据也不是我们之前遇到的那种特征+标签的数据，而是基于了真实的业务场景， 拿到的用户的点击日志</li>
</ul>
<p>所以拿到这个题目，我们的思考方向就是结合我们的目标，<strong>把该预测问题转成一个监督学习的问题(特征+标签)，然后我们才能进行ML，DL等建模预测</strong>。那么我们自然而然的就应该在心里会有这么几个问题：如何转成一个监督学习问题呢？ 转成一个什么样的监督学习问题呢？ 我们能利用的特征又有哪些呢？ 又有哪些模型可以尝试呢？ 此次面对数万级别的文章推荐，我们又有哪些策略呢？ </p>
<p>当然这些问题不会在我们刚看到赛题之后就一下出来答案， 但是只要有了问题之后， 我们就能想办法解决问题了， 比如上面的第二个问题，转成一个什么样的监督学习问题？  由于我们是预测用户最后一次点击的新闻文章，从36万篇文章中预测某一篇的话我们首先可能会想到这可能是一个多分类的问题(36万类里面选1)， 但是如此庞大的分类问题， 我们做起来可能比较困难， 那么能不能转化一下？ 既然是要预测最后一次点击的文章， 那么如果我们能预测出某个用户最后一次对于某一篇文章会进行点击的概率， 是不是就间接性的解决了这个问题呢？概率最大的那篇文章不就是用户最后一次可能点击的新闻文章吗？ 这样就把原问题变成了一个点击率预测的问题(用户, 文章) –&gt; 点击的概率(软分类)， 而这个问题， 就是我们所熟悉的监督学习领域分类问题了， 这样我们后面建模的时候， 对于模型的选择就基本上有大致方向了，比如最简单的逻辑回归模型。</p>
<p>这样， 我们对于该赛题的解决方案应该有了一个大致的解决思路，要先转成一个分类问题来做， 而分类的标签就是用户是否会点击某篇文章，分类问题的特征中会有用户和文章，我们要训练一个分类模型， 对某用户最后一次点击某篇文章的概率进行预测。 那么又会有几个问题：如何转成监督学习问题？ 训练集和测试集怎么制作？ 我们又能利用哪些特征？ 我们又可以尝试哪些模型？ 面对36万篇文章， 20多万用户的推荐， 我们又有哪些策略来缩减问题的规模？如何进行最后的预测？  </p>
<h1 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h1><h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> time, math, os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_path = <span class="string">&#x27;./data_raw/&#x27;</span></span><br><span class="line">save_path = <span class="string">&#x27;./tmp_results/&#x27;</span></span><br></pre></td></tr></table></figure>



<h2 id="df节省内存函数"><a href="#df节省内存函数" class="headerlink" title="df节省内存函数"></a>df节省内存函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 节约内存的一个标配函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_mem</span>(<span class="params">df</span>):</span></span><br><span class="line">    starttime = time.time()</span><br><span class="line">    numerics = [<span class="string">&#x27;int16&#x27;</span>, <span class="string">&#x27;int32&#x27;</span>, <span class="string">&#x27;int64&#x27;</span>, <span class="string">&#x27;float16&#x27;</span>, <span class="string">&#x27;float32&#x27;</span>, <span class="string">&#x27;float64&#x27;</span>]</span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type <span class="keyword">in</span> numerics:</span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> pd.isnull(c_min) <span class="keyword">or</span> pd.isnull(c_max):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    print(<span class="string">&#x27;-- Mem. usage decreased to &#123;:5.2f&#125; Mb (&#123;:.1f&#125;% reduction),time spend:&#123;:2.2f&#125; min&#x27;</span>.<span class="built_in">format</span>(end_mem,</span><br><span class="line">                                                                                                           <span class="number">100</span>*(start_mem-end_mem)/start_mem,</span><br><span class="line">                                                                                                           (time.time()-starttime)/<span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>



<h2 id="读取采样或全量数据"><a href="#读取采样或全量数据" class="headerlink" title="读取采样或全量数据"></a>读取采样或全量数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># debug模式：从训练集中划出一部分数据来调试代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_click_sample</span>(<span class="params">data_path, sample_nums=<span class="number">10000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练集中采样一部分数据调试</span></span><br><span class="line"><span class="string">        data_path: 原数据的存储路径</span></span><br><span class="line"><span class="string">        sample_nums: 采样数目（这里由于机器的内存限制，可以采样用户做）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    all_click = pd.read_csv(data_path + <span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">    all_user_ids = all_click.user_id.unique()</span><br><span class="line"></span><br><span class="line">    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=<span class="literal">False</span>) </span><br><span class="line">    all_click = all_click[all_click[<span class="string">&#x27;user_id&#x27;</span>].isin(sample_user_ids)]</span><br><span class="line">    </span><br><span class="line">    all_click = all_click.drop_duplicates(([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> all_click</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取点击数据，这里分成线上和线下，如果是为了获取线上提交结果应该讲测试集中的点击数据合并到总的数据中</span></span><br><span class="line"><span class="comment"># 如果是为了线下验证模型的有效性或者特征的有效性，可以只使用训练集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_click_df</span>(<span class="params">data_path=<span class="string">&#x27;./data_raw/&#x27;</span>, offline=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> offline:</span><br><span class="line">        all_click = pd.read_csv(data_path + <span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        trn_click = pd.read_csv(data_path + <span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">        tst_click = pd.read_csv(data_path + <span class="string">&#x27;testA_click_log.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        all_click = trn_click.append(tst_click)</span><br><span class="line">    </span><br><span class="line">    all_click = all_click.drop_duplicates(([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> all_click</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 全量训练集</span></span><br><span class="line">all_click_df = get_all_click_df(offline=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>



<h2 id="获取-用户-文章-点击时间字典"><a href="#获取-用户-文章-点击时间字典" class="headerlink" title="获取 用户 - 文章 - 点击时间字典"></a>获取 用户 - 文章 - 点击时间字典</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据点击时间获取用户的点击文章序列   &#123;user1: &#123;item1: time1, item2: time2..&#125;...&#125;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user_item_time</span>(<span class="params">click_df</span>):</span></span><br><span class="line">    </span><br><span class="line">    click_df = click_df.sort_values(<span class="string">&#x27;click_timestamp&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_item_time_pair</span>(<span class="params">df</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(df[<span class="string">&#x27;click_article_id&#x27;</span>], df[<span class="string">&#x27;click_timestamp&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    user_item_time_df = click_df.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>].apply(<span class="keyword">lambda</span> x: make_item_time_pair(x))\</span><br><span class="line">                                                            .reset_index().rename(columns=&#123;<span class="number">0</span>: <span class="string">&#x27;item_time_list&#x27;</span>&#125;)</span><br><span class="line">    user_item_time_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(user_item_time_df[<span class="string">&#x27;user_id&#x27;</span>], user_item_time_df[<span class="string">&#x27;item_time_list&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> user_item_time_dict</span><br></pre></td></tr></table></figure>



<h2 id="获取点击最多的Topk个文章"><a href="#获取点击最多的Topk个文章" class="headerlink" title="获取点击最多的Topk个文章"></a>获取点击最多的Topk个文章</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取近期点击最多的文章</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_topk_click</span>(<span class="params">click_df, k</span>):</span></span><br><span class="line">    topk_click = click_df[<span class="string">&#x27;click_article_id&#x27;</span>].value_counts().index[:k]</span><br><span class="line">    <span class="keyword">return</span> topk_click</span><br></pre></td></tr></table></figure>



<h2 id="itemCF的物品相似度计算"><a href="#itemCF的物品相似度计算" class="headerlink" title="itemCF的物品相似度计算"></a>itemCF的物品相似度计算</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_sim</span>(<span class="params">df</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        文章与文章之间的相似性矩阵计算</span></span><br><span class="line"><span class="string">        :param df: 数据表</span></span><br><span class="line"><span class="string">        :item_created_time_dict:  文章创建时间的字典</span></span><br><span class="line"><span class="string">        return : 文章与文章的相似性矩阵</span></span><br><span class="line"><span class="string">        思路: 基于物品的协同过滤(详细请参考上一期推荐系统基础的组队学习)， 在多路召回部分会加上关联规则的召回策略</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    user_item_time_dict = get_user_item_time(df)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算物品相似度</span></span><br><span class="line">    i2i_sim = &#123;&#125;</span><br><span class="line">    item_cnt = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> user, item_time_list <span class="keyword">in</span> tqdm(user_item_time_dict.items()):</span><br><span class="line">        <span class="comment"># 在基于商品的协同过滤优化的时候可以考虑时间因素</span></span><br><span class="line">        <span class="keyword">for</span> i, i_click_time <span class="keyword">in</span> item_time_list:</span><br><span class="line">            item_cnt[i] += <span class="number">1</span></span><br><span class="line">            i2i_sim.setdefault(i, &#123;&#125;)</span><br><span class="line">            <span class="keyword">for</span> j, j_click_time <span class="keyword">in</span> item_time_list:</span><br><span class="line">                <span class="keyword">if</span>(i == j):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                i2i_sim[i].setdefault(j, <span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">                i2i_sim[i][j] += <span class="number">1</span> / math.log(<span class="built_in">len</span>(item_time_list) + <span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">    i2i_sim_ = i2i_sim.copy()</span><br><span class="line">    <span class="keyword">for</span> i, related_items <span class="keyword">in</span> i2i_sim.items():</span><br><span class="line">        <span class="keyword">for</span> j, wij <span class="keyword">in</span> related_items.items():</span><br><span class="line">            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将得到的相似性矩阵保存到本地</span></span><br><span class="line">    pickle.dump(i2i_sim_, <span class="built_in">open</span>(save_path + <span class="string">&#x27;itemcf_i2i_sim.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> i2i_sim_</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i2i_sim = itemcf_sim(all_click_df)</span><br></pre></td></tr></table></figure>



<h2 id="itemCF-的文章推荐"><a href="#itemCF-的文章推荐" class="headerlink" title="itemCF 的文章推荐"></a>itemCF 的文章推荐</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基于商品的召回i2i</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">item_based_recommend</span>(<span class="params">user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于文章协同过滤的召回</span></span><br><span class="line"><span class="string">        :param user_id: 用户id</span></span><br><span class="line"><span class="string">        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   &#123;user1: &#123;item1: time1, item2: time2..&#125;...&#125;</span></span><br><span class="line"><span class="string">        :param i2i_sim: 字典，文章相似性矩阵</span></span><br><span class="line"><span class="string">        :param sim_item_topk: 整数， 选择与当前文章最相似的前k篇文章</span></span><br><span class="line"><span class="string">        :param recall_item_num: 整数， 最后的召回文章数量</span></span><br><span class="line"><span class="string">        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全        </span></span><br><span class="line"><span class="string">        return: 召回的文章列表 &#123;item1:score1, item2: score2...&#125;</span></span><br><span class="line"><span class="string">        注意: 基于物品的协同过滤(详细请参考上一期推荐系统基础的组队学习)， 在多路召回部分会加上关联规则的召回策略</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取用户历史交互的文章</span></span><br><span class="line">    user_hist_items = user_item_time_dict[user_id]</span><br><span class="line">    </span><br><span class="line">    item_rank = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> loc, (i, click_time) <span class="keyword">in</span> <span class="built_in">enumerate</span>(user_hist_items):</span><br><span class="line">        <span class="keyword">for</span> j, wij <span class="keyword">in</span> <span class="built_in">sorted</span>(i2i_sim[i].items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:sim_item_topk]:</span><br><span class="line">            <span class="keyword">if</span> j <span class="keyword">in</span> user_hist_items:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">                </span><br><span class="line">            item_rank.setdefault(j, <span class="number">0</span>)</span><br><span class="line">            item_rank[j] +=  wij</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 不足10个，用热门商品补全</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item_rank) &lt; recall_item_num:</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(item_topk_click):</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> item_rank.items(): <span class="comment"># 填充的item应该不在原来的列表中</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            item_rank[item] = - i - <span class="number">100</span> <span class="comment"># 随便给个负数就行</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(item_rank) == recall_item_num:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    item_rank = <span class="built_in">sorted</span>(item_rank.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:recall_item_num]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> item_rank</span><br></pre></td></tr></table></figure>



<h2 id="给每个用户根据物品的协同过滤推荐文章"><a href="#给每个用户根据物品的协同过滤推荐文章" class="headerlink" title="给每个用户根据物品的协同过滤推荐文章"></a>给每个用户根据物品的协同过滤推荐文章</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义</span></span><br><span class="line">user_recall_items_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 用户 - 文章 - 点击时间的字典</span></span><br><span class="line">user_item_time_dict = get_user_item_time(all_click_df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去取文章相似度</span></span><br><span class="line">i2i_sim = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;itemcf_i2i_sim.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相似文章的数量</span></span><br><span class="line">sim_item_topk = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 召回文章数量</span></span><br><span class="line">recall_item_num = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户热度补全</span></span><br><span class="line">item_topk_click = get_item_topk_click(all_click_df, k=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> tqdm(all_click_df[<span class="string">&#x27;user_id&#x27;</span>].unique()):</span><br><span class="line">    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, </span><br><span class="line">                                                        sim_item_topk, recall_item_num, item_topk_click)</span><br></pre></td></tr></table></figure>



<h2 id="召回字典转换成df"><a href="#召回字典转换成df" class="headerlink" title="召回字典转换成df"></a>召回字典转换成df</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将字典的形式转换成df</span></span><br><span class="line">user_item_score_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user, items <span class="keyword">in</span> tqdm(user_recall_items_dict.items()):</span><br><span class="line">    <span class="keyword">for</span> item, score <span class="keyword">in</span> items:</span><br><span class="line">        user_item_score_list.append([user, item, score])</span><br><span class="line"></span><br><span class="line">recall_df = pd.DataFrame(user_item_score_list, columns=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br></pre></td></tr></table></figure>



<h2 id="生成提交文件"><a href="#生成提交文件" class="headerlink" title="生成提交文件"></a>生成提交文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成提交文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submit</span>(<span class="params">recall_df, topk=<span class="number">5</span>, model_name=<span class="literal">None</span></span>):</span></span><br><span class="line">    recall_df = recall_df.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">    recall_df[<span class="string">&#x27;rank&#x27;</span>] = recall_df.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 判断是不是每个用户都有5篇文章及以上</span></span><br><span class="line">    tmp = recall_df.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(<span class="keyword">lambda</span> x: x[<span class="string">&#x27;rank&#x27;</span>].<span class="built_in">max</span>())</span><br><span class="line">    <span class="keyword">assert</span> tmp.<span class="built_in">min</span>() &gt;= topk</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">del</span> recall_df[<span class="string">&#x27;pred_score&#x27;</span>]</span><br><span class="line">    submit = recall_df[recall_df[<span class="string">&#x27;rank&#x27;</span>] &lt;= topk].set_index([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>]).unstack(-<span class="number">1</span>).reset_index()</span><br><span class="line">    </span><br><span class="line">    submit.columns = [<span class="built_in">int</span>(col) <span class="keyword">if</span> <span class="built_in">isinstance</span>(col, <span class="built_in">int</span>) <span class="keyword">else</span> col <span class="keyword">for</span> col <span class="keyword">in</span> submit.columns.droplevel(<span class="number">0</span>)]</span><br><span class="line">    <span class="comment"># 按照提交格式定义列名</span></span><br><span class="line">    submit = submit.rename(columns=&#123;<span class="string">&#x27;&#x27;</span>: <span class="string">&#x27;user_id&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;article_1&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;article_2&#x27;</span>, </span><br><span class="line">                                                  <span class="number">3</span>: <span class="string">&#x27;article_3&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;article_4&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;article_5&#x27;</span>&#125;)</span><br><span class="line">    </span><br><span class="line">    save_name = save_path + model_name + <span class="string">&#x27;_&#x27;</span> + datetime.today().strftime(<span class="string">&#x27;%m-%d&#x27;</span>) + <span class="string">&#x27;.csv&#x27;</span></span><br><span class="line">    submit.to_csv(save_name, index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取测试集</span></span><br><span class="line">tst_click = pd.read_csv(data_path + <span class="string">&#x27;testA_click_log.csv&#x27;</span>)</span><br><span class="line">tst_users = tst_click[<span class="string">&#x27;user_id&#x27;</span>].unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从所有的召回数据中将测试集中的用户选出来</span></span><br><span class="line">tst_recall = recall_df[recall_df[<span class="string">&#x27;user_id&#x27;</span>].isin(tst_users)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成提交文件</span></span><br><span class="line">submit(tst_recall, topk=<span class="number">5</span>, model_name=<span class="string">&#x27;itemcf_baseline&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本节内容主要包括赛题简介，数据概况，评价方式以及对该赛题进行了一个总体上的思路分析，作为竞赛前的预热，旨在帮助学习者们能够更好切入该赛题，为后面的学习内容打下一个良好的基础。最后我们给出了关于本赛题的一个简易Baseline， 帮助学习者们先了解一下新闻推荐比赛的一个整理流程， 接下来我们就对于流程中的每个步骤进行详细的介绍。</p>
<p>今天的学习比较简单，下面整理一下关于赛题理解的一些经验：</p>
<ul>
<li>赛题理解究竟是在理解什么? </li>
</ul>
<blockquote>
<p><strong>理解赛题</strong>：从直观上对问题进行梳理， 分析问题的目标，到底要让做什么事情, <strong>这个非常重要</strong></p>
<p><strong>理解数据</strong>：对赛题数据有一个初步了解，知道和任务相关的数据字段和数据字段的类型， 数据之间的内在关联等，大体梳理一下哪些数据会对我们解决问题非常有用，方便后面我们的数据分析和特征工程。</p>
<p><strong>理解评估指标</strong>：评估指标是检验我们提出的方法，我们给出结果好坏的标准，只有正确的理解了评估指标，我们才能进行更好的训练模型，更好的进行预测。此外，很多情况下，线上验证是有一定的时间和次数限制的，<strong>所以在比赛中构建一个合理的本地的验证集和验证的评价指标是很关键的步骤，能有效的节省很多时间</strong>。 不同的指标对于同样的预测结果是具有误差敏感的差异性的所以不同的评价指标会影响后续一些预测的侧重点。</p>
</blockquote>
<ul>
<li><p>有了赛题理解之后，我们该做什么？</p>
<blockquote>
<p>在对于赛题有了一定的了解后，分析清楚了问题的类型性质和对于数据理解 的这一基础上，我们可以梳理一个解决赛题的一个大题思路和框架</p>
<p>我们至少要有一些相应的理解分析，比如<strong>这题的难点可能在哪里，关键点可能在哪里，哪些地方可以挖掘更好的特征</strong>.</p>
<p>用什么样得线下验证方式更为稳定，<strong>出现了过拟合或者其他问题，估摸可以用什么方法去解决这些问题</strong></p>
</blockquote>
<p>这时是在一个宏观的大体下分析的，有助于摸清整个题的思路脉络，以及后续的分析方向</p>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>天池推荐系统入门赛——特征工程</title>
    <url>/2020/12/02/%E5%A4%A9%E6%B1%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%85%A5%E9%97%A8%E8%B5%9B%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="特征工程-制作特征和标签，-转成监督学习问题"><a href="#特征工程-制作特征和标签，-转成监督学习问题" class="headerlink" title="特征工程(制作特征和标签， 转成监督学习问题)"></a>特征工程(制作特征和标签， 转成监督学习问题)</h1><p>我们先捋一下基于原始的给定数据， 有哪些特征可以直接利用：</p>
<ol>
<li>文章的自身特征， category_id表示这文章的类型， created_at_ts表示文章建立的时间， 这个关系着文章的时效性， words_count是文章的字数， 一般字数太长我们不太喜欢点击, 也不排除有人就喜欢读长文。</li>
<li>文章的内容embedding特征， 这个召回的时候用过， 这里可以选择使用， 也可以选择不用， 也可以尝试其他类型的embedding特征， 比如W2V等</li>
<li>用户的设备特征信息</li>
</ol>
<p>上面这些直接可以用的特征， 待做完特征工程之后， 直接就可以根据article_id或者是user_id把这些特征加入进去。 但是我们需要先基于召回的结果， 构造一些特征，然后制作标签，形成一个监督学习的数据集。<br><br><br>构造监督数据集的思路， 根据召回结果， 我们会得到一个{user_id: [可能点击的文章列表]}形式的字典。 那么我们就可以对于每个用户， 每篇可能点击的文章构造一个监督测试集， 比如对于用户user1， 假设得到的他的召回列表{user1: [item1, item2, item3]}， 我们就可以得到三行数据(user1, item1), (user1, item2), (user1, item3)的形式， 这就是监督测试集时候的前两列特征。<br><br></p>
<p>构造特征的思路是这样， 我们知道每个用户的点击文章是与其历史点击的文章信息是有很大关联的， 比如同一个主题， 相似等等。 所以特征构造这块很重要的一系列特征<strong>是要结合用户的历史点击文章信息</strong>。我们已经得到了每个用户及点击候选文章的两列的一个数据集， 而我们的目的是要预测最后一次点击的文章， 比较自然的一个思路就是和其最后几次点击的文章产生关系， 这样既考虑了其历史点击文章信息， 又得离最后一次点击较近，因为新闻很大的一个特点就是注重时效性。 往往用户的最后一次点击会和其最后几次点击有很大的关联。 所以我们就可以对于每个候选文章， 做出与最后几次点击相关的特征如下：</p>
<ol>
<li>候选item与最后几次点击的相似性特征(embedding内积）  — 这个直接关联用户历史行为</li>
<li>候选item与最后几次点击的相似性特征的统计特征 — 统计特征可以减少一些波动和异常</li>
<li>候选item与最后几次点击文章的字数差的特征 — 可以通过字数看用户偏好</li>
<li>候选item与最后几次点击的文章建立的时间差特征 — 时间差特征可以看出该用户对于文章的实时性的偏好   </li>
</ol>
<p>还需要考虑一下<br><strong>5. 如果使用了youtube召回的话， 我们还可以制作用户与候选item的相似特征</strong></p>
<p>当然， 上面只是提供了一种基于用户历史行为做特征工程的思路， 大家也可以思维风暴一下，尝试一些其他的特征。 下面我们就实现上面的这些特征的制作， 下面的逻辑是这样：</p>
<ol>
<li>我们首先获得用户的最后一次点击操作和用户的历史点击， 这个基于我们的日志数据集做</li>
<li>基于用户的历史行为制作特征， 这个会用到用户的历史点击表， 最后的召回列表， 文章的信息表和embedding向量</li>
<li>制作标签， 形成最后的监督学习数据集</li>
</ol>
<h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> gc, os</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="df节省内存函数"><a href="#df节省内存函数" class="headerlink" title="df节省内存函数"></a>df节省内存函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 节省内存的一个函数</span></span><br><span class="line"><span class="comment"># 减少内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_mem</span>(<span class="params">df</span>):</span></span><br><span class="line">    starttime = time.time()</span><br><span class="line">    numerics = [<span class="string">&#x27;int16&#x27;</span>, <span class="string">&#x27;int32&#x27;</span>, <span class="string">&#x27;int64&#x27;</span>, <span class="string">&#x27;float16&#x27;</span>, <span class="string">&#x27;float32&#x27;</span>, <span class="string">&#x27;float64&#x27;</span>]</span><br><span class="line">    start_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type <span class="keyword">in</span> numerics:</span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">if</span> pd.isnull(c_min) <span class="keyword">or</span> pd.isnull(c_max):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">    end_mem = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    print(<span class="string">&#x27;-- Mem. usage decreased to &#123;:5.2f&#125; Mb (&#123;:.1f&#125;% reduction),time spend:&#123;:2.2f&#125; min&#x27;</span>.<span class="built_in">format</span>(end_mem,</span><br><span class="line">                                                                                                           <span class="number">100</span>*(start_mem-end_mem)/start_mem,</span><br><span class="line">                                                                                                           (time.time()-starttime)/<span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>

<h2 id="定义数据路径"><a href="#定义数据路径" class="headerlink" title="定义数据路径"></a>定义数据路径</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_path = <span class="string">&#x27;./data_raw/&#x27;</span></span><br><span class="line">save_path = <span class="string">&#x27;./temp_results/&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><h3 id="训练和验证集的划分"><a href="#训练和验证集的划分" class="headerlink" title="训练和验证集的划分"></a>训练和验证集的划分</h3><p>划分训练和验证集的原因是为了在线下验证模型参数的好坏，为了完全模拟测试集，我们这里就在训练集中抽取部分用户的所有信息来作为验证集。提前做训练验证集划分的好处就是可以分解制作排序特征时的压力，一次性做整个数据集的排序特征可能时间会比较长。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># all_click_df指的是训练集</span></span><br><span class="line"><span class="comment"># sample_user_nums 采样作为验证集的用户数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trn_val_split</span>(<span class="params">all_click_df, sample_user_nums</span>):</span></span><br><span class="line">    all_click = all_click_df</span><br><span class="line">    all_user_ids = all_click.user_id.unique()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># replace=True表示可以重复抽样，反之不可以</span></span><br><span class="line">    sample_user_ids = np.random.choice(all_user_ids, size=sample_user_nums, replace=<span class="literal">False</span>) </span><br><span class="line">    </span><br><span class="line">    click_val = all_click[all_click[<span class="string">&#x27;user_id&#x27;</span>].isin(sample_user_ids)]</span><br><span class="line">    click_trn = all_click[~all_click[<span class="string">&#x27;user_id&#x27;</span>].isin(sample_user_ids)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将验证集中的最后一次点击给抽取出来作为答案</span></span><br><span class="line">    click_val = click_val.sort_values([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>])</span><br><span class="line">    val_ans = click_val.groupby(<span class="string">&#x27;user_id&#x27;</span>).tail(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    click_val = click_val.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(<span class="keyword">lambda</span> x: x[:-<span class="number">1</span>]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 去除val_ans中某些用户只有一个点击数据的情况，如果该用户只有一个点击数据，又被分到ans中，</span></span><br><span class="line">    <span class="comment"># 那么训练集中就没有这个用户的点击数据，出现用户冷启动问题，给自己模型验证带来麻烦</span></span><br><span class="line">    val_ans = val_ans[val_ans.user_id.isin(click_val.user_id.unique())] <span class="comment"># 保证答案中出现的用户再验证集中还有</span></span><br><span class="line">    click_val = click_val[click_val.user_id.isin(val_ans.user_id.unique())]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> click_trn, click_val, val_ans</span><br></pre></td></tr></table></figure>

<h3 id="获取历史点击和最后一次点击"><a href="#获取历史点击和最后一次点击" class="headerlink" title="获取历史点击和最后一次点击"></a>获取历史点击和最后一次点击</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取当前数据的历史点击和最后一次点击</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_hist_and_last_click</span>(<span class="params">all_click</span>):</span></span><br><span class="line">    all_click = all_click.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>])</span><br><span class="line">    click_last_df = all_click.groupby(<span class="string">&#x27;user_id&#x27;</span>).tail(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hist_func</span>(<span class="params">user_df</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(user_df) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> user_df</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> user_df[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    click_hist_df = all_click.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(hist_func).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> click_hist_df, click_last_df</span><br></pre></td></tr></table></figure>

<h3 id="读取训练、验证及测试集"><a href="#读取训练、验证及测试集" class="headerlink" title="读取训练、验证及测试集"></a>读取训练、验证及测试集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_trn_val_tst_data</span>(<span class="params">data_path, offline=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> offline:</span><br><span class="line">        click_trn_data = pd.read_csv(data_path+<span class="string">&#x27;train_click_log.csv&#x27;</span>)  <span class="comment"># 训练集用户点击日志</span></span><br><span class="line">        click_trn_data = reduce_mem(click_trn_data)</span><br><span class="line">        click_trn, click_val, val_ans = trn_val_split(all_click_df, sample_user_nums)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        click_trn = pd.read_csv(data_path+<span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">        click_trn = reduce_mem(click_trn)</span><br><span class="line">        click_val = <span class="literal">None</span></span><br><span class="line">        val_ans = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    click_tst = pd.read_csv(data_path+<span class="string">&#x27;testA_click_log.csv&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> click_trn, click_val, click_tst, val_ans</span><br></pre></td></tr></table></figure>

<h3 id="读取召回列表"><a href="#读取召回列表" class="headerlink" title="读取召回列表"></a>读取召回列表</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 返回多路召回列表或者单路召回</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_recall_list</span>(<span class="params">save_path, single_recall_model=<span class="literal">None</span>, multi_recall=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> multi_recall:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;final_recall_items_dict.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> single_recall_model == <span class="string">&#x27;i2i_itemcf&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;itemcf_recall_dict.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">elif</span> single_recall_model == <span class="string">&#x27;i2i_emb_itemcf&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;itemcf_emb_dict.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">elif</span> single_recall_model == <span class="string">&#x27;user_cf&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;youtubednn_usercf_dict.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">elif</span> single_recall_model == <span class="string">&#x27;youtubednn&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;youtube_u2i_dict.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h3 id="读取各种Embedding"><a href="#读取各种Embedding" class="headerlink" title="读取各种Embedding"></a>读取各种Embedding</h3><h4 id="Word2Vec训练及gensim的使用"><a href="#Word2Vec训练及gensim的使用" class="headerlink" title="Word2Vec训练及gensim的使用"></a>Word2Vec训练及gensim的使用</h4><p>Word2Vec主要思想是：一个词的上下文可以很好的表达出词的语义。通过无监督学习产生词向量的方式。word2vec中有两个非常经典的模型：skip-gram和cbow。</p>
<ul>
<li>skip-gram：已知中心词预测周围词。</li>
<li>cbow：已知周围词预测中心词。<br><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/Javaimage-20201106225233086.png" alt="image-20201106225233086"></li>
</ul>
<p>在使用gensim训练word2vec的时候，有几个比较重要的参数</p>
<ul>
<li>size: 表示词向量的维度。</li>
<li>window：决定了目标词会与多远距离的上下文产生关系。</li>
<li>sg: 如果是0，则是CBOW模型，是1则是Skip-Gram模型。</li>
<li>workers: 表示训练时候的线程数量</li>
<li>min_count: 设置最小的</li>
<li>iter: 训练时遍历整个数据集的次数</li>
</ul>
<p><strong>注意</strong></p>
<ol>
<li>训练的时候输入的语料库一定要是字符组成的二维数组，如：[[‘北’, ‘京’, ‘你’, ‘好’], [‘上’, ‘海’, ‘你’, ‘好’]]</li>
<li>使用模型的时候有一些默认值，可以通过在Jupyter里面通过<code>Word2Vec??</code>查看</li>
</ol>
<p>下面是个简单的测试样例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from gensim.models import Word2Vec</span><br><span class="line">doc &#x3D; [[&#39;30760&#39;, &#39;157507&#39;],</span><br><span class="line">       [&#39;289197&#39;, &#39;63746&#39;],</span><br><span class="line">       [&#39;36162&#39;, &#39;168401&#39;],</span><br><span class="line">       [&#39;50644&#39;, &#39;36162&#39;]]</span><br><span class="line">w2v &#x3D; Word2Vec(docs, size&#x3D;12, sg&#x3D;1, window&#x3D;2, seed&#x3D;2020, workers&#x3D;2, min_count&#x3D;1, iter&#x3D;1)</span><br><span class="line"></span><br><span class="line"># 查看&#39;30760&#39;表示的词向量</span><br><span class="line">w2v[&#39;30760&#39;]</span><br></pre></td></tr></table></figure>

<p>skip-gram和cbow的详细原理可以参考下面的博客：</p>
<ul>
<li><a href="https://www.cnblogs.com/pinard/p/7160330.html">word2vec原理(一) CBOW与Skip-Gram模型基础</a>   </li>
<li><a href="https://www.cnblogs.com/pinard/p/7160330.html">word2vec原理(二) 基于Hierarchical Softmax的模型</a>   </li>
<li><a href="https://www.cnblogs.com/pinard/p/7249903.html">word2vec原理(三) 基于Negative Sampling的模型</a>   </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trian_item_word2vec</span>(<span class="params">click_df, embed_size=<span class="number">64</span>, save_name=<span class="string">&#x27;item_w2v_emb.pkl&#x27;</span>, split_char=<span class="string">&#x27; &#x27;</span></span>):</span></span><br><span class="line">    click_df = click_df.sort_values(<span class="string">&#x27;click_timestamp&#x27;</span>)</span><br><span class="line">    <span class="comment"># 只有转换成字符串才可以进行训练</span></span><br><span class="line">    click_df[<span class="string">&#x27;click_article_id&#x27;</span>] = click_df[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">    <span class="comment"># 转换成句子的形式</span></span><br><span class="line">    docs = click_df.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;click_article_id&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">list</span>(x)).reset_index()</span><br><span class="line">    docs = docs[<span class="string">&#x27;click_article_id&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了方便查看训练的进度，这里设定一个log信息</span></span><br><span class="line">    logging.basicConfig(<span class="built_in">format</span>=<span class="string">&#x27;%(asctime)s:%(levelname)s:%(message)s&#x27;</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的参数对训练得到的向量影响也很大,默认负采样为5</span></span><br><span class="line">    w2v = Word2Vec(docs, size=<span class="number">16</span>, sg=<span class="number">1</span>, window=<span class="number">5</span>, seed=<span class="number">2020</span>, workers=<span class="number">24</span>, min_count=<span class="number">1</span>, <span class="built_in">iter</span>=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存成字典的形式</span></span><br><span class="line">    item_w2v_emb_dict = &#123;k: w2v[k] <span class="keyword">for</span> k <span class="keyword">in</span> click_df[<span class="string">&#x27;click_article_id&#x27;</span>]&#125;</span><br><span class="line">    pickle.dump(item_w2v_emb_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;item_w2v_emb.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> item_w2v_emb_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以通过字典查询对应的item的Embedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_embedding</span>(<span class="params">save_path, all_click_df</span>):</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(save_path + <span class="string">&#x27;item_content_emb.pkl&#x27;</span>):</span><br><span class="line">        item_content_emb_dict = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;item_content_emb.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&#x27;item_content_emb.pkl 文件不存在...&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># w2v Embedding是需要提前训练好的</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(save_path + <span class="string">&#x27;item_w2v_emb.pkl&#x27;</span>):</span><br><span class="line">        item_w2v_emb_dict = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;item_w2v_emb.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        item_w2v_emb_dict = trian_item_word2vec(all_click_df)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> os.path.exists(save_path + <span class="string">&#x27;item_youtube_emb.pkl&#x27;</span>):</span><br><span class="line">        item_youtube_emb_dict = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;item_youtube_emb.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&#x27;item_youtube_emb.pkl 文件不存在...&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> os.path.exists(save_path + <span class="string">&#x27;user_youtube_emb.pkl&#x27;</span>):</span><br><span class="line">        user_youtube_emb_dict = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;user_youtube_emb.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&#x27;user_youtube_emb.pkl 文件不存在...&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> item_content_emb_dict, item_w2v_emb_dict, item_youtube_emb_dict, user_youtube_emb_dict</span><br></pre></td></tr></table></figure>

<h3 id="读取文章信息"><a href="#读取文章信息" class="headerlink" title="读取文章信息"></a>读取文章信息</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_article_info_df</span>():</span></span><br><span class="line">    article_info_df = pd.read_csv(data_path + <span class="string">&#x27;articles.csv&#x27;</span>)</span><br><span class="line">    article_info_df = reduce_mem(article_info_df)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> article_info_df</span><br></pre></td></tr></table></figure>

<h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里offline的online的区别就是验证集是否为空</span></span><br><span class="line">click_trn, click_val, click_tst, val_ans = get_trn_val_tst_data(data_path, offline=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>-- Mem. usage decreased to 23.34 Mb (69.4% reduction),time spend:0.00 min</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">click_trn_hist, click_trn_last = get_hist_and_last_click(click_trn)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> click_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    click_val_hist, click_val_last = click_val, val_ans</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    click_val_hist, click_val_last = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">click_tst_hist = click_tst</span><br></pre></td></tr></table></figure>

<h2 id="对训练数据做负采样"><a href="#对训练数据做负采样" class="headerlink" title="对训练数据做负采样"></a>对训练数据做负采样</h2><p>通过召回我们将数据转换成三元组的形式（user1, item1, label）的形式，观察发现正负样本差距极度不平衡，我们可以先对负样本进行下采样，下采样的目的一方面缓解了正负样本比例的问题，另一方面也减小了我们做排序特征的压力，我们在做负采样的时候又有哪些东西是需要注意的呢？</p>
<ol>
<li>只对负样本进行下采样(如果有比较好的正样本扩充的方法其实也是可以考虑的)</li>
<li>负采样之后，保证所有的用户和文章仍然出现在采样之后的数据中</li>
<li>下采样的比例可以根据实际情况人为的控制</li>
<li>做完负采样之后，更新此时新的用户召回文章列表，因为后续做特征的时候可能用到相对位置的信息。</li>
</ol>
<p>其实负采样也可以留在后面做完特征在进行，这里由于做排序特征太慢了，所以把负采样的环节提到前面了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将召回列表转换成df的形式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recall_dict_2_df</span>(<span class="params">recall_list_dict</span>):</span></span><br><span class="line">    df_row_list = [] <span class="comment"># [user, item, score]</span></span><br><span class="line">    <span class="keyword">for</span> user, recall_list <span class="keyword">in</span> tqdm(recall_list_dict.items()):</span><br><span class="line">        <span class="keyword">for</span> item, score <span class="keyword">in</span> recall_list:</span><br><span class="line">            df_row_list.append([user, item, score])</span><br><span class="line">    </span><br><span class="line">    col_names = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;sim_item&#x27;</span>, <span class="string">&#x27;score&#x27;</span>]</span><br><span class="line">    recall_list_df = pd.DataFrame(df_row_list, columns=col_names)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> recall_list_df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 负采样函数，这里可以控制负采样时的比例, 这里给了一个默认的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_sample_recall_data</span>(<span class="params">recall_items_df, sample_rate=<span class="number">0.001</span></span>):</span></span><br><span class="line">    pos_data = recall_items_df[recall_items_df[<span class="string">&#x27;label&#x27;</span>] == <span class="number">1</span>]</span><br><span class="line">    neg_data = recall_items_df[recall_items_df[<span class="string">&#x27;label&#x27;</span>] == <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">&#x27;pos_data_num:&#x27;</span>, <span class="built_in">len</span>(pos_data), <span class="string">&#x27;neg_data_num:&#x27;</span>, <span class="built_in">len</span>(neg_data), <span class="string">&#x27;pos/neg:&#x27;</span>, <span class="built_in">len</span>(pos_data)/<span class="built_in">len</span>(neg_data))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分组采样函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_sample_func</span>(<span class="params">group_df</span>):</span></span><br><span class="line">        neg_num = <span class="built_in">len</span>(group_df)</span><br><span class="line">        sample_num = <span class="built_in">max</span>(<span class="built_in">int</span>(neg_num * sample_rate), <span class="number">1</span>) <span class="comment"># 保证最少有一个</span></span><br><span class="line">        sample_num = <span class="built_in">min</span>(sample_num, <span class="number">5</span>) <span class="comment"># 保证最多不超过5个，这里可以根据实际情况进行选择</span></span><br><span class="line">        <span class="keyword">return</span> group_df.sample(n=sample_num, replace=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对用户进行负采样，保证所有用户都在采样后的数据中</span></span><br><span class="line">    neg_data_user_sample = neg_data.groupby(<span class="string">&#x27;user_id&#x27;</span>, group_keys=<span class="literal">False</span>).apply(neg_sample_func)</span><br><span class="line">    <span class="comment"># 对文章进行负采样，保证所有文章都在采样后的数据中</span></span><br><span class="line">    neg_data_item_sample = neg_data.groupby(<span class="string">&#x27;sim_item&#x27;</span>, group_keys=<span class="literal">False</span>).apply(neg_sample_func)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将上述两种情况下的采样数据合并</span></span><br><span class="line">    neg_data_new = neg_data_user_sample.append(neg_data_item_sample)</span><br><span class="line">    <span class="comment"># 由于上述两个操作是分开的，可能将两个相同的数据给重复选择了，所以需要对合并后的数据进行去重</span></span><br><span class="line">    neg_data_new = neg_data_new.sort_values([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;score&#x27;</span>]).drop_duplicates([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;sim_item&#x27;</span>], keep=<span class="string">&#x27;last&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将正样本数据合并</span></span><br><span class="line">    data_new = pd.concat([pos_data, neg_data_new], ignore_index=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data_new</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 召回数据打标签</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_rank_label_df</span>(<span class="params">recall_list_df, label_df, is_test=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="comment"># 测试集是没有标签了，为了后面代码同一一些，这里直接给一个负数替代</span></span><br><span class="line">    <span class="keyword">if</span> is_test:</span><br><span class="line">        recall_list_df[<span class="string">&#x27;label&#x27;</span>] = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> recall_list_df</span><br><span class="line">    </span><br><span class="line">    label_df = label_df.rename(columns=&#123;<span class="string">&#x27;click_article_id&#x27;</span>: <span class="string">&#x27;sim_item&#x27;</span>&#125;)</span><br><span class="line">    recall_list_df_ = recall_list_df.merge(label_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;sim_item&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>]], \</span><br><span class="line">                                               how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;sim_item&#x27;</span>])</span><br><span class="line">    recall_list_df_[<span class="string">&#x27;label&#x27;</span>] = recall_list_df_[<span class="string">&#x27;click_timestamp&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">0.0</span> <span class="keyword">if</span> np.isnan(x) <span class="keyword">else</span> <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">del</span> recall_list_df_[<span class="string">&#x27;click_timestamp&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> recall_list_df_</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user_recall_item_label_df</span>(<span class="params">click_trn_hist, click_val_hist, click_tst_hist,click_trn_last, click_val_last, recall_list_df</span>):</span></span><br><span class="line">    <span class="comment"># 获取训练数据的召回列表</span></span><br><span class="line">    trn_user_items_df = recall_list_df[recall_list_df[<span class="string">&#x27;user_id&#x27;</span>].isin(click_trn_hist[<span class="string">&#x27;user_id&#x27;</span>].unique())]</span><br><span class="line">    <span class="comment"># 训练数据打标签</span></span><br><span class="line">    trn_user_item_label_df = get_rank_label_df(trn_user_items_df, click_trn_last, is_test=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 训练数据负采样</span></span><br><span class="line">    trn_user_item_label_df = neg_sample_recall_data(trn_user_item_label_df)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> click_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        val_user_items_df = recall_list_df[recall_list_df[<span class="string">&#x27;user_id&#x27;</span>].isin(click_val_hist[<span class="string">&#x27;user_id&#x27;</span>].unique())]</span><br><span class="line">        val_user_item_label_df = get_rank_label_df(val_user_items_df, click_val_last, is_test=<span class="literal">False</span>)</span><br><span class="line">        val_user_item_label_df = neg_sample_recall_data(val_user_item_label_df)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        val_user_item_label_df = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 测试数据不需要进行负采样，直接对所有的召回商品进行打-1标签</span></span><br><span class="line">    tst_user_items_df = recall_list_df[recall_list_df[<span class="string">&#x27;user_id&#x27;</span>].isin(click_tst_hist[<span class="string">&#x27;user_id&#x27;</span>].unique())]</span><br><span class="line">    tst_user_item_label_df = get_rank_label_df(tst_user_items_df, <span class="literal">None</span>, is_test=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取召回列表</span></span><br><span class="line">recall_list_dict = get_recall_list(save_path, single_recall_model=<span class="string">&#x27;i2i_itemcf&#x27;</span>) <span class="comment"># 这里只选择了单路召回的结果，也可以选择多路召回结果</span></span><br><span class="line"><span class="comment"># 将召回数据转换成df</span></span><br><span class="line">recall_list_df = recall_dict_2_df(recall_list_dict)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [00:12&lt;00:00, 20689.39it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给训练验证数据打标签，并负采样（这一部分时间比较久）</span></span><br><span class="line">trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df = get_user_recall_item_label_df(click_trn_hist, </span><br><span class="line">                                                                                                       click_val_hist, </span><br><span class="line">                                                                                                       click_tst_hist,</span><br><span class="line">                                                                                                       click_trn_last, </span><br><span class="line">                                                                                                       click_val_last, </span><br><span class="line">                                                                                                       recall_list_df)</span><br></pre></td></tr></table></figure>

<pre><code>pos_data_num: 64190 neg_data_num: 1935810 pos/neg: 0.03315924600038227</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_user_item_label_df.label</span><br></pre></td></tr></table></figure>

<h2 id="将召回数据转换成字典"><a href="#将召回数据转换成字典" class="headerlink" title="将召回数据转换成字典"></a>将召回数据转换成字典</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将最终的召回的df数据转换成字典的形式做排序特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_tuple_func</span>(<span class="params">group_df</span>):</span></span><br><span class="line">    row_data = []</span><br><span class="line">    <span class="keyword">for</span> name, row_df <span class="keyword">in</span> group_df.iterrows():</span><br><span class="line">        row_data.append((row_df[<span class="string">&#x27;sim_item&#x27;</span>], row_df[<span class="string">&#x27;score&#x27;</span>], row_df[<span class="string">&#x27;label&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> row_data</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_user_item_label_tuples = trn_user_item_label_df.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(make_tuple_func).reset_index()</span><br><span class="line">trn_user_item_label_tuples_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(trn_user_item_label_tuples[<span class="string">&#x27;user_id&#x27;</span>], trn_user_item_label_tuples[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> val_user_item_label_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    val_user_item_label_tuples = val_user_item_label_df.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(make_tuple_func).reset_index()</span><br><span class="line">    val_user_item_label_tuples_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(val_user_item_label_tuples[<span class="string">&#x27;user_id&#x27;</span>], val_user_item_label_tuples[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_label_tuples_dict = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">tst_user_item_label_tuples = tst_user_item_label_df.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(make_tuple_func).reset_index()</span><br><span class="line">tst_user_item_label_tuples_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(tst_user_item_label_tuples[<span class="string">&#x27;user_id&#x27;</span>], tst_user_item_label_tuples[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<h2 id="用户历史行为相关特征"><a href="#用户历史行为相关特征" class="headerlink" title="用户历史行为相关特征"></a>用户历史行为相关特征</h2><p>对于每个用户召回的每个商品， 做特征。 具体步骤如下：</p>
<ul>
<li>对于每个用户， 获取最后点击的N个商品的item_id， <ul>
<li>对于该用户的每个召回商品， 计算与上面最后N次点击商品的相似度的和(最大， 最小，均值)， 时间差特征，相似性特征，字数差特征，与该用户的相似性特征</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下面基于data做历史相关的特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_feature</span>(<span class="params">users_id, recall_list, click_hist_df,  articles_info, articles_emb, user_emb=<span class="literal">None</span>, N=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于用户的历史行为做相关特征</span></span><br><span class="line"><span class="string">    :param users_id: 用户id</span></span><br><span class="line"><span class="string">    :param recall_list: 对于每个用户召回的候选文章列表</span></span><br><span class="line"><span class="string">    :param click_hist_df: 用户的历史点击信息</span></span><br><span class="line"><span class="string">    :param articles_info: 文章信息</span></span><br><span class="line"><span class="string">    :param articles_emb: 文章的embedding向量, 这个可以用item_content_emb, item_w2v_emb, item_youtube_emb</span></span><br><span class="line"><span class="string">    :param user_emb: 用户的embedding向量， 这个是user_youtube_emb, 如果没有也可以不用， 但要注意如果要用的话， articles_emb就要用item_youtube_emb的形式， 这样维度才一样</span></span><br><span class="line"><span class="string">    :param N: 最近的N次点击  由于testA日志里面很多用户只存在一次历史点击， 所以为了不产生空值，默认是1</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 建立一个二维列表保存结果， 后面要转成DataFrame</span></span><br><span class="line">    all_user_feas = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user_id <span class="keyword">in</span> tqdm(users_id):</span><br><span class="line">        <span class="comment"># 该用户的最后N次点击</span></span><br><span class="line">        hist_user_items = click_hist_df[click_hist_df[<span class="string">&#x27;user_id&#x27;</span>]==user_id][<span class="string">&#x27;click_article_id&#x27;</span>][-N:]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历该用户的召回列表</span></span><br><span class="line">        <span class="keyword">for</span> rank, (article_id, score, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(recall_list[user_id]):</span><br><span class="line">            <span class="comment"># 该文章建立时间, 字数</span></span><br><span class="line">            a_create_time = articles_info[articles_info[<span class="string">&#x27;article_id&#x27;</span>]==article_id][<span class="string">&#x27;created_at_ts&#x27;</span>].values[<span class="number">0</span>]</span><br><span class="line">            a_words_count = articles_info[articles_info[<span class="string">&#x27;article_id&#x27;</span>]==article_id][<span class="string">&#x27;words_count&#x27;</span>].values[<span class="number">0</span>]</span><br><span class="line">            single_user_fea = [user_id, article_id]</span><br><span class="line">            <span class="comment"># 计算与最后点击的商品的相似度的和， 最大值和最小值， 均值</span></span><br><span class="line">            sim_fea = []</span><br><span class="line">            time_fea = []</span><br><span class="line">            word_fea = []</span><br><span class="line">            <span class="comment"># 遍历用户的最后N次点击文章</span></span><br><span class="line">            <span class="keyword">for</span> hist_item <span class="keyword">in</span> hist_user_items:</span><br><span class="line">                b_create_time = articles_info[articles_info[<span class="string">&#x27;article_id&#x27;</span>]==hist_item][<span class="string">&#x27;created_at_ts&#x27;</span>].values[<span class="number">0</span>]</span><br><span class="line">                b_words_count = articles_info[articles_info[<span class="string">&#x27;article_id&#x27;</span>]==hist_item][<span class="string">&#x27;words_count&#x27;</span>].values[<span class="number">0</span>]</span><br><span class="line">                </span><br><span class="line">                sim_fea.append(np.dot(articles_emb[hist_item], articles_emb[article_id]))</span><br><span class="line">                time_fea.append(<span class="built_in">abs</span>(a_create_time-b_create_time))</span><br><span class="line">                word_fea.append(<span class="built_in">abs</span>(a_words_count-b_words_count))</span><br><span class="line">                </span><br><span class="line">            single_user_fea.extend(sim_fea)      <span class="comment"># 相似性特征</span></span><br><span class="line">            single_user_fea.extend(time_fea)    <span class="comment"># 时间差特征</span></span><br><span class="line">            single_user_fea.extend(word_fea)    <span class="comment"># 字数差特征</span></span><br><span class="line">            single_user_fea.extend([<span class="built_in">max</span>(sim_fea), <span class="built_in">min</span>(sim_fea), <span class="built_in">sum</span>(sim_fea), <span class="built_in">sum</span>(sim_fea) / <span class="built_in">len</span>(sim_fea)])  <span class="comment"># 相似性的统计特征</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> user_emb:  <span class="comment"># 如果用户向量有的话， 这里计算该召回文章与用户的相似性特征 </span></span><br><span class="line">                single_user_fea.append(np.dot(user_emb[user_id], articles_emb[article_id]))</span><br><span class="line">                </span><br><span class="line">            single_user_fea.extend([score, rank, label])    </span><br><span class="line">            <span class="comment"># 加入到总的表中</span></span><br><span class="line">            all_user_feas.append(single_user_fea)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义列名</span></span><br><span class="line">    id_cols = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>]</span><br><span class="line">    sim_cols = [<span class="string">&#x27;sim&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">    time_cols = [<span class="string">&#x27;time_diff&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">    word_cols = [<span class="string">&#x27;word_diff&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">    sat_cols = [<span class="string">&#x27;sim_max&#x27;</span>, <span class="string">&#x27;sim_min&#x27;</span>, <span class="string">&#x27;sim_sum&#x27;</span>, <span class="string">&#x27;sim_mean&#x27;</span>]</span><br><span class="line">    user_item_sim_cols = [<span class="string">&#x27;user_item_sim&#x27;</span>] <span class="keyword">if</span> user_emb <span class="keyword">else</span> []</span><br><span class="line">    user_score_rank_label = [<span class="string">&#x27;score&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    cols = id_cols + sim_cols + time_cols + word_cols + sat_cols + user_item_sim_cols + user_score_rank_label</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 转成DataFrame</span></span><br><span class="line">    df = pd.DataFrame( all_user_feas, columns=cols)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">article_info_df = get_article_info_df()</span><br><span class="line">all_click = click_trn.append(click_tst)</span><br><span class="line">item_content_emb_dict, item_w2v_emb_dict, item_youtube_emb_dict, user_youtube_emb_dict = get_embedding(save_path, all_click)</span><br></pre></td></tr></table></figure>

<pre><code>-- Mem. usage decreased to  5.56 Mb (50.0% reduction),time spend:0.00 min</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取训练验证及测试数据中召回列文章相关特征</span></span><br><span class="line">trn_user_item_feats_df = create_feature(trn_user_item_label_tuples_dict.keys(), trn_user_item_label_tuples_dict, \</span><br><span class="line">                                            click_trn_hist, article_info_df, item_content_emb_dict)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> val_user_item_label_tuples_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    val_user_item_feats_df = create_feature(val_user_item_label_tuples_dict.keys(), val_user_item_label_tuples_dict, \</span><br><span class="line">                                                click_val_hist, article_info_df, item_content_emb_dict)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">tst_user_item_feats_df = create_feature(tst_user_item_label_tuples_dict.keys(), tst_user_item_label_tuples_dict, \</span><br><span class="line">                                            click_tst_hist, article_info_df, item_content_emb_dict)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 200000/200000 [50:16&lt;00:00, 66.31it/s] 
100%|██████████| 50000/50000 [1:07:21&lt;00:00, 12.37it/s]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存一份省的每次都要重新跑，每次跑的时间都比较长</span></span><br><span class="line">trn_user_item_feats_df.to_csv(save_path + <span class="string">&#x27;trn_user_item_feats_df.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> val_user_item_feats_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    val_user_item_feats_df.to_csv(save_path + <span class="string">&#x27;val_user_item_feats_df.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">tst_user_item_feats_df.to_csv(save_path + <span class="string">&#x27;tst_user_item_feats_df.csv&#x27;</span>, index=<span class="literal">False</span>)    </span><br></pre></td></tr></table></figure>

<h2 id="用户和文章特征"><a href="#用户和文章特征" class="headerlink" title="用户和文章特征"></a>用户和文章特征</h2><h3 id="用户相关特征"><a href="#用户相关特征" class="headerlink" title="用户相关特征"></a>用户相关特征</h3><p>这一块，正式进行特征工程，既要拼接上已有的特征， 也会做更多的特征出来，我们来梳理一下已有的特征和可构造特征：</p>
<ol>
<li>文章自身的特征， 文章字数，文章创建时间， 文章的embedding （articles表中)</li>
<li>用户点击环境特征， 那些设备的特征(这个在df中)</li>
<li>对于用户和商品还可以构造的特征：<ul>
<li>基于用户的点击文章次数和点击时间构造可以表现用户活跃度的特征</li>
<li>基于文章被点击次数和时间构造可以反映文章热度的特征</li>
<li>用户的时间统计特征： 根据其点击的历史文章列表的点击时间和文章的创建时间做统计特征，比如求均值， 这个可以反映用户对于文章时效的偏好</li>
<li>用户的主题爱好特征， 对于用户点击的历史文章主题进行一个统计， 然后对于当前文章看看是否属于用户已经点击过的主题</li>
<li>用户的字数爱好特征， 对于用户点击的历史文章的字数统计， 求一个均值</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">click_tst.head()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取文章特征</span></span><br><span class="line">articles =  pd.read_csv(data_path+<span class="string">&#x27;articles.csv&#x27;</span>)</span><br><span class="line">articles = reduce_mem(articles)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志数据，就是前面的所有数据</span></span><br><span class="line"><span class="keyword">if</span> click_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    all_data = click_trn.append(click_val)</span><br><span class="line">all_data = click_trn.append(click_tst)</span><br><span class="line">all_data = reduce_mem(all_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼上文章信息</span></span><br><span class="line">all_data = all_data.merge(articles, left_on=<span class="string">&#x27;click_article_id&#x27;</span>, right_on=<span class="string">&#x27;article_id&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_data.shape</span><br></pre></td></tr></table></figure>

<h4 id="分析一下点击时间和点击文章的次数，区分用户活跃度"><a href="#分析一下点击时间和点击文章的次数，区分用户活跃度" class="headerlink" title="分析一下点击时间和点击文章的次数，区分用户活跃度"></a>分析一下点击时间和点击文章的次数，区分用户活跃度</h4><p>如果某个用户点击文章之间的时间间隔比较小， 同时点击的文章次数很多的话， 那么我们认为这种用户一般就是活跃用户, 当然衡量用户活跃度的方式可能多种多样， 这里我们只提供其中一种，我们写一个函数， 得到可以衡量用户活跃度的特征，逻辑如下：</p>
<ol>
<li>首先根据用户user_id分组， 对于每个用户，计算点击文章的次数， 两两点击文章时间间隔的均值</li>
<li>把点击次数取倒数和时间间隔的均值统一归一化，然后两者相加合并，该值越小， 说明用户越活跃</li>
<li>注意， 上面两两点击文章的时间间隔均值， 会出现如果用户只点击了一次的情况，这时候时间间隔均值那里会出现空值， 对于这种情况最后特征那里给个大数进行区分</li>
</ol>
<p>这个的衡量标准就是先把点击的次数取到数然后归一化， 然后点击的时间差归一化， 然后两者相加进行合并， 该值越小， 说明被点击的次数越多， 且间隔时间短。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">active_level</span>(<span class="params">all_data, cols</span>):</span></span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   制作区分用户活跃度的特征</span></span><br><span class="line"><span class="string">   :param all_data: 数据集</span></span><br><span class="line"><span class="string">   :param cols: 用到的特征列</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   data = all_data[cols]</span><br><span class="line">   data.sort_values([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">   user_act = pd.DataFrame(data.groupby(<span class="string">&#x27;user_id&#x27;</span>, as_index=<span class="literal">False</span>)[[<span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>]].\</span><br><span class="line">                           agg(&#123;<span class="string">&#x27;click_article_id&#x27;</span>:np.size, <span class="string">&#x27;click_timestamp&#x27;</span>: &#123;<span class="built_in">list</span>&#125;&#125;).values, columns=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_size&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>])</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 计算时间间隔的均值</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">time_diff_mean</span>(<span class="params">l</span>):</span></span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">len</span>(l) == <span class="number">1</span>:</span><br><span class="line">           <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           <span class="keyword">return</span> np.mean([j-i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(l[:-<span class="number">1</span>], l[<span class="number">1</span>:]))])</span><br><span class="line">       </span><br><span class="line">   user_act[<span class="string">&#x27;time_diff_mean&#x27;</span>] = user_act[<span class="string">&#x27;click_timestamp&#x27;</span>].apply(<span class="keyword">lambda</span> x: time_diff_mean(x))</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 点击次数取倒数</span></span><br><span class="line">   user_act[<span class="string">&#x27;click_size&#x27;</span>] = <span class="number">1</span> / user_act[<span class="string">&#x27;click_size&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 两者归一化</span></span><br><span class="line">   user_act[<span class="string">&#x27;click_size&#x27;</span>] = (user_act[<span class="string">&#x27;click_size&#x27;</span>] - user_act[<span class="string">&#x27;click_size&#x27;</span>].<span class="built_in">min</span>()) / (user_act[<span class="string">&#x27;click_size&#x27;</span>].<span class="built_in">max</span>() - user_act[<span class="string">&#x27;click_size&#x27;</span>].<span class="built_in">min</span>())</span><br><span class="line">   user_act[<span class="string">&#x27;time_diff_mean&#x27;</span>] = (user_act[<span class="string">&#x27;time_diff_mean&#x27;</span>] - user_act[<span class="string">&#x27;time_diff_mean&#x27;</span>].<span class="built_in">min</span>()) / (user_act[<span class="string">&#x27;time_diff_mean&#x27;</span>].<span class="built_in">max</span>() - user_act[<span class="string">&#x27;time_diff_mean&#x27;</span>].<span class="built_in">min</span>())     </span><br><span class="line">   user_act[<span class="string">&#x27;active_level&#x27;</span>] = user_act[<span class="string">&#x27;click_size&#x27;</span>] + user_act[<span class="string">&#x27;time_diff_mean&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">   user_act[<span class="string">&#x27;user_id&#x27;</span>] = user_act[<span class="string">&#x27;user_id&#x27;</span>].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line">   <span class="keyword">del</span> user_act[<span class="string">&#x27;click_timestamp&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">   <span class="keyword">return</span> user_act</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_act_fea = active_level(all_data, [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_act_fea.head()</span><br></pre></td></tr></table></figure>

<h4 id="分析一下点击时间和被点击文章的次数，-衡量文章热度特征"><a href="#分析一下点击时间和被点击文章的次数，-衡量文章热度特征" class="headerlink" title="分析一下点击时间和被点击文章的次数， 衡量文章热度特征"></a>分析一下点击时间和被点击文章的次数， 衡量文章热度特征</h4><p>和上面同样的思路， 如果一篇文章在很短的时间间隔之内被点击了很多次， 说明文章比较热门，实现的逻辑和上面的基本一致， 只不过这里是按照点击的文章进行分组：</p>
<ol>
<li>根据文章进行分组， 对于每篇文章的用户， 计算点击的时间间隔</li>
<li>将用户的数量取倒数， 然后用户的数量和时间间隔归一化， 然后相加得到热度特征， 该值越小， 说明被点击的次数越大且时间间隔越短， 文章比较热</li>
</ol>
<p>当然， 这只是给出一种判断文章热度的一种方法， 这里大家也可以头脑风暴一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hot_level</span>(<span class="params">all_data, cols</span>):</span></span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   制作衡量文章热度的特征</span></span><br><span class="line"><span class="string">   :param all_data: 数据集</span></span><br><span class="line"><span class="string">   :param cols: 用到的特征列</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   data = all_data[cols]</span><br><span class="line">   data.sort_values([<span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">   article_hot = pd.DataFrame(data.groupby(<span class="string">&#x27;click_article_id&#x27;</span>, as_index=<span class="literal">False</span>)[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>]].\</span><br><span class="line">                              agg(&#123;<span class="string">&#x27;user_id&#x27;</span>:np.size, <span class="string">&#x27;click_timestamp&#x27;</span>: &#123;<span class="built_in">list</span>&#125;&#125;).values, columns=[<span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;user_num&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>])</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 计算被点击时间间隔的均值</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">time_diff_mean</span>(<span class="params">l</span>):</span></span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">len</span>(l) == <span class="number">1</span>:</span><br><span class="line">           <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           <span class="keyword">return</span> np.mean([j-i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(l[:-<span class="number">1</span>], l[<span class="number">1</span>:]))])</span><br><span class="line">       </span><br><span class="line">   article_hot[<span class="string">&#x27;time_diff_mean&#x27;</span>] = article_hot[<span class="string">&#x27;click_timestamp&#x27;</span>].apply(<span class="keyword">lambda</span> x: time_diff_mean(x))</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 点击次数取倒数</span></span><br><span class="line">   article_hot[<span class="string">&#x27;user_num&#x27;</span>] = <span class="number">1</span> / article_hot[<span class="string">&#x27;user_num&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 两者归一化</span></span><br><span class="line">   article_hot[<span class="string">&#x27;user_num&#x27;</span>] = (article_hot[<span class="string">&#x27;user_num&#x27;</span>] - article_hot[<span class="string">&#x27;user_num&#x27;</span>].<span class="built_in">min</span>()) / (article_hot[<span class="string">&#x27;user_num&#x27;</span>].<span class="built_in">max</span>() - article_hot[<span class="string">&#x27;user_num&#x27;</span>].<span class="built_in">min</span>())</span><br><span class="line">   article_hot[<span class="string">&#x27;time_diff_mean&#x27;</span>] = (article_hot[<span class="string">&#x27;time_diff_mean&#x27;</span>] - article_hot[<span class="string">&#x27;time_diff_mean&#x27;</span>].<span class="built_in">min</span>()) / (article_hot[<span class="string">&#x27;time_diff_mean&#x27;</span>].<span class="built_in">max</span>() - article_hot[<span class="string">&#x27;time_diff_mean&#x27;</span>].<span class="built_in">min</span>())     </span><br><span class="line">   article_hot[<span class="string">&#x27;hot_level&#x27;</span>] = article_hot[<span class="string">&#x27;user_num&#x27;</span>] + article_hot[<span class="string">&#x27;time_diff_mean&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">   article_hot[<span class="string">&#x27;click_article_id&#x27;</span>] = article_hot[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="keyword">del</span> article_hot[<span class="string">&#x27;click_timestamp&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">   <span class="keyword">return</span> article_hot</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">article_hot_fea = hot_level(all_data, [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>])    </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">article_hot_fea.head()</span><br></pre></td></tr></table></figure>

<h4 id="用户的系列习惯"><a href="#用户的系列习惯" class="headerlink" title="用户的系列习惯"></a>用户的系列习惯</h4><p>这个基于原来的日志表做一个类似于article的那种DataFrame， 存放用户特有的信息, 主要包括点击习惯， 爱好特征之类的</p>
<ul>
<li>用户的设备习惯， 这里取最常用的设备（众数）</li>
<li>用户的时间习惯： 根据其点击过得历史文章的时间来做一个统计（这个感觉最好是把时间戳里的时间特征的h特征提出来，看看用户习惯一天的啥时候点击文章）， 但这里先用转换的时间吧， 求个均值</li>
<li>用户的爱好特征， 对于用户点击的历史文章主题进行用户的爱好判别， 更偏向于哪几个主题， 这个最好是multi-hot进行编码， 先试试行不</li>
<li>用户文章的字数差特征， 用户的爱好文章的字数习惯</li>
</ul>
<p>这些就是对用户进行分组， 然后统计即可</p>
<h4 id="用户的设备习惯"><a href="#用户的设备习惯" class="headerlink" title="用户的设备习惯"></a>用户的设备习惯</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">device_fea</span>(<span class="params">all_data, cols</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    制作用户的设备特征</span></span><br><span class="line"><span class="string">    :param all_data: 数据集</span></span><br><span class="line"><span class="string">    :param cols: 用到的特征列</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    user_device_info = all_data[cols]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用众数来表示每个用户的设备信息</span></span><br><span class="line">    user_device_info = user_device_info.groupby(<span class="string">&#x27;user_id&#x27;</span>).agg(<span class="keyword">lambda</span> x: x.value_counts().index[<span class="number">0</span>]).reset_index()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> user_device_info</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设备特征(这里时间会比较长)</span></span><br><span class="line">device_cols = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_environment&#x27;</span>, <span class="string">&#x27;click_deviceGroup&#x27;</span>, <span class="string">&#x27;click_os&#x27;</span>, <span class="string">&#x27;click_country&#x27;</span>, <span class="string">&#x27;click_region&#x27;</span>, <span class="string">&#x27;click_referrer_type&#x27;</span>]</span><br><span class="line">user_device_info = device_fea(all_data, device_cols)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_device_info.head()</span><br></pre></td></tr></table></figure>

<h4 id="用户的时间习惯"><a href="#用户的时间习惯" class="headerlink" title="用户的时间习惯"></a>用户的时间习惯</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_time_hob_fea</span>(<span class="params">all_data, cols</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    制作用户的时间习惯特征</span></span><br><span class="line"><span class="string">    :param all_data: 数据集</span></span><br><span class="line"><span class="string">    :param cols: 用到的特征列</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    user_time_hob_info = all_data[cols]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 先把时间戳进行归一化</span></span><br><span class="line">    mm = MinMaxScaler()</span><br><span class="line">    user_time_hob_info[<span class="string">&#x27;click_timestamp&#x27;</span>] = mm.fit_transform(user_time_hob_info[[<span class="string">&#x27;click_timestamp&#x27;</span>]])</span><br><span class="line">    user_time_hob_info[<span class="string">&#x27;created_at_ts&#x27;</span>] = mm.fit_transform(user_time_hob_info[[<span class="string">&#x27;created_at_ts&#x27;</span>]])</span><br><span class="line"></span><br><span class="line">    user_time_hob_info = user_time_hob_info.groupby(<span class="string">&#x27;user_id&#x27;</span>).agg(<span class="string">&#x27;mean&#x27;</span>).reset_index()</span><br><span class="line">    </span><br><span class="line">    user_time_hob_info.rename(columns=&#123;<span class="string">&#x27;click_timestamp&#x27;</span>: <span class="string">&#x27;user_time_hob1&#x27;</span>, <span class="string">&#x27;created_at_ts&#x27;</span>: <span class="string">&#x27;user_time_hob2&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> user_time_hob_info</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_time_hob_cols = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>, <span class="string">&#x27;created_at_ts&#x27;</span>]</span><br><span class="line">user_time_hob_info = user_time_hob_fea(all_data, user_time_hob_cols)</span><br></pre></td></tr></table></figure>

<h4 id="用户的主题爱好"><a href="#用户的主题爱好" class="headerlink" title="用户的主题爱好"></a>用户的主题爱好</h4><p>这里先把用户点击的文章属于的主题转成一个列表， 后面再总的汇总的时候单独制作一个特征， 就是文章的主题如果属于这里面， 就是1， 否则就是0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_cat_hob_fea</span>(<span class="params">all_data, cols</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用户的主题爱好</span></span><br><span class="line"><span class="string">    :param all_data: 数据集</span></span><br><span class="line"><span class="string">    :param cols: 用到的特征列</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    user_category_hob_info = all_data[cols]</span><br><span class="line">    user_category_hob_info = user_category_hob_info.groupby(<span class="string">&#x27;user_id&#x27;</span>).agg(&#123;<span class="built_in">list</span>&#125;).reset_index()</span><br><span class="line">    </span><br><span class="line">    user_cat_hob_info = pd.DataFrame()</span><br><span class="line">    user_cat_hob_info[<span class="string">&#x27;user_id&#x27;</span>] = user_category_hob_info[<span class="string">&#x27;user_id&#x27;</span>]</span><br><span class="line">    user_cat_hob_info[<span class="string">&#x27;cate_list&#x27;</span>] = user_category_hob_info[<span class="string">&#x27;category_id&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> user_cat_hob_info</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_category_hob_cols = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;category_id&#x27;</span>]</span><br><span class="line">user_cat_hob_info = user_cat_hob_fea(all_data, user_category_hob_cols)</span><br></pre></td></tr></table></figure>

<h4 id="用户的字数偏好特征"><a href="#用户的字数偏好特征" class="headerlink" title="用户的字数偏好特征"></a>用户的字数偏好特征</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_wcou_info = all_data.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;words_count&#x27;</span>].agg(<span class="string">&#x27;mean&#x27;</span>).reset_index()</span><br><span class="line">user_wcou_info.rename(columns=&#123;<span class="string">&#x27;words_count&#x27;</span>: <span class="string">&#x27;words_hbo&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="用户的信息特征合并保存"><a href="#用户的信息特征合并保存" class="headerlink" title="用户的信息特征合并保存"></a>用户的信息特征合并保存</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 所有表进行合并</span></span><br><span class="line">user_info = pd.merge(user_act_fea, user_device_info, on=<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">user_info = user_info.merge(user_time_hob_info, on=<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">user_info = user_info.merge(user_cat_hob_info, on=<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">user_info = user_info.merge(user_wcou_info, on=<span class="string">&#x27;user_id&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这样用户特征以后就可以直接读取了</span></span><br><span class="line">user_info.to_csv(save_path + <span class="string">&#x27;user_info.csv&#x27;</span>, index=<span class="literal">False</span>)   </span><br></pre></td></tr></table></figure>

<h3 id="用户特征直接读入"><a href="#用户特征直接读入" class="headerlink" title="用户特征直接读入"></a>用户特征直接读入</h3><p>如果前面关于用户的特征工程已经给做完了，后面可以直接读取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 把用户信息直接读入进来</span></span><br><span class="line">user_info = pd.read_csv(save_path + <span class="string">&#x27;user_info.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.exists(save_path + <span class="string">&#x27;trn_user_item_feats_df.csv&#x27;</span>):</span><br><span class="line">    trn_user_item_feats_df = pd.read_csv(save_path + <span class="string">&#x27;trn_user_item_feats_df.csv&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> os.path.exists(save_path + <span class="string">&#x27;tst_user_item_feats_df.csv&#x27;</span>):</span><br><span class="line">    tst_user_item_feats_df = pd.read_csv(save_path + <span class="string">&#x27;tst_user_item_feats_df.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(save_path + <span class="string">&#x27;val_user_item_feats_df.csv&#x27;</span>):</span><br><span class="line">    val_user_item_feats_df = pd.read_csv(save_path + <span class="string">&#x27;val_user_item_feats_df.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼上用户特征</span></span><br><span class="line"><span class="comment"># 下面是线下验证的</span></span><br><span class="line">trn_user_item_feats_df = trn_user_item_feats_df.merge(user_info, on=<span class="string">&#x27;user_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> val_user_item_feats_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    val_user_item_feats_df = val_user_item_feats_df.merge(user_info, on=<span class="string">&#x27;user_id&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">tst_user_item_feats_df = tst_user_item_feats_df.merge(user_info, on=<span class="string">&#x27;user_id&#x27;</span>,how=<span class="string">&#x27;left&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_user_item_feats_df.columns</span><br></pre></td></tr></table></figure>




<pre><code>Index([&#39;user_id&#39;, &#39;click_article_id&#39;, &#39;sim0&#39;, &#39;time_diff0&#39;, &#39;word_diff0&#39;,
       &#39;sim_max&#39;, &#39;sim_min&#39;, &#39;sim_sum&#39;, &#39;sim_mean&#39;, &#39;score&#39;, &#39;rank&#39;, &#39;label&#39;,
       &#39;click_size&#39;, &#39;time_diff_mean&#39;, &#39;active_level&#39;, &#39;click_environment&#39;,
       &#39;click_deviceGroup&#39;, &#39;click_os&#39;, &#39;click_country&#39;, &#39;click_region&#39;,
       &#39;click_referrer_type&#39;, &#39;user_time_hob1&#39;, &#39;user_time_hob2&#39;, &#39;cate_list&#39;,
       &#39;words_hbo&#39;],
      dtype=&#39;object&#39;)</code></pre>
<h3 id="文章的特征直接读入"><a href="#文章的特征直接读入" class="headerlink" title="文章的特征直接读入"></a>文章的特征直接读入</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">articles =  pd.read_csv(data_path+<span class="string">&#x27;articles.csv&#x27;</span>)</span><br><span class="line">articles = reduce_mem(articles)</span><br></pre></td></tr></table></figure>

<pre><code>-- Mem. usage decreased to  5.56 Mb (50.0% reduction),time spend:0.00 min</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拼上文章特征</span></span><br><span class="line">trn_user_item_feats_df = trn_user_item_feats_df.merge(articles, left_on=<span class="string">&#x27;click_article_id&#x27;</span>, right_on=<span class="string">&#x27;article_id&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> val_user_item_feats_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    val_user_item_feats_df = val_user_item_feats_df.merge(articles, left_on=<span class="string">&#x27;click_article_id&#x27;</span>, right_on=<span class="string">&#x27;article_id&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">tst_user_item_feats_df = tst_user_item_feats_df.merge(articles, left_on=<span class="string">&#x27;click_article_id&#x27;</span>, right_on=<span class="string">&#x27;article_id&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="召回文章的主题是否在用户的爱好里面"><a href="#召回文章的主题是否在用户的爱好里面" class="headerlink" title="召回文章的主题是否在用户的爱好里面"></a>召回文章的主题是否在用户的爱好里面</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_user_item_feats_df[<span class="string">&#x27;is_cat_hab&#x27;</span>] = trn_user_item_feats_df.apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x.category_id <span class="keyword">in</span> <span class="built_in">set</span>(x.cate_list) <span class="keyword">else</span> <span class="number">0</span>, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> val_user_item_feats_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    val_user_item_feats_df[<span class="string">&#x27;is_cat_hab&#x27;</span>] = val_user_item_feats_df.apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x.category_id <span class="keyword">in</span> <span class="built_in">set</span>(x.cate_list) <span class="keyword">else</span> <span class="number">0</span>, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br><span class="line">tst_user_item_feats_df[<span class="string">&#x27;is_cat_hab&#x27;</span>] = tst_user_item_feats_df.apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x.category_id <span class="keyword">in</span> <span class="built_in">set</span>(x.cate_list) <span class="keyword">else</span> <span class="number">0</span>, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 线下验证</span></span><br><span class="line"><span class="keyword">del</span> trn_user_item_feats_df[<span class="string">&#x27;cate_list&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> val_user_item_feats_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">del</span> val_user_item_feats_df[<span class="string">&#x27;cate_list&#x27;</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">del</span> tst_user_item_feats_df[<span class="string">&#x27;cate_list&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> trn_user_item_feats_df[<span class="string">&#x27;article_id&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> val_user_item_feats_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">del</span> val_user_item_feats_df[<span class="string">&#x27;article_id&#x27;</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">del</span> tst_user_item_feats_df[<span class="string">&#x27;article_id&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="保存特征"><a href="#保存特征" class="headerlink" title="保存特征"></a>保存特征</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练验证特征</span></span><br><span class="line">trn_user_item_feats_df.to_csv(save_path + <span class="string">&#x27;trn_user_item_feats_df.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> val_user_item_feats_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    val_user_item_feats_df.to_csv(save_path + <span class="string">&#x27;val_user_item_feats_df.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">tst_user_item_feats_df.to_csv(save_path + <span class="string">&#x27;tst_user_item_feats_df.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>特征工程和数据清洗转换是比赛中至关重要的一块， 因为<strong>数据和特征决定了机器学习的上限，而算法和模型只是逼近这个上限而已</strong>，所以特征工程的好坏往往决定着最后的结果，<strong>特征工程</strong>可以一步增强数据的表达能力，通过构造新特征，我们可以挖掘出数据的更多信息，使得数据的表达能力进一步放大。 在本节内容中，我们主要是先通过制作特征和标签把预测问题转成了监督学习问题，然后围绕着用户画像和文章画像进行一系列特征的制作， 此外，为了保证正负样本的数据均衡，我们还学习了负采样就技术等。当然本节内容只是对构造特征提供了一些思路，也请学习者们在学习过程中开启头脑风暴，尝试更多的构造特征的方法，也欢迎我们一块探讨和交流。</p>
]]></content>
  </entry>
  <entry>
    <title>天池推荐系统入门赛——多路召回</title>
    <url>/2020/12/02/%E5%A4%A9%E6%B1%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%85%A5%E9%97%A8%E8%B5%9B%E2%80%94%E2%80%94%E5%A4%9A%E8%B7%AF%E5%8F%AC%E5%9B%9E/</url>
    <content><![CDATA[<p>﻿# 多路召回</p>
<p>所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用，可以明显的看出，“多路召回策略”是在“计算速度”和“召回率”之间进行权衡的结果。其中，各种简单策略保证候选集的快速召回，从不同角度设计的策略保证召回率接近理想的状态，不至于损伤排序效果。如下图是多路召回的一个示意图，在多路召回中，每个策略之间毫不相关，所以一般可以写并发多线程同时进行，这样可以更加高效。</p>
<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201119132726873.png" alt="image-20201119132726873" style="zoom:67%;" />

<p>上图只是一个多路召回的例子，也就是说可以使用多种不同的策略来获取用户排序的候选商品集合，而具体使用哪些召回策略其实是与业务强相关的 ，针对不同的任务就会有对于该业务真实场景下需要考虑的召回规则。例如新闻推荐，召回规则可以是“热门视频”、“导演召回”、“演员召回”、“最近上映“、”流行趋势“、”类型召回“等等。  </p>
<h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm  </span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict  </span><br><span class="line"><span class="keyword">import</span> os, math, warnings, math, pickle</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> deepctr.feature_column <span class="keyword">import</span> SparseFeat, VarLenSparseFeat</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> deepmatch.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> deepmatch.utils <span class="keyword">import</span> sampledsoftmaxloss</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_path = <span class="string">&#x27;./data_raw/&#x27;</span></span><br><span class="line">save_path = <span class="string">&#x27;./temp_results/&#x27;</span></span><br><span class="line"><span class="comment"># 做召回评估的一个标志, 如果不进行评估就是直接使用全量数据进行召回</span></span><br><span class="line">metric_recall = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p>在一般的推荐系统比赛中读取数据部分主要分为三种模式， 不同的模式对应的不同的数据集：</p>
<ol>
<li>Debug模式： 这个的目的是帮助我们基于数据先搭建一个简易的baseline并跑通， 保证写的baseline代码没有什么问题。 由于推荐比赛的数据往往非常巨大， 如果一上来直接采用全部的数据进行分析，搭建baseline框架， 往往会带来时间和设备上的损耗， **所以这时候我们往往需要从海量数据的训练集中随机抽取一部分样本来进行调试(train_click_log_sample)**， 先跑通一个baseline。</li>
<li>线下验证模式： 这个的目的是帮助我们在线下基于已有的训练集数据， 来选择好合适的模型和一些超参数。 **所以我们这一块只需要加载整个训练集(train_click_log)**， 然后把整个训练集再分成训练集和验证集。 训练集是模型的训练数据， 验证集部分帮助我们调整模型的参数和其他的一些超参数。</li>
<li>线上模式： 我们用debug模式搭建起一个推荐系统比赛的baseline， 用线下验证模式选择好了模型和一些超参数， 这一部分就是真正的对于给定的测试集进行预测， 提交到线上， <strong>所以这一块使用的训练数据集是全量的数据集(train_click_log+test_click_log)</strong></li>
</ol>
<p>下面就分别对这三种不同的数据读取模式先建立不同的代导入函数， 方便后面针对不同的模式下导入数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># debug模式： 从训练集中划出一部分数据来调试代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_click_sample</span>(<span class="params">data_path, sample_nums=<span class="number">10000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练集中采样一部分数据调试</span></span><br><span class="line"><span class="string">        data_path: 原数据的存储路径</span></span><br><span class="line"><span class="string">        sample_nums: 采样数目（这里由于机器的内存限制，可以采样用户做）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    all_click = pd.read_csv(data_path + <span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">    all_user_ids = all_click.user_id.unique()</span><br><span class="line"></span><br><span class="line">    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=<span class="literal">False</span>) </span><br><span class="line">    all_click = all_click[all_click[<span class="string">&#x27;user_id&#x27;</span>].isin(sample_user_ids)]</span><br><span class="line">    </span><br><span class="line">    all_click = all_click.drop_duplicates(([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> all_click</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取点击数据，这里分成线上和线下，如果是为了获取线上提交结果应该讲测试集中的点击数据合并到总的数据中</span></span><br><span class="line"><span class="comment"># 如果是为了线下验证模型的有效性或者特征的有效性，可以只使用训练集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_click_df</span>(<span class="params">data_path=<span class="string">&#x27;./data_raw/&#x27;</span>, offline=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> offline:</span><br><span class="line">        all_click = pd.read_csv(data_path + <span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        trn_click = pd.read_csv(data_path + <span class="string">&#x27;train_click_log.csv&#x27;</span>)</span><br><span class="line">        tst_click = pd.read_csv(data_path + <span class="string">&#x27;testA_click_log.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        all_click = trn_click.append(tst_click)</span><br><span class="line">    </span><br><span class="line">    all_click = all_click.drop_duplicates(([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> all_click</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取文章的基本属性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_info_df</span>(<span class="params">data_path</span>):</span></span><br><span class="line">    item_info_df = pd.read_csv(data_path + <span class="string">&#x27;articles.csv&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 为了方便与训练集中的click_article_id拼接，需要把article_id修改成click_article_id</span></span><br><span class="line">    item_info_df = item_info_df.rename(columns=&#123;<span class="string">&#x27;article_id&#x27;</span>: <span class="string">&#x27;click_article_id&#x27;</span>&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> item_info_df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取文章的Embedding数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_emb_dict</span>(<span class="params">data_path</span>):</span></span><br><span class="line">    item_emb_df = pd.read_csv(data_path + <span class="string">&#x27;articles_emb.csv&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    item_emb_cols = [x <span class="keyword">for</span> x <span class="keyword">in</span> item_emb_df.columns <span class="keyword">if</span> <span class="string">&#x27;emb&#x27;</span> <span class="keyword">in</span> x]</span><br><span class="line">    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])</span><br><span class="line">    <span class="comment"># 进行归一化</span></span><br><span class="line">    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    item_emb_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_emb_df[<span class="string">&#x27;article_id&#x27;</span>], item_emb_np))</span><br><span class="line">    pickle.dump(item_emb_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;item_content_emb.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> item_emb_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_min_scaler = <span class="keyword">lambda</span> x : (x-np.<span class="built_in">min</span>(x))/(np.<span class="built_in">max</span>(x)-np.<span class="built_in">min</span>(x))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 采样数据</span></span><br><span class="line"><span class="comment"># all_click_df = get_all_click_sample(data_path)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全量训练集</span></span><br><span class="line">all_click_df = get_all_click_df(offline=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对时间戳进行归一化,用于在关联规则的时候计算权重</span></span><br><span class="line">all_click_df[<span class="string">&#x27;click_timestamp&#x27;</span>] = all_click_df[[<span class="string">&#x27;click_timestamp&#x27;</span>]].apply(max_min_scaler)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_info_df = get_item_info_df(data_path)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_emb_dict = get_item_emb_dict(data_path)</span><br></pre></td></tr></table></figure>



<h2 id="工具函数"><a href="#工具函数" class="headerlink" title="工具函数"></a>工具函数</h2><h3 id="获取用户-文章-时间函数"><a href="#获取用户-文章-时间函数" class="headerlink" title="获取用户-文章-时间函数"></a>获取用户-文章-时间函数</h3><p>这个在基于关联规则的用户协同过滤的时候会用到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据点击时间获取用户的点击文章序列   &#123;user1: &#123;item1: time1, item2: time2..&#125;...&#125;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user_item_time</span>(<span class="params">click_df</span>):</span></span><br><span class="line">    </span><br><span class="line">    click_df = click_df.sort_values(<span class="string">&#x27;click_timestamp&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_item_time_pair</span>(<span class="params">df</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(df[<span class="string">&#x27;click_article_id&#x27;</span>], df[<span class="string">&#x27;click_timestamp&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    user_item_time_df = click_df.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>].apply(<span class="keyword">lambda</span> x: make_item_time_pair(x))\</span><br><span class="line">                                                            .reset_index().rename(columns=&#123;<span class="number">0</span>: <span class="string">&#x27;item_time_list&#x27;</span>&#125;)</span><br><span class="line">    user_item_time_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(user_item_time_df[<span class="string">&#x27;user_id&#x27;</span>], user_item_time_df[<span class="string">&#x27;item_time_list&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> user_item_time_dict</span><br></pre></td></tr></table></figure>



<h3 id="获取文章-用户-时间函数"><a href="#获取文章-用户-时间函数" class="headerlink" title="获取文章-用户-时间函数"></a>获取文章-用户-时间函数</h3><p>这个在基于关联规则的文章协同过滤的时候会用到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据时间获取商品被点击的用户序列  &#123;item1: &#123;user1: time1, user2: time2...&#125;...&#125;</span></span><br><span class="line"><span class="comment"># 这里的时间是用户点击当前商品的时间，好像没有直接的关系。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_user_time_dict</span>(<span class="params">click_df</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_user_time_pair</span>(<span class="params">df</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(df[<span class="string">&#x27;user_id&#x27;</span>], df[<span class="string">&#x27;click_timestamp&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    click_df = click_df.sort_values(<span class="string">&#x27;click_timestamp&#x27;</span>)</span><br><span class="line">    item_user_time_df = click_df.groupby(<span class="string">&#x27;click_article_id&#x27;</span>)[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>].apply(<span class="keyword">lambda</span> x: make_user_time_pair(x))\</span><br><span class="line">                                                            .reset_index().rename(columns=&#123;<span class="number">0</span>: <span class="string">&#x27;user_time_list&#x27;</span>&#125;)</span><br><span class="line">    </span><br><span class="line">    item_user_time_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_user_time_df[<span class="string">&#x27;click_article_id&#x27;</span>], item_user_time_df[<span class="string">&#x27;user_time_list&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> item_user_time_dict</span><br></pre></td></tr></table></figure>



<h3 id="获取历史和最后一次点击"><a href="#获取历史和最后一次点击" class="headerlink" title="获取历史和最后一次点击"></a>获取历史和最后一次点击</h3><p>这个在评估召回结果， 特征工程和制作标签转成监督学习测试集的时候回用到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取当前数据的历史点击和最后一次点击</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_hist_and_last_click</span>(<span class="params">all_click</span>):</span></span><br><span class="line">    </span><br><span class="line">    all_click = all_click.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_timestamp&#x27;</span>])</span><br><span class="line">    click_last_df = all_click.groupby(<span class="string">&#x27;user_id&#x27;</span>).tail(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hist_func</span>(<span class="params">user_df</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(user_df) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> user_df</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> user_df[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    click_hist_df = all_click.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(hist_func).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> click_hist_df, click_last_df</span><br></pre></td></tr></table></figure>



<h3 id="获取文章属性特征"><a href="#获取文章属性特征" class="headerlink" title="获取文章属性特征"></a>获取文章属性特征</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取文章id对应的基本属性，保存成字典的形式，方便后面召回阶段，冷启动阶段直接使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_info_dict</span>(<span class="params">item_info_df</span>):</span></span><br><span class="line">    max_min_scaler = <span class="keyword">lambda</span> x : (x-np.<span class="built_in">min</span>(x))/(np.<span class="built_in">max</span>(x)-np.<span class="built_in">min</span>(x))</span><br><span class="line">    item_info_df[<span class="string">&#x27;created_at_ts&#x27;</span>] = item_info_df[[<span class="string">&#x27;created_at_ts&#x27;</span>]].apply(max_min_scaler)</span><br><span class="line">    </span><br><span class="line">    item_type_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_info_df[<span class="string">&#x27;click_article_id&#x27;</span>], item_info_df[<span class="string">&#x27;category_id&#x27;</span>]))</span><br><span class="line">    item_words_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_info_df[<span class="string">&#x27;click_article_id&#x27;</span>], item_info_df[<span class="string">&#x27;words_count&#x27;</span>]))</span><br><span class="line">    item_created_time_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_info_df[<span class="string">&#x27;click_article_id&#x27;</span>], item_info_df[<span class="string">&#x27;created_at_ts&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> item_type_dict, item_words_dict, item_created_time_dict</span><br></pre></td></tr></table></figure>



<h3 id="获取用户历史点击的文章信息"><a href="#获取用户历史点击的文章信息" class="headerlink" title="获取用户历史点击的文章信息"></a>获取用户历史点击的文章信息</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user_hist_item_info_dict</span>(<span class="params">all_click</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取user_id对应的用户历史点击文章类型的集合字典</span></span><br><span class="line">    user_hist_item_typs = all_click.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;category_id&#x27;</span>].agg(<span class="built_in">set</span>).reset_index()</span><br><span class="line">    user_hist_item_typs_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(user_hist_item_typs[<span class="string">&#x27;user_id&#x27;</span>], user_hist_item_typs[<span class="string">&#x27;category_id&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取user_id对应的用户点击文章的集合</span></span><br><span class="line">    user_hist_item_ids_dict = all_click.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_article_id&#x27;</span>].agg(<span class="built_in">set</span>).reset_index()</span><br><span class="line">    user_hist_item_ids_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(user_hist_item_ids_dict[<span class="string">&#x27;user_id&#x27;</span>], user_hist_item_ids_dict[<span class="string">&#x27;click_article_id&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取user_id对应的用户历史点击的文章的平均字数字典</span></span><br><span class="line">    user_hist_item_words = all_click.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;words_count&#x27;</span>].agg(<span class="string">&#x27;mean&#x27;</span>).reset_index()</span><br><span class="line">    user_hist_item_words_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(user_hist_item_words[<span class="string">&#x27;user_id&#x27;</span>], user_hist_item_words[<span class="string">&#x27;words_count&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取user_id对应的用户最后一次点击的文章的创建时间</span></span><br><span class="line">    all_click_ = all_click.sort_values(<span class="string">&#x27;click_timestamp&#x27;</span>)</span><br><span class="line">    user_last_item_created_time = all_click_.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;created_at_ts&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.iloc[-<span class="number">1</span>]).reset_index()</span><br><span class="line">    </span><br><span class="line">    max_min_scaler = <span class="keyword">lambda</span> x : (x-np.<span class="built_in">min</span>(x))/(np.<span class="built_in">max</span>(x)-np.<span class="built_in">min</span>(x))</span><br><span class="line">    user_last_item_created_time[<span class="string">&#x27;created_at_ts&#x27;</span>] = user_last_item_created_time[[<span class="string">&#x27;created_at_ts&#x27;</span>]].apply(max_min_scaler)</span><br><span class="line">    </span><br><span class="line">    user_last_item_created_time_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(user_last_item_created_time[<span class="string">&#x27;user_id&#x27;</span>], \</span><br><span class="line">                                                user_last_item_created_time[<span class="string">&#x27;created_at_ts&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict</span><br></pre></td></tr></table></figure>



<h3 id="获取点击次数最多的Top-k个文章"><a href="#获取点击次数最多的Top-k个文章" class="headerlink" title="获取点击次数最多的Top-k个文章"></a>获取点击次数最多的Top-k个文章</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取近期点击最多的文章</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_item_topk_click</span>(<span class="params">click_df, k</span>):</span></span><br><span class="line">    topk_click = click_df[<span class="string">&#x27;click_article_id&#x27;</span>].value_counts().index[:k]</span><br><span class="line">    <span class="keyword">return</span> topk_click</span><br></pre></td></tr></table></figure>



<h3 id="定义多路召回字典"><a href="#定义多路召回字典" class="headerlink" title="定义多路召回字典"></a>定义多路召回字典</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取文章的属性信息，保存成字典的形式方便查询</span></span><br><span class="line">item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个多路召回的字典，将各路召回的结果都保存在这个字典当中</span></span><br><span class="line">user_multi_recall_dict =  &#123;<span class="string">&#x27;itemcf_sim_itemcf_recall&#x27;</span>: &#123;&#125;,</span><br><span class="line">                           <span class="string">&#x27;embedding_sim_item_recall&#x27;</span>: &#123;&#125;,</span><br><span class="line">                           <span class="string">&#x27;youtubednn_recall&#x27;</span>: &#123;&#125;,</span><br><span class="line">                           <span class="string">&#x27;youtubednn_usercf_recall&#x27;</span>: &#123;&#125;, </span><br><span class="line">                           <span class="string">&#x27;cold_start_recall&#x27;</span>: &#123;&#125;&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取最后一次点击作为召回评估，如果不需要做召回评估直接使用全量的训练集进行召回(线下验证模型)</span></span><br><span class="line"><span class="comment"># 如果不是召回评估，直接使用全量数据进行召回，不用将最后一次提取出来</span></span><br><span class="line">trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)</span><br></pre></td></tr></table></figure>



<h3 id="召回效果评估"><a href="#召回效果评估" class="headerlink" title="召回效果评估"></a>召回效果评估</h3><p>做完了召回有时候也需要对当前的召回方法或者参数进行调整以达到更好的召回效果，因为召回的结果决定了最终排序的上限，下面也会提供一个召回评估的方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 依次评估召回的前10, 20, 30, 40, 50个文章中的击中率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">metrics_recall</span>(<span class="params">user_recall_items_dict, trn_last_click_df, topk=<span class="number">5</span></span>):</span></span><br><span class="line">    last_click_item_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(trn_last_click_df[<span class="string">&#x27;user_id&#x27;</span>], trn_last_click_df[<span class="string">&#x27;click_article_id&#x27;</span>]))</span><br><span class="line">    user_num = <span class="built_in">len</span>(user_recall_items_dict)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>, topk+<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">        hit_num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> user, item_list <span class="keyword">in</span> user_recall_items_dict.items():</span><br><span class="line">            <span class="comment"># 获取前k个召回的结果</span></span><br><span class="line">            tmp_recall_items = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> user_recall_items_dict[user][:k]]</span><br><span class="line">            <span class="keyword">if</span> last_click_item_dict[user] <span class="keyword">in</span> <span class="built_in">set</span>(tmp_recall_items):</span><br><span class="line">                hit_num += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        hit_rate = <span class="built_in">round</span>(hit_num * <span class="number">1.0</span> / user_num, <span class="number">5</span>)</span><br><span class="line">        print(<span class="string">&#x27; topk: &#x27;</span>, k, <span class="string">&#x27; : &#x27;</span>, <span class="string">&#x27;hit_num: &#x27;</span>, hit_num, <span class="string">&#x27;hit_rate: &#x27;</span>, hit_rate, <span class="string">&#x27;user_num : &#x27;</span>, user_num)</span><br></pre></td></tr></table></figure>



<h2 id="计算相似性矩阵"><a href="#计算相似性矩阵" class="headerlink" title="计算相似性矩阵"></a>计算相似性矩阵</h2><p>这一部分主要是通过协同过滤以及向量检索得到相似性矩阵，相似性矩阵主要分为user2user和item2item，下面依次获取基于itemCF的item2item的相似性矩阵。</p>
<h3 id="itemCF-i2i-sim"><a href="#itemCF-i2i-sim" class="headerlink" title="itemCF i2i_sim"></a>itemCF i2i_sim</h3><p>借鉴KDD2020的去偏商品推荐，在计算item2item相似性矩阵时，使用关联规则，使得计算的文章的相似性还考虑到了:</p>
<ol>
<li>用户点击的时间权重</li>
<li>用户点击的顺序权重</li>
<li>文章创建的时间权重</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">itemcf_sim</span>(<span class="params">df, item_created_time_dict</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        文章与文章之间的相似性矩阵计算</span></span><br><span class="line"><span class="string">        :param df: 数据表</span></span><br><span class="line"><span class="string">        :item_created_time_dict:  文章创建时间的字典</span></span><br><span class="line"><span class="string">        return : 文章与文章的相似性矩阵</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        思路: 基于物品的协同过滤(详细请参考上一期推荐系统基础的组队学习) + 关联规则</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    user_item_time_dict = get_user_item_time(df)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算物品相似度</span></span><br><span class="line">    i2i_sim = &#123;&#125;</span><br><span class="line">    item_cnt = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> user, item_time_list <span class="keyword">in</span> tqdm(user_item_time_dict.items()):</span><br><span class="line">        <span class="comment"># 在基于商品的协同过滤优化的时候可以考虑时间因素</span></span><br><span class="line">        <span class="keyword">for</span> loc1, (i, i_click_time) <span class="keyword">in</span> <span class="built_in">enumerate</span>(item_time_list):</span><br><span class="line">            item_cnt[i] += <span class="number">1</span></span><br><span class="line">            i2i_sim.setdefault(i, &#123;&#125;)</span><br><span class="line">            <span class="keyword">for</span> loc2, (j, j_click_time) <span class="keyword">in</span> <span class="built_in">enumerate</span>(item_time_list):</span><br><span class="line">                <span class="keyword">if</span>(i == j):</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                    </span><br><span class="line">                <span class="comment"># 考虑文章的正向顺序点击和反向顺序点击    </span></span><br><span class="line">                loc_alpha = <span class="number">1.0</span> <span class="keyword">if</span> loc2 &gt; loc1 <span class="keyword">else</span> <span class="number">0.7</span></span><br><span class="line">                <span class="comment"># 位置信息权重，其中的参数可以调节</span></span><br><span class="line">                loc_weight = loc_alpha * (<span class="number">0.9</span> ** (np.<span class="built_in">abs</span>(loc2 - loc1) - <span class="number">1</span>))</span><br><span class="line">                <span class="comment"># 点击时间权重，其中的参数可以调节</span></span><br><span class="line">                click_time_weight = np.exp(<span class="number">0.7</span> ** np.<span class="built_in">abs</span>(i_click_time - j_click_time))</span><br><span class="line">                <span class="comment"># 两篇文章创建时间的权重，其中的参数可以调节</span></span><br><span class="line">                created_time_weight = np.exp(<span class="number">0.8</span> ** np.<span class="built_in">abs</span>(item_created_time_dict[i] - item_created_time_dict[j]))</span><br><span class="line">                i2i_sim[i].setdefault(j, <span class="number">0</span>)</span><br><span class="line">                <span class="comment"># 考虑多种因素的权重计算最终的文章之间的相似度</span></span><br><span class="line">                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(<span class="built_in">len</span>(item_time_list) + <span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">    i2i_sim_ = i2i_sim.copy()</span><br><span class="line">    <span class="keyword">for</span> i, related_items <span class="keyword">in</span> i2i_sim.items():</span><br><span class="line">        <span class="keyword">for</span> j, wij <span class="keyword">in</span> related_items.items():</span><br><span class="line">            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将得到的相似性矩阵保存到本地</span></span><br><span class="line">    pickle.dump(i2i_sim_, <span class="built_in">open</span>(save_path + <span class="string">&#x27;itemcf_i2i_sim.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> i2i_sim_</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [14:20&lt;00:00, 290.38it/s]</code></pre>
<h3 id="userCF-u2u-sim"><a href="#userCF-u2u-sim" class="headerlink" title="userCF u2u_sim"></a>userCF u2u_sim</h3><p>在计算用户之间的相似度的时候，也可以使用一些简单的关联规则，比如用户活跃度权重，这里将用户的点击次数作为用户活跃度的指标</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_user_activate_degree_dict</span>(<span class="params">all_click_df</span>):</span></span><br><span class="line">    all_click_df_ = all_click_df.groupby(<span class="string">&#x27;user_id&#x27;</span>)[<span class="string">&#x27;click_article_id&#x27;</span>].count().reset_index()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用户活跃度归一化</span></span><br><span class="line">    mm = MinMaxScaler()</span><br><span class="line">    all_click_df_[<span class="string">&#x27;click_article_id&#x27;</span>] = mm.fit_transform(all_click_df_[[<span class="string">&#x27;click_article_id&#x27;</span>]])</span><br><span class="line">    user_activate_degree_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(all_click_df_[<span class="string">&#x27;user_id&#x27;</span>], all_click_df_[<span class="string">&#x27;click_article_id&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> user_activate_degree_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">usercf_sim</span>(<span class="params">all_click_df, user_activate_degree_dict</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        用户相似性矩阵计算</span></span><br><span class="line"><span class="string">        :param all_click_df: 数据表</span></span><br><span class="line"><span class="string">        :param user_activate_degree_dict: 用户活跃度的字典</span></span><br><span class="line"><span class="string">        return 用户相似性矩阵</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        思路: 基于用户的协同过滤(详细请参考上一期推荐系统基础的组队学习) + 关联规则</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    item_user_time_dict = get_item_user_time_dict(all_click_df)</span><br><span class="line">    </span><br><span class="line">    u2u_sim = &#123;&#125;</span><br><span class="line">    user_cnt = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> item, user_time_list <span class="keyword">in</span> tqdm(item_user_time_dict.items()):</span><br><span class="line">        <span class="keyword">for</span> u, click_time <span class="keyword">in</span> user_time_list:</span><br><span class="line">            user_cnt[u] += <span class="number">1</span></span><br><span class="line">            u2u_sim.setdefault(u, &#123;&#125;)</span><br><span class="line">            <span class="keyword">for</span> v, click_time <span class="keyword">in</span> user_time_list:</span><br><span class="line">                u2u_sim[u].setdefault(v, <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">if</span> u == v:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># 用户平均活跃度作为活跃度的权重，这里的式子也可以改善</span></span><br><span class="line">                activate_weight = <span class="number">100</span> * <span class="number">0.5</span> * (user_activate_degree_dict[u] + user_activate_degree_dict[v])   </span><br><span class="line">                u2u_sim[u][v] += activate_weight / math.log(<span class="built_in">len</span>(user_time_list) + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    u2u_sim_ = u2u_sim.copy()</span><br><span class="line">    <span class="keyword">for</span> u, related_users <span class="keyword">in</span> u2u_sim.items():</span><br><span class="line">        <span class="keyword">for</span> v, wij <span class="keyword">in</span> related_users.items():</span><br><span class="line">            u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将得到的相似性矩阵保存到本地</span></span><br><span class="line">    pickle.dump(u2u_sim_, <span class="built_in">open</span>(save_path + <span class="string">&#x27;usercf_u2u_sim.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> u2u_sim_</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 由于usercf计算时候太耗费内存了，这里就不直接运行了</span></span><br><span class="line"><span class="comment"># 如果是采样的话，是可以运行的</span></span><br><span class="line">user_activate_degree_dict = get_user_activate_degree_dict(all_click_df)</span><br><span class="line">u2u_sim = usercf_sim(all_click_df, user_activate_degree_dict)</span><br></pre></td></tr></table></figure>



<h3 id="item-embedding-sim"><a href="#item-embedding-sim" class="headerlink" title="item embedding sim"></a>item embedding sim</h3><p>使用Embedding计算item之间的相似度是为了后续冷启动的时候可以获取未出现在点击数据中的文章，后面有对冷启动专门的介绍，这里简单的说一下faiss。</p>
<p>aiss是Facebook的AI团队开源的一套用于做聚类或者相似性搜索的软件库，底层是用C++实现。Faiss因为超级优越的性能，被广泛应用于推荐相关的业务当中.</p>
<p>faiss工具包一般使用在推荐系统中的向量召回部分。在做向量召回的时候要么是u2u,u2i或者i2i，这里的u和i指的是user和item.我们知道在实际的场景中user和item的数量都是海量的，我们最容易想到的基于向量相似度的召回就是使用两层循环遍历user列表或者item列表计算两个向量的相似度，但是这样做在面对海量数据是不切实际的，faiss就是用来加速计算某个查询向量最相似的topk个索引向量。</p>
<p><strong>faiss查询的原理：</strong></p>
<p>faiss使用了PCA和PQ(Product quantization乘积量化)两种技术进行向量压缩和编码，当然还使用了其他的技术进行优化，但是PCA和PQ是其中最核心部分。</p>
<ol>
<li><p>PCA降维算法细节参考下面这个链接进行学习<br><a href="https://www.cnblogs.com/pinard/p/6239403.html">主成分分析（PCA）原理总结</a>  </p>
</li>
<li><p>PQ编码的细节下面这个链接进行学习<br><a href="http://www.fabwrite.com/productquantization">实例理解product quantization算法</a></p>
</li>
</ol>
<p><strong>faiss使用</strong></p>
<p><a href="https://github.com/facebookresearch/faiss/wiki/Getting-started">faiss官方教程</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向量检索相似度计算</span></span><br><span class="line"><span class="comment"># topk指的是每个item, faiss搜索后返回最相似的topk个item</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embdding_sim</span>(<span class="params">click_df, item_emb_df, save_path, topk</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于内容的文章embedding相似性矩阵计算</span></span><br><span class="line"><span class="string">        :param click_df: 数据表</span></span><br><span class="line"><span class="string">        :param item_emb_df: 文章的embedding</span></span><br><span class="line"><span class="string">        :param save_path: 保存路径</span></span><br><span class="line"><span class="string">        :patam topk: 找最相似的topk篇</span></span><br><span class="line"><span class="string">        return 文章相似性矩阵</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        思路: 对于每一篇文章， 基于embedding的相似性返回topk个与其最相似的文章， 只不过由于文章数量太多，这里用了faiss进行加速</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 文章索引与文章id的字典映射</span></span><br><span class="line">    item_idx_2_rawid_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_emb_df.index, item_emb_df[<span class="string">&#x27;article_id&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    item_emb_cols = [x <span class="keyword">for</span> x <span class="keyword">in</span> item_emb_df.columns <span class="keyword">if</span> <span class="string">&#x27;emb&#x27;</span> <span class="keyword">in</span> x]</span><br><span class="line">    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols].values, dtype=np.float32)</span><br><span class="line">    <span class="comment"># 向量进行单位化</span></span><br><span class="line">    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 建立faiss索引</span></span><br><span class="line">    item_index = faiss.IndexFlatIP(item_emb_np.shape[<span class="number">1</span>])</span><br><span class="line">    item_index.add(item_emb_np)</span><br><span class="line">    <span class="comment"># 相似度查询，给每个索引位置上的向量返回topk个item以及相似度</span></span><br><span class="line">    sim, idx = item_index.search(item_emb_np, topk) <span class="comment"># 返回的是列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将向量检索的结果保存成原始id的对应关系</span></span><br><span class="line">    item_sim_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">    <span class="keyword">for</span> target_idx, sim_value_list, rele_idx_list <span class="keyword">in</span> tqdm(<span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(item_emb_np)), sim, idx)):</span><br><span class="line">        target_raw_id = item_idx_2_rawid_dict[target_idx]</span><br><span class="line">        <span class="comment"># 从1开始是为了去掉商品本身, 所以最终获得的相似商品只有topk-1</span></span><br><span class="line">        <span class="keyword">for</span> rele_idx, sim_value <span class="keyword">in</span> <span class="built_in">zip</span>(rele_idx_list[<span class="number">1</span>:], sim_value_list[<span class="number">1</span>:]): </span><br><span class="line">            rele_raw_id = item_idx_2_rawid_dict[rele_idx]</span><br><span class="line">            item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, &#123;&#125;).get(rele_raw_id, <span class="number">0</span>) + sim_value</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存i2i相似度矩阵</span></span><br><span class="line">    pickle.dump(item_sim_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;emb_i2i_sim.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))   </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> item_sim_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item_emb_df = pd.read_csv(data_path + <span class="string">&#x27;/articles_emb.csv&#x27;</span>)</span><br><span class="line">emb_i2i_sim = embdding_sim(all_click_df, item_emb_df, save_path, topk=<span class="number">10</span>) <span class="comment"># topk可以自行设置</span></span><br></pre></td></tr></table></figure>

<pre><code>364047it [00:23, 15292.14it/s]</code></pre>
<h2 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h2><p>这个就是我们开篇提到的那个问题， 面的36万篇文章， 20多万用户的推荐， 我们又有哪些策略来缩减问题的规模？ 我们就可以再召回阶段筛选出用户对于点击文章的候选集合， 从而降低问题的规模。召回常用的策略：</p>
<ul>
<li>Youtube DNN 召回</li>
<li>基于文章的召回<ul>
<li>文章的协同过滤</li>
<li>基于文章embedding的召回</li>
</ul>
</li>
<li>基于用户的召回<ul>
<li>用户的协同过滤</li>
<li>用户embedding</li>
</ul>
</li>
</ul>
<p>上面的各种召回方式一部分在基于用户已经看得文章的基础上去召回与这些文章相似的一些文章， 而这个相似性的计算方式不同， 就得到了不同的召回方式， 比如文章的协同过滤， 文章内容的embedding等。还有一部分是根据用户的相似性进行推荐，对于某用户推荐与其相似的其他用户看过的文章，比如用户的协同过滤和用户embedding。 还有一种思路是类似矩阵分解的思路，先计算出用户和文章的embedding之后，就可以直接算用户和文章的相似度， 根据这个相似度进行推荐， 比如YouTube DNN。 我们下面详细来看一下每一个召回方法：</p>
<h3 id="YoutubeDNN召回"><a href="#YoutubeDNN召回" class="headerlink" title="YoutubeDNN召回"></a>YoutubeDNN召回</h3><p><strong>(这一步是直接获取用户召回的候选文章列表)</strong></p>
<p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf">论文下载地址</a></p>
<p><strong>Youtubednn召回架构</strong></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/59576beef26f07e6e94fd2a1d73929bd.png" alt="image-20201111160516562"></p>
<p>关于YoutubeDNN原理和应用推荐看王喆的两篇博客：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/52169807">重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/52504407">YouTube深度学习推荐系统的十大工程问题</a></li>
</ol>
<p><strong>参考文献:</strong></p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/52169807">https://zhuanlan.zhihu.com/p/52169807</a> (YouTubeDNN原理)</li>
<li><a href="https://zhuanlan.zhihu.com/p/26306795">https://zhuanlan.zhihu.com/p/26306795</a> (Word2Vec知乎众赞文章) — word2vec放到排序中的w2v的介绍部分</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取双塔召回时的训练验证数据</span></span><br><span class="line"><span class="comment"># negsample指的是通过滑窗构建样本的时候，负样本的数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_data_set</span>(<span class="params">data, negsample=<span class="number">0</span></span>):</span></span><br><span class="line">    data.sort_values(<span class="string">&quot;click_timestamp&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    item_ids = data[<span class="string">&#x27;click_article_id&#x27;</span>].unique()</span><br><span class="line"></span><br><span class="line">    train_set = []</span><br><span class="line">    test_set = []</span><br><span class="line">    <span class="keyword">for</span> reviewerID, hist <span class="keyword">in</span> tqdm(data.groupby(<span class="string">&#x27;user_id&#x27;</span>)):</span><br><span class="line">        pos_list = hist[<span class="string">&#x27;click_article_id&#x27;</span>].tolist()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> negsample &gt; <span class="number">0</span>:</span><br><span class="line">            candidate_set = <span class="built_in">list</span>(<span class="built_in">set</span>(item_ids) - <span class="built_in">set</span>(pos_list))   <span class="comment"># 用户没看过的文章里面选择负样本</span></span><br><span class="line">            neg_list = np.random.choice(candidate_set,size=<span class="built_in">len</span>(pos_list)*negsample,replace=<span class="literal">True</span>)  <span class="comment"># 对于每个正样本，选择n个负样本</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 长度只有一个的时候，需要把这条数据也放到训练集中，不然的话最终学到的embedding就会有缺失</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pos_list) == <span class="number">1</span>:</span><br><span class="line">            train_set.append((reviewerID, [pos_list[<span class="number">0</span>]], pos_list[<span class="number">0</span>],<span class="number">1</span>,<span class="built_in">len</span>(pos_list)))</span><br><span class="line">            test_set.append((reviewerID, [pos_list[<span class="number">0</span>]], pos_list[<span class="number">0</span>],<span class="number">1</span>,<span class="built_in">len</span>(pos_list)))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 滑窗构造正负样本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(pos_list)):</span><br><span class="line">            hist = pos_list[:i]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i != <span class="built_in">len</span>(pos_list) - <span class="number">1</span>:</span><br><span class="line">                train_set.append((reviewerID, hist[::-<span class="number">1</span>], pos_list[i], <span class="number">1</span>, <span class="built_in">len</span>(hist[::-<span class="number">1</span>])))  <span class="comment"># 正样本 [user_id, his_item, pos_item, label, len(his_item)]</span></span><br><span class="line">                <span class="keyword">for</span> negi <span class="keyword">in</span> <span class="built_in">range</span>(negsample):</span><br><span class="line">                    train_set.append((reviewerID, hist[::-<span class="number">1</span>], neg_list[i*negsample+negi], <span class="number">0</span>,<span class="built_in">len</span>(hist[::-<span class="number">1</span>]))) <span class="comment"># 负样本 [user_id, his_item, neg_item, label, len(his_item)]</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 将最长的那一个序列长度作为测试数据</span></span><br><span class="line">                test_set.append((reviewerID, hist[::-<span class="number">1</span>], pos_list[i],<span class="number">1</span>,<span class="built_in">len</span>(hist[::-<span class="number">1</span>])))</span><br><span class="line">                </span><br><span class="line">    random.shuffle(train_set)</span><br><span class="line">    random.shuffle(test_set)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_set, test_set</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入的数据进行padding，使得序列特征的长度都一致</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_model_input</span>(<span class="params">train_set,user_profile,seq_max_len</span>):</span></span><br><span class="line"></span><br><span class="line">    train_uid = np.array([line[<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> train_set])</span><br><span class="line">    train_seq = [line[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> train_set]</span><br><span class="line">    train_iid = np.array([line[<span class="number">2</span>] <span class="keyword">for</span> line <span class="keyword">in</span> train_set])</span><br><span class="line">    train_label = np.array([line[<span class="number">3</span>] <span class="keyword">for</span> line <span class="keyword">in</span> train_set])</span><br><span class="line">    train_hist_len = np.array([line[<span class="number">4</span>] <span class="keyword">for</span> line <span class="keyword">in</span> train_set])</span><br><span class="line"></span><br><span class="line">    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding=<span class="string">&#x27;post&#x27;</span>, truncating=<span class="string">&#x27;post&#x27;</span>, value=<span class="number">0</span>)</span><br><span class="line">    train_model_input = &#123;<span class="string">&quot;user_id&quot;</span>: train_uid, <span class="string">&quot;click_article_id&quot;</span>: train_iid, <span class="string">&quot;hist_article_id&quot;</span>: train_seq_pad,</span><br><span class="line">                         <span class="string">&quot;hist_len&quot;</span>: train_hist_len&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_model_input, train_label</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">youtubednn_u2i_dict</span>(<span class="params">data, topk=<span class="number">20</span></span>):</span>    </span><br><span class="line">    sparse_features = [<span class="string">&quot;click_article_id&quot;</span>, <span class="string">&quot;user_id&quot;</span>]</span><br><span class="line">    SEQ_LEN = <span class="number">30</span> <span class="comment"># 用户点击序列的长度，短的填充，长的截断</span></span><br><span class="line">    </span><br><span class="line">    user_profile_ = data[[<span class="string">&quot;user_id&quot;</span>]].drop_duplicates(<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">    item_profile_ = data[[<span class="string">&quot;click_article_id&quot;</span>]].drop_duplicates(<span class="string">&#x27;click_article_id&#x27;</span>)  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 类别编码</span></span><br><span class="line">    features = [<span class="string">&quot;click_article_id&quot;</span>, <span class="string">&quot;user_id&quot;</span>]</span><br><span class="line">    feature_max_idx = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        lbe = LabelEncoder()</span><br><span class="line">        data[feature] = lbe.fit_transform(data[feature])</span><br><span class="line">        feature_max_idx[feature] = data[feature].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 提取user和item的画像，这里具体选择哪些特征还需要进一步的分析和考虑</span></span><br><span class="line">    user_profile = data[[<span class="string">&quot;user_id&quot;</span>]].drop_duplicates(<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">    item_profile = data[[<span class="string">&quot;click_article_id&quot;</span>]].drop_duplicates(<span class="string">&#x27;click_article_id&#x27;</span>)  </span><br><span class="line">    </span><br><span class="line">    user_index_2_rawid = <span class="built_in">dict</span>(<span class="built_in">zip</span>(user_profile[<span class="string">&#x27;user_id&#x27;</span>], user_profile_[<span class="string">&#x27;user_id&#x27;</span>]))</span><br><span class="line">    item_index_2_rawid = <span class="built_in">dict</span>(<span class="built_in">zip</span>(item_profile[<span class="string">&#x27;click_article_id&#x27;</span>], item_profile_[<span class="string">&#x27;click_article_id&#x27;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 划分训练和测试集</span></span><br><span class="line">    <span class="comment"># 由于深度学习需要的数据量通常都是非常大的，所以为了保证召回的效果，往往会通过滑窗的形式扩充训练样本</span></span><br><span class="line">    train_set, test_set = gen_data_set(data, <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 整理输入数据，具体的操作可以看上面的函数</span></span><br><span class="line">    train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)</span><br><span class="line">    test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确定Embedding的维度</span></span><br><span class="line">    embedding_dim = <span class="number">16</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将数据整理成模型可以直接输入的形式</span></span><br><span class="line">    user_feature_columns = [SparseFeat(<span class="string">&#x27;user_id&#x27;</span>, feature_max_idx[<span class="string">&#x27;user_id&#x27;</span>], embedding_dim),</span><br><span class="line">                            VarLenSparseFeat(SparseFeat(<span class="string">&#x27;hist_article_id&#x27;</span>, feature_max_idx[<span class="string">&#x27;click_article_id&#x27;</span>], embedding_dim,</span><br><span class="line">                                                        embedding_name=<span class="string">&quot;click_article_id&quot;</span>), SEQ_LEN, <span class="string">&#x27;mean&#x27;</span>, <span class="string">&#x27;hist_len&#x27;</span>),]</span><br><span class="line">    item_feature_columns = [SparseFeat(<span class="string">&#x27;click_article_id&#x27;</span>, feature_max_idx[<span class="string">&#x27;click_article_id&#x27;</span>], embedding_dim)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型的定义 </span></span><br><span class="line">    <span class="comment"># num_sampled: 负采样时的样本数量</span></span><br><span class="line">    model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=<span class="number">5</span>, user_dnn_hidden_units=(<span class="number">64</span>, embedding_dim))</span><br><span class="line">    <span class="comment"># 模型编译</span></span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&quot;adam&quot;</span>, loss=sampledsoftmaxloss)  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型训练，这里可以定义验证集的比例，如果设置为0的话就是全量数据直接进行训练</span></span><br><span class="line">    history = model.fit(train_model_input, train_label, batch_size=<span class="number">256</span>, epochs=<span class="number">1</span>, verbose=<span class="number">1</span>, validation_split=<span class="number">0.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练完模型之后,提取训练的Embedding，包括user端和item端</span></span><br><span class="line">    test_user_model_input = test_model_input</span><br><span class="line">    all_item_model_input = &#123;<span class="string">&quot;click_article_id&quot;</span>: item_profile[<span class="string">&#x27;click_article_id&#x27;</span>].values&#125;</span><br><span class="line"></span><br><span class="line">    user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)</span><br><span class="line">    item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存当前的item_embedding 和 user_embedding 排序的时候可能能够用到，但是需要注意保存的时候需要和原始的id对应</span></span><br><span class="line">    user_embs = user_embedding_model.predict(test_user_model_input, batch_size=<span class="number">2</span> ** <span class="number">12</span>)</span><br><span class="line">    item_embs = item_embedding_model.predict(all_item_model_input, batch_size=<span class="number">2</span> ** <span class="number">12</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># embedding保存之前归一化一下</span></span><br><span class="line">    user_embs = user_embs / np.linalg.norm(user_embs, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    item_embs = item_embs / np.linalg.norm(item_embs, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将Embedding转换成字典的形式方便查询</span></span><br><span class="line">    raw_user_id_emb_dict = &#123;user_index_2_rawid[k]: \</span><br><span class="line">                                v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">zip</span>(user_profile[<span class="string">&#x27;user_id&#x27;</span>], user_embs)&#125;</span><br><span class="line">    raw_item_id_emb_dict = &#123;item_index_2_rawid[k]: \</span><br><span class="line">                                v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">zip</span>(item_profile[<span class="string">&#x27;click_article_id&#x27;</span>], item_embs)&#125;</span><br><span class="line">    <span class="comment"># 将Embedding保存到本地</span></span><br><span class="line">    pickle.dump(raw_user_id_emb_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;user_youtube_emb.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    pickle.dump(raw_item_id_emb_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;item_youtube_emb.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># faiss紧邻搜索，通过user_embedding 搜索与其相似性最高的topk个item</span></span><br><span class="line">    index = faiss.IndexFlatIP(embedding_dim)</span><br><span class="line">    <span class="comment"># 上面已经进行了归一化，这里可以不进行归一化了</span></span><br><span class="line"><span class="comment">#     faiss.normalize_L2(user_embs)</span></span><br><span class="line"><span class="comment">#     faiss.normalize_L2(item_embs)</span></span><br><span class="line">    index.add(item_embs) <span class="comment"># 将item向量构建索引</span></span><br><span class="line">    sim, idx = index.search(np.ascontiguousarray(user_embs), topk) <span class="comment"># 通过user去查询最相似的topk个item</span></span><br><span class="line">    </span><br><span class="line">    user_recall_items_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">    <span class="keyword">for</span> target_idx, sim_value_list, rele_idx_list <span class="keyword">in</span> tqdm(<span class="built_in">zip</span>(test_user_model_input[<span class="string">&#x27;user_id&#x27;</span>], sim, idx)):</span><br><span class="line">        target_raw_id = user_index_2_rawid[target_idx]</span><br><span class="line">        <span class="comment"># 从1开始是为了去掉商品本身, 所以最终获得的相似商品只有topk-1</span></span><br><span class="line">        <span class="keyword">for</span> rele_idx, sim_value <span class="keyword">in</span> <span class="built_in">zip</span>(rele_idx_list[<span class="number">1</span>:], sim_value_list[<span class="number">1</span>:]): </span><br><span class="line">            rele_raw_id = item_index_2_rawid[rele_idx]</span><br><span class="line">            user_recall_items_dict[target_raw_id][rele_raw_id] = user_recall_items_dict.get(target_raw_id, &#123;&#125;)\</span><br><span class="line">                                                                    .get(rele_raw_id, <span class="number">0</span>) + sim_value</span><br><span class="line">            </span><br><span class="line">    user_recall_items_dict = &#123;k: <span class="built_in">sorted</span>(v.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>) <span class="keyword">for</span> k, v <span class="keyword">in</span> user_recall_items_dict.items()&#125;</span><br><span class="line">    <span class="comment"># 将召回的结果进行排序</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存召回的结果</span></span><br><span class="line">    <span class="comment"># 这里是直接通过向量的方式得到了召回结果，相比于上面的召回方法，上面的只是得到了i2i及u2u的相似性矩阵，还需要进行协同过滤召回才能得到召回结果</span></span><br><span class="line">    <span class="comment"># 可以直接对这个召回结果进行评估，为了方便可以统一写一个评估函数对所有的召回结果进行评估</span></span><br><span class="line">    pickle.dump(user_recall_items_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;youtube_u2i_dict.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> user_recall_items_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 由于这里需要做召回评估，所以讲训练集中的最后一次点击都提取了出来</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> metric_recall:</span><br><span class="line">    user_multi_recall_dict[<span class="string">&#x27;youtubednn_recall&#x27;</span>] = youtubednn_u2i_dict(all_click_df, topk=<span class="number">20</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)</span><br><span class="line">    user_multi_recall_dict[<span class="string">&#x27;youtubednn_recall&#x27;</span>] = youtubednn_u2i_dict(trn_hist_click_df, topk=<span class="number">20</span>)</span><br><span class="line">    <span class="comment"># 召回效果评估</span></span><br><span class="line">    metrics_recall(user_multi_recall_dict[<span class="string">&#x27;youtubednn_recall&#x27;</span>], trn_last_click_df, topk=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [02:02&lt;00:00, 2038.57it/s]


WARNING:tensorflow:From /home/ryluo/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/ryluo/anaconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:253: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/ryluo/anaconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:253: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From /home/ryluo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
1149673/1149673 [==============================] - 216s 188us/sample - loss: 0.1326


250000it [00:32, 7720.75it/s]</code></pre>
<h3 id="itemCF-recall"><a href="#itemCF-recall" class="headerlink" title="itemCF recall"></a>itemCF recall</h3><p>上面已经通过协同过滤，Embedding检索的方式得到了文章的相似度矩阵，下面使用协同过滤的思想，给用户召回与其历史文章相似的文章。<br>这里在召回的时候，也是用了关联规则的方式：</p>
<ol>
<li>考虑相似文章与历史点击文章顺序的权重(细节看代码)</li>
<li>考虑文章创建时间的权重，也就是考虑相似文章与历史点击文章创建时间差的权重</li>
<li>考虑文章内容相似度权重(使用Embedding计算相似文章相似度，但是这里需要注意，在Embedding的时候并没有计算所有商品两两之间的相似度，所以相似的文章与历史点击文章不存在相似度，需要做特殊处理)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基于商品的召回i2i</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">item_based_recommend</span>(<span class="params">user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于文章协同过滤的召回</span></span><br><span class="line"><span class="string">        :param user_id: 用户id</span></span><br><span class="line"><span class="string">        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   &#123;user1: &#123;item1: time1, item2: time2..&#125;...&#125;</span></span><br><span class="line"><span class="string">        :param i2i_sim: 字典，文章相似性矩阵</span></span><br><span class="line"><span class="string">        :param sim_item_topk: 整数， 选择与当前文章最相似的前k篇文章</span></span><br><span class="line"><span class="string">        :param recall_item_num: 整数， 最后的召回文章数量</span></span><br><span class="line"><span class="string">        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全</span></span><br><span class="line"><span class="string">        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        return: 召回的文章列表 &#123;item1:score1, item2: score2...&#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取用户历史交互的文章</span></span><br><span class="line">    user_hist_items = user_item_time_dict[user_id]</span><br><span class="line">    </span><br><span class="line">    item_rank = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> loc, (i, click_time) <span class="keyword">in</span> <span class="built_in">enumerate</span>(user_hist_items):</span><br><span class="line">        <span class="keyword">for</span> j, wij <span class="keyword">in</span> <span class="built_in">sorted</span>(i2i_sim[i].items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:sim_item_topk]:</span><br><span class="line">            <span class="keyword">if</span> j <span class="keyword">in</span> user_hist_items:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 文章创建时间差权重</span></span><br><span class="line">            created_time_weight = np.exp(<span class="number">0.8</span> ** np.<span class="built_in">abs</span>(item_created_time_dict[i] - item_created_time_dict[j]))</span><br><span class="line">            <span class="comment"># 相似文章和历史点击文章序列中历史文章所在的位置权重</span></span><br><span class="line">            loc_weight = (<span class="number">0.9</span> ** (<span class="built_in">len</span>(user_hist_items) - loc))</span><br><span class="line">            </span><br><span class="line">            content_weight = <span class="number">1.0</span></span><br><span class="line">            <span class="keyword">if</span> emb_i2i_sim.get(i, &#123;&#125;).get(j, <span class="literal">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                content_weight += emb_i2i_sim[i][j]</span><br><span class="line">            <span class="keyword">if</span> emb_i2i_sim.get(j, &#123;&#125;).get(i, <span class="literal">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                content_weight += emb_i2i_sim[j][i]</span><br><span class="line">                </span><br><span class="line">            item_rank.setdefault(j, <span class="number">0</span>)</span><br><span class="line">            item_rank[j] += created_time_weight * loc_weight * content_weight * wij</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 不足10个，用热门商品补全</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(item_rank) &lt; recall_item_num:</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(item_topk_click):</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> item_rank.items(): <span class="comment"># 填充的item应该不在原来的列表中</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            item_rank[item] = - i - <span class="number">100</span> <span class="comment"># 随便给个负数就行</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(item_rank) == recall_item_num:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    item_rank = <span class="built_in">sorted</span>(item_rank.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:recall_item_num]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> item_rank</span><br></pre></td></tr></table></figure>



<h4 id="itemCF-sim召回"><a href="#itemCF-sim召回" class="headerlink" title="itemCF sim召回"></a>itemCF sim召回</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先进行itemcf召回, 为了召回评估，所以提取最后一次点击</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trn_hist_click_df = all_click_df</span><br><span class="line"></span><br><span class="line">user_recall_items_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">user_item_time_dict = get_user_item_time(trn_hist_click_df)</span><br><span class="line"></span><br><span class="line">i2i_sim = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;itemcf_i2i_sim.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">emb_i2i_sim = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;emb_i2i_sim.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">sim_item_topk = <span class="number">20</span></span><br><span class="line">recall_item_num = <span class="number">10</span></span><br><span class="line">item_topk_click = get_item_topk_click(trn_hist_click_df, k=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> tqdm(trn_hist_click_df[<span class="string">&#x27;user_id&#x27;</span>].unique()):</span><br><span class="line">    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, \</span><br><span class="line">                                                        i2i_sim, sim_item_topk, recall_item_num, \</span><br><span class="line">                                                        item_topk_click, item_created_time_dict, emb_i2i_sim)</span><br><span class="line"></span><br><span class="line">user_multi_recall_dict[<span class="string">&#x27;itemcf_sim_itemcf_recall&#x27;</span>] = user_recall_items_dict</span><br><span class="line">pickle.dump(user_multi_recall_dict[<span class="string">&#x27;itemcf_sim_itemcf_recall&#x27;</span>], <span class="built_in">open</span>(save_path + <span class="string">&#x27;itemcf_recall_dict.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    <span class="comment"># 召回效果评估</span></span><br><span class="line">    metrics_recall(user_multi_recall_dict[<span class="string">&#x27;itemcf_sim_itemcf_recall&#x27;</span>], trn_last_click_df, topk=recall_item_num)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [2:51:13&lt;00:00, 24.33it/s]  </code></pre>
<h4 id="embedding-sim-召回"><a href="#embedding-sim-召回" class="headerlink" title="embedding sim 召回"></a>embedding sim 召回</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里是为了召回评估，所以提取最后一次点击</span></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trn_hist_click_df = all_click_df</span><br><span class="line"></span><br><span class="line">user_recall_items_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">user_item_time_dict = get_user_item_time(trn_hist_click_df)</span><br><span class="line">i2i_sim = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;emb_i2i_sim.pkl&#x27;</span>,<span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">sim_item_topk = <span class="number">20</span></span><br><span class="line">recall_item_num = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">item_topk_click = get_item_topk_click(trn_hist_click_df, k=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> tqdm(trn_hist_click_df[<span class="string">&#x27;user_id&#x27;</span>].unique()):</span><br><span class="line">    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk, </span><br><span class="line">                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)</span><br><span class="line">    </span><br><span class="line">user_multi_recall_dict[<span class="string">&#x27;embedding_sim_item_recall&#x27;</span>] = user_recall_items_dict</span><br><span class="line">pickle.dump(user_multi_recall_dict[<span class="string">&#x27;embedding_sim_item_recall&#x27;</span>], <span class="built_in">open</span>(save_path + <span class="string">&#x27;embedding_sim_item_recall.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    <span class="comment"># 召回效果评估</span></span><br><span class="line">    metrics_recall(user_multi_recall_dict[<span class="string">&#x27;embedding_sim_item_recall&#x27;</span>], trn_last_click_df, topk=recall_item_num)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [04:35&lt;00:00, 905.85it/s] </code></pre>
<h3 id="userCF召回"><a href="#userCF召回" class="headerlink" title="userCF召回"></a>userCF召回</h3><p>基于用户协同过滤，核心思想是给用户推荐与其相似的用户历史点击文章，因为这里涉及到了相似用户的历史文章，这里仍然可以加上一些关联规则来给用户可能点击的文章进行加权，这里使用的关联规则主要是考虑相似用户的历史点击文章与被推荐用户历史点击商品的关系权重，而这里的关系就可以直接借鉴基于物品的协同过滤相似的做法，只不过这里是对被推荐物品关系的一个累加的过程，下面是使用的一些关系权重，及相关的代码：</p>
<ol>
<li>计算被推荐用户历史点击文章与相似用户历史点击文章的相似度，文章创建时间差，相对位置的总和，作为各自的权重</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基于用户的召回 u2u2i</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_based_recommend</span>(<span class="params">user_id, user_item_time_dict, u2u_sim, sim_user_topk, recall_item_num, </span></span></span><br><span class="line"><span class="function"><span class="params">                         item_topk_click, item_created_time_dict, emb_i2i_sim</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        基于文章协同过滤的召回</span></span><br><span class="line"><span class="string">        :param user_id: 用户id</span></span><br><span class="line"><span class="string">        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   &#123;user1: &#123;item1: time1, item2: time2..&#125;...&#125;</span></span><br><span class="line"><span class="string">        :param u2u_sim: 字典，文章相似性矩阵</span></span><br><span class="line"><span class="string">        :param sim_user_topk: 整数， 选择与当前用户最相似的前k个用户</span></span><br><span class="line"><span class="string">        :param recall_item_num: 整数， 最后的召回文章数量</span></span><br><span class="line"><span class="string">        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全</span></span><br><span class="line"><span class="string">        :param item_created_time_dict: 文章创建时间列表</span></span><br><span class="line"><span class="string">        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        return: 召回的文章列表 &#123;item1:score1, item2: score2...&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 历史交互</span></span><br><span class="line">    user_item_time_list = user_item_time_dict[user_id]    <span class="comment"># &#123;item1: time1, item2: time2...&#125;</span></span><br><span class="line">    user_hist_items = <span class="built_in">set</span>([i <span class="keyword">for</span> i, t <span class="keyword">in</span> user_item_time_list])   <span class="comment"># 存在一个用户与某篇文章的多次交互， 这里得去重</span></span><br><span class="line">    </span><br><span class="line">    items_rank = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> sim_u, wuv <span class="keyword">in</span> <span class="built_in">sorted</span>(u2u_sim[user_id].items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:sim_user_topk]:</span><br><span class="line">        <span class="keyword">for</span> i, click_time <span class="keyword">in</span> user_item_time_dict[sim_u]:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> user_hist_items:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            items_rank.setdefault(i, <span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">            loc_weight = <span class="number">1.0</span></span><br><span class="line">            content_weight = <span class="number">1.0</span></span><br><span class="line">            created_time_weight = <span class="number">1.0</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 当前文章与该用户看的历史文章进行一个权重交互</span></span><br><span class="line">            <span class="keyword">for</span> loc, (j, click_time) <span class="keyword">in</span> <span class="built_in">enumerate</span>(user_item_time_list):</span><br><span class="line">                <span class="comment"># 点击时的相对位置权重</span></span><br><span class="line">                loc_weight += <span class="number">0.9</span> ** (<span class="built_in">len</span>(user_item_time_list) - loc)</span><br><span class="line">                <span class="comment"># 内容相似性权重</span></span><br><span class="line">                <span class="keyword">if</span> emb_i2i_sim.get(i, &#123;&#125;).get(j, <span class="literal">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    content_weight += emb_i2i_sim[i][j]</span><br><span class="line">                <span class="keyword">if</span> emb_i2i_sim.get(j, &#123;&#125;).get(i, <span class="literal">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    content_weight += emb_i2i_sim[j][i]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 创建时间差权重</span></span><br><span class="line">                created_time_weight += np.exp(<span class="number">0.8</span> * np.<span class="built_in">abs</span>(item_created_time_dict[i] - item_created_time_dict[j]))</span><br><span class="line">                </span><br><span class="line">            items_rank[i] += loc_weight * content_weight * created_time_weight * wuv</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 热度补全</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(items_rank) &lt; recall_item_num:</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(item_topk_click):</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> items_rank.items(): <span class="comment"># 填充的item应该不在原来的列表中</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            items_rank[item] = - i - <span class="number">100</span> <span class="comment"># 随便给个复数就行</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(items_rank) == recall_item_num:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">    items_rank = <span class="built_in">sorted</span>(items_rank.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:recall_item_num]    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> items_rank</span><br></pre></td></tr></table></figure>

<h4 id="userCF-sim召回"><a href="#userCF-sim召回" class="headerlink" title="userCF sim召回"></a>userCF sim召回</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里是为了召回评估，所以提取最后一次点击</span></span><br><span class="line"><span class="comment"># 由于usercf中计算user之间的相似度的过程太费内存了，全量数据这里就没有跑，跑了一个采样之后的数据</span></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trn_hist_click_df = all_click_df</span><br><span class="line">    </span><br><span class="line">user_recall_items_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">user_item_time_dict = get_user_item_time(trn_hist_click_df)</span><br><span class="line"></span><br><span class="line">u2u_sim = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;usercf_u2u_sim.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">sim_user_topk = <span class="number">20</span></span><br><span class="line">recall_item_num = <span class="number">10</span></span><br><span class="line">item_topk_click = get_item_topk_click(trn_hist_click_df, k=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> tqdm(trn_hist_click_df[<span class="string">&#x27;user_id&#x27;</span>].unique()):</span><br><span class="line">    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \</span><br><span class="line">                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)    </span><br><span class="line"></span><br><span class="line">pickle.dump(user_recall_items_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;usercf_u2u2i_recall.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    <span class="comment"># 召回效果评估</span></span><br><span class="line">    metrics_recall(user_recall_items_dict, trn_last_click_df, topk=recall_item_num)</span><br></pre></td></tr></table></figure>



<h4 id="user-embedding-sim召回"><a href="#user-embedding-sim召回" class="headerlink" title="user embedding sim召回"></a>user embedding sim召回</h4><p>虽然没有直接跑usercf的计算用户之间的相似度，为了验证上述基于用户的协同过滤的代码，下面使用了YoutubeDNN过程中产生的user embedding来进行向量检索每个user最相似的topk个user，在使用这里得到的u2u的相似性矩阵，使用usercf进行召回，具体代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用Embedding的方式获取u2u的相似性矩阵</span></span><br><span class="line"><span class="comment"># topk指的是每个user, faiss搜索后返回最相似的topk个user</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">u2u_embdding_sim</span>(<span class="params">click_df, user_emb_dict, save_path, topk</span>):</span></span><br><span class="line">    </span><br><span class="line">    user_list = []</span><br><span class="line">    user_emb_list = []</span><br><span class="line">    <span class="keyword">for</span> user_id, user_emb <span class="keyword">in</span> user_emb_dict.items():</span><br><span class="line">        user_list.append(user_id)</span><br><span class="line">        user_emb_list.append(user_emb)</span><br><span class="line">        </span><br><span class="line">    user_index_2_rawid_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(user_list)), user_list)&#125;    </span><br><span class="line">    </span><br><span class="line">    user_emb_np = np.array(user_emb_list, dtype=np.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 建立faiss索引</span></span><br><span class="line">    user_index = faiss.IndexFlatIP(user_emb_np.shape[<span class="number">1</span>])</span><br><span class="line">    user_index.add(user_emb_np)</span><br><span class="line">    <span class="comment"># 相似度查询，给每个索引位置上的向量返回topk个item以及相似度</span></span><br><span class="line">    sim, idx = user_index.search(user_emb_np, topk) <span class="comment"># 返回的是列表</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 将向量检索的结果保存成原始id的对应关系</span></span><br><span class="line">    user_sim_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">    <span class="keyword">for</span> target_idx, sim_value_list, rele_idx_list <span class="keyword">in</span> tqdm(<span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(user_emb_np)), sim, idx)):</span><br><span class="line">        target_raw_id = user_index_2_rawid_dict[target_idx]</span><br><span class="line">        <span class="comment"># 从1开始是为了去掉商品本身, 所以最终获得的相似商品只有topk-1</span></span><br><span class="line">        <span class="keyword">for</span> rele_idx, sim_value <span class="keyword">in</span> <span class="built_in">zip</span>(rele_idx_list[<span class="number">1</span>:], sim_value_list[<span class="number">1</span>:]): </span><br><span class="line">            rele_raw_id = user_index_2_rawid_dict[rele_idx]</span><br><span class="line">            user_sim_dict[target_raw_id][rele_raw_id] = user_sim_dict.get(target_raw_id, &#123;&#125;).get(rele_raw_id, <span class="number">0</span>) + sim_value</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存i2i相似度矩阵</span></span><br><span class="line">    pickle.dump(user_sim_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;youtube_u2u_sim.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))   </span><br><span class="line">    <span class="keyword">return</span> user_sim_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取YoutubeDNN过程中产生的user embedding, 然后使用faiss计算用户之间的相似度</span></span><br><span class="line"><span class="comment"># 这里需要注意，这里得到的user embedding其实并不是很好，因为YoutubeDNN中使用的是用户点击序列来训练的user embedding,</span></span><br><span class="line"><span class="comment"># 如果序列普遍都比较短的话，其实效果并不是很好</span></span><br><span class="line">user_emb_dict = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;user_youtube_emb.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">u2u_sim = u2u_embdding_sim(all_click_df, user_emb_dict, save_path, topk=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<pre><code>250000it [00:23, 10507.45it/s]</code></pre>
<p>通过YoutubeDNN得到的user_embedding</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用召回评估函数验证当前召回方式的效果</span></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trn_hist_click_df = all_click_df</span><br><span class="line"></span><br><span class="line">user_recall_items_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">user_item_time_dict = get_user_item_time(trn_hist_click_df)</span><br><span class="line">u2u_sim = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;youtube_u2u_sim.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">sim_user_topk = <span class="number">20</span></span><br><span class="line">recall_item_num = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">item_topk_click = get_item_topk_click(trn_hist_click_df, k=<span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> tqdm(trn_hist_click_df[<span class="string">&#x27;user_id&#x27;</span>].unique()):</span><br><span class="line">    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \</span><br><span class="line">                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)</span><br><span class="line">    </span><br><span class="line">user_multi_recall_dict[<span class="string">&#x27;youtubednn_usercf_recall&#x27;</span>] = user_recall_items_dict</span><br><span class="line">pickle.dump(user_multi_recall_dict[<span class="string">&#x27;youtubednn_usercf_recall&#x27;</span>], <span class="built_in">open</span>(save_path + <span class="string">&#x27;youtubednn_usercf_recall.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> metric_recall:</span><br><span class="line">    <span class="comment"># 召回效果评估</span></span><br><span class="line">    metrics_recall(user_multi_recall_dict[<span class="string">&#x27;youtubednn_usercf_recall&#x27;</span>], trn_last_click_df, topk=recall_item_num)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [19:43&lt;00:00, 211.22it/s]</code></pre>
<h2 id="冷启动问题"><a href="#冷启动问题" class="headerlink" title="冷启动问题"></a>冷启动问题</h2><p><strong>冷启动问题可以分成三类：文章冷启动，用户冷启动，系统冷启动。</strong></p>
<ul>
<li>文章冷启动：对于一个平台系统新加入的文章，该文章没有任何的交互记录，如何推荐给用户的问题。(对于我们场景可以认为是，日志数据中没有出现过的文章都可以认为是冷启动的文章)</li>
<li>用户冷启动：对于一个平台系统新来的用户，该用户还没有文章的交互信息，如何给该用户进行推荐。(对于我们场景就是，测试集中的用户是否在测试集对应的log数据中出现过，如果没有出现过，那么可以认为该用户是冷启动用户。但是有时候并没有这么严格，我们也可以自己设定某些指标来判别哪些用户是冷启动用户，比如通过使用时长，点击率，留存率等等)</li>
<li>系统冷启动：就是对于一个平台刚上线，还没有任何的相关历史数据，此时就是系统冷启动，其实也就是前面两种的一个综合。</li>
</ul>
<p><strong>当前场景下冷启动问题的分析：</strong></p>
<p>对当前的数据进行分析会发现，日志中所有出现过的点击文章只有3w多个，而整个文章库中却有30多万，那么测试集中的用户最后一次点击是否会点击没有出现在日志中的文章呢？如果存在这种情况，说明用户点击的文章之前没有任何的交互信息，这也就是我们所说的文章冷启动。通过数据分析还可以发现，测试集用户只有一次点击的数据占得比例还不少，其实仅仅通过用户的一次点击就给用户推荐文章使用模型的方式也是比较难的，这里其实也可以考虑用户冷启动的问题，但是这里只给出物品冷启动的一些解决方案及代码，关于用户冷启动的话提一些可行性的做法。</p>
<ol>
<li>文章冷启动(没有冷启动的探索问题)<br>其实我们这里不是为了做文章的冷启动而做冷启动，而是猜测用户可能会点击一些没有在log数据中出现的文章，我们要做的就是如何从将近27万的文章中选择一些文章作为用户冷启动的文章，这里其实也可以看成是一种召回策略，我们这里就采用简单的比较好理解的基于规则的召回策略来获取用户可能点击的未出现在log数据中的文章。<br>现在的问题变成了：如何给每个用户考虑从27万个商品中获取一小部分商品？随机选一些可能是一种方案。下面给出一些参考的方案。<ol>
<li>首先基于Embedding召回一部分与用户历史相似的文章</li>
<li>从基于Embedding召回的文章中通过一些规则过滤掉一些文章，使得留下的文章用户更可能点击。我们这里的规则，可以是，留下那些与用户历史点击文章主题相同的文章，或者字数相差不大的文章。并且留下的文章尽量是与测试集用户最后一次点击时间更接近的文章，或者是当天的文章也行。</li>
</ol>
</li>
<li>用户冷启动<br>这里对测试集中的用户点击数据进行分析会发现，测试集中有百分之20的用户只有一次点击，那么这些点击特别少的用户的召回是不是可以单独做一些策略上的补充呢？或者是在排序后直接基于规则加上一些文章呢？这些都可以去尝试，这里没有提供具体的做法。</li>
</ol>
<p><strong>注意：</strong>   </p>
<p>这里看似和基于embedding计算的item之间相似度然后做itemcf是一致的，但是现在我们的目的不一样，我们这里的目的是找到相似的向量，并且还没有出现在log日志中的商品，再加上一些其他的冷启动的策略，这里需要找回的数量会偏多一点，不然被筛选完之后可能都没有文章了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先进行itemcf召回，这里不需要做召回评估，这里只是一种策略</span></span><br><span class="line">trn_hist_click_df = all_click_df</span><br><span class="line"></span><br><span class="line">user_recall_items_dict = collections.defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">user_item_time_dict = get_user_item_time(trn_hist_click_df)</span><br><span class="line">i2i_sim = pickle.load(<span class="built_in">open</span>(save_path + <span class="string">&#x27;emb_i2i_sim.pkl&#x27;</span>,<span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">sim_item_topk = <span class="number">150</span></span><br><span class="line">recall_item_num = <span class="number">100</span> <span class="comment"># 稍微召回多一点文章，便于后续的规则筛选</span></span><br><span class="line"></span><br><span class="line">item_topk_click = get_item_topk_click(trn_hist_click_df, k=<span class="number">50</span>)</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> tqdm(trn_hist_click_df[<span class="string">&#x27;user_id&#x27;</span>].unique()):</span><br><span class="line">    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk, </span><br><span class="line">                                                        recall_item_num, item_topk_click,item_created_time_dict, emb_i2i_sim)</span><br><span class="line">pickle.dump(user_recall_items_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;cold_start_items_raw_dict.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [05:01&lt;00:00, 828.60it/s] </code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基于规则进行文章过滤</span></span><br><span class="line"><span class="comment"># 保留文章主题与用户历史浏览主题相似的文章</span></span><br><span class="line"><span class="comment"># 保留文章字数与用户历史浏览文章字数相差不大的文章</span></span><br><span class="line"><span class="comment"># 保留最后一次点击当天的文章</span></span><br><span class="line"><span class="comment"># 按照相似度返回最终的结果</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_click_article_ids_set</span>(<span class="params">all_click_df</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>(all_click_df.click_article_id.values)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cold_start_items</span>(<span class="params">user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \</span></span></span><br><span class="line"><span class="function"><span class="params">                     user_last_item_created_time_dict, item_type_dict, item_words_dict, </span></span></span><br><span class="line"><span class="function"><span class="params">                     item_created_time_dict, click_article_ids_set, recall_item_num</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        冷启动的情况下召回一些文章</span></span><br><span class="line"><span class="string">        :param user_recall_items_dict: 基于内容embedding相似性召回来的很多文章， 字典， &#123;user1: [item1, item2, ..], &#125;</span></span><br><span class="line"><span class="string">        :param user_hist_item_typs_dict: 字典， 用户点击的文章的主题映射</span></span><br><span class="line"><span class="string">        :param user_hist_item_words_dict: 字典， 用户点击的历史文章的字数映射</span></span><br><span class="line"><span class="string">        :param user_last_item_created_time_idct: 字典，用户点击的历史文章创建时间映射</span></span><br><span class="line"><span class="string">        :param item_tpye_idct: 字典，文章主题映射</span></span><br><span class="line"><span class="string">        :param item_words_dict: 字典，文章字数映射</span></span><br><span class="line"><span class="string">        :param item_created_time_dict: 字典， 文章创建时间映射</span></span><br><span class="line"><span class="string">        :param click_article_ids_set: 集合，用户点击过得文章, 也就是日志里面出现过的文章</span></span><br><span class="line"><span class="string">        :param recall_item_num: 召回文章的数量， 这个指的是没有出现在日志里面的文章数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    cold_start_user_items_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> user, item_list <span class="keyword">in</span> tqdm(user_recall_items_dict.items()):</span><br><span class="line">        cold_start_user_items_dict.setdefault(user, [])</span><br><span class="line">        <span class="keyword">for</span> item, score <span class="keyword">in</span> item_list:</span><br><span class="line">            <span class="comment"># 获取历史文章信息</span></span><br><span class="line">            hist_item_type_set = user_hist_item_typs_dict[user]</span><br><span class="line">            hist_mean_words = user_hist_item_words_dict[user]</span><br><span class="line">            hist_last_item_created_time = user_last_item_created_time_dict[user]</span><br><span class="line">            hist_last_item_created_time = datetime.fromtimestamp(hist_last_item_created_time)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 获取当前召回文章的信息</span></span><br><span class="line">            curr_item_type = item_type_dict[item]</span><br><span class="line">            curr_item_words = item_words_dict[item]</span><br><span class="line">            curr_item_created_time = item_created_time_dict[item]</span><br><span class="line">            curr_item_created_time = datetime.fromtimestamp(curr_item_created_time)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 首先，文章不能出现在用户的历史点击中， 然后根据文章主题，文章单词数，文章创建时间进行筛选</span></span><br><span class="line">            <span class="keyword">if</span> curr_item_type <span class="keyword">not</span> <span class="keyword">in</span> hist_item_type_set <span class="keyword">or</span> \</span><br><span class="line">                item <span class="keyword">in</span> click_article_ids_set <span class="keyword">or</span> \</span><br><span class="line">                <span class="built_in">abs</span>(curr_item_words - hist_mean_words) &gt; <span class="number">200</span> <span class="keyword">or</span> \</span><br><span class="line">                <span class="built_in">abs</span>((curr_item_created_time - hist_last_item_created_time).days) &gt; <span class="number">90</span>: </span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">                </span><br><span class="line">            cold_start_user_items_dict[user].append((item, score))      <span class="comment"># &#123;user1: [(item1, score1), (item2, score2)..]...&#125;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 需要控制一下冷启动召回的数量</span></span><br><span class="line">    cold_start_user_items_dict = &#123;k: <span class="built_in">sorted</span>(v, key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:recall_item_num] \</span><br><span class="line">                                  <span class="keyword">for</span> k, v <span class="keyword">in</span> cold_start_user_items_dict.items()&#125;</span><br><span class="line">    </span><br><span class="line">    pickle.dump(cold_start_user_items_dict, <span class="built_in">open</span>(save_path + <span class="string">&#x27;cold_start_user_items_dict.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cold_start_user_items_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_click_df_ = all_click_df.copy()</span><br><span class="line">all_click_df_ = all_click_df_.merge(item_info_df, how=<span class="string">&#x27;left&#x27;</span>, on=<span class="string">&#x27;click_article_id&#x27;</span>)</span><br><span class="line">user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict = get_user_hist_item_info_dict(all_click_df_)</span><br><span class="line">click_article_ids_set = get_click_article_ids_set(all_click_df)</span><br><span class="line"><span class="comment"># 需要注意的是</span></span><br><span class="line"><span class="comment"># 这里使用了很多规则来筛选冷启动的文章，所以前面再召回的阶段就应该尽可能的多召回一些文章，否则很容易被删掉</span></span><br><span class="line">cold_start_user_items_dict = cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \</span><br><span class="line">                                              user_last_item_created_time_dict, item_type_dict, item_words_dict, \</span><br><span class="line">                                              item_created_time_dict, click_article_ids_set, recall_item_num)</span><br><span class="line"></span><br><span class="line">user_multi_recall_dict[<span class="string">&#x27;cold_start_recall&#x27;</span>] = cold_start_user_items_dict</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 250000/250000 [01:49&lt;00:00, 2293.37it/s]</code></pre>
<h2 id="多路召回合并"><a href="#多路召回合并" class="headerlink" title="多路召回合并"></a>多路召回合并</h2><p>多路召回合并就是将前面所有的召回策略得到的用户文章列表合并起来，下面是对前面所有召回结果的汇总</p>
<ol>
<li>基于itemcf计算的item之间的相似度sim进行的召回 </li>
<li>基于embedding搜索得到的item之间的相似度进行的召回</li>
<li>YoutubeDNN召回</li>
<li>YoutubeDNN得到的user之间的相似度进行的召回</li>
<li>基于冷启动策略的召回</li>
</ol>
<p><strong>注意：</strong><br>在做召回评估的时候就会发现有些召回的效果不错有些召回的效果很差，所以对每一路召回的结果，我们可以认为的定义一些权重，来做最终的相似度融合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine_recall_results</span>(<span class="params">user_multi_recall_dict, weight_dict=<span class="literal">None</span>, topk=<span class="number">25</span></span>):</span></span><br><span class="line">    final_recall_items_dict = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对每一种召回结果按照用户进行归一化，方便后面多种召回结果，相同用户的物品之间权重相加</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">norm_user_recall_items_sim</span>(<span class="params">sorted_item_list</span>):</span></span><br><span class="line">        <span class="comment"># 如果冷启动中没有文章或者只有一篇文章，直接返回，出现这种情况的原因可能是冷启动召回的文章数量太少了，</span></span><br><span class="line">        <span class="comment"># 基于规则筛选之后就没有文章了, 这里还可以做一些其他的策略性的筛选</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(sorted_item_list) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> sorted_item_list</span><br><span class="line">        </span><br><span class="line">        min_sim = sorted_item_list[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">        max_sim = sorted_item_list[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        norm_sorted_item_list = []</span><br><span class="line">        <span class="keyword">for</span> item, score <span class="keyword">in</span> sorted_item_list:</span><br><span class="line">            <span class="keyword">if</span> max_sim &gt; <span class="number">0</span>:</span><br><span class="line">                norm_score = <span class="number">1.0</span> * (score - min_sim) / (max_sim - min_sim) <span class="keyword">if</span> max_sim &gt; min_sim <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                norm_score = <span class="number">0.0</span></span><br><span class="line">            norm_sorted_item_list.append((item, norm_score))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> norm_sorted_item_list</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">&#x27;多路召回合并...&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> method, user_recall_items <span class="keyword">in</span> tqdm(user_multi_recall_dict.items()):</span><br><span class="line">        print(method + <span class="string">&#x27;...&#x27;</span>)</span><br><span class="line">        <span class="comment"># 在计算最终召回结果的时候，也可以为每一种召回结果设置一个权重</span></span><br><span class="line">        <span class="keyword">if</span> weight_dict == <span class="literal">None</span>:</span><br><span class="line">            recall_method_weight = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            recall_method_weight = weight_dict[method]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> user_id, sorted_item_list <span class="keyword">in</span> user_recall_items.items(): <span class="comment"># 进行归一化</span></span><br><span class="line">            user_recall_items[user_id] = norm_user_recall_items_sim(sorted_item_list)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> user_id, sorted_item_list <span class="keyword">in</span> user_recall_items.items():</span><br><span class="line">            <span class="comment"># print(&#x27;user_id&#x27;)</span></span><br><span class="line">            final_recall_items_dict.setdefault(user_id, &#123;&#125;)</span><br><span class="line">            <span class="keyword">for</span> item, score <span class="keyword">in</span> sorted_item_list:</span><br><span class="line">                final_recall_items_dict[user_id].setdefault(item, <span class="number">0</span>)</span><br><span class="line">                final_recall_items_dict[user_id][item] += recall_method_weight * score  </span><br><span class="line">    </span><br><span class="line">    final_recall_items_dict_rank = &#123;&#125;</span><br><span class="line">    <span class="comment"># 多路召回时也可以控制最终的召回数量</span></span><br><span class="line">    <span class="keyword">for</span> user, recall_item_dict <span class="keyword">in</span> final_recall_items_dict.items():</span><br><span class="line">        final_recall_items_dict_rank[user] = <span class="built_in">sorted</span>(recall_item_dict.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:topk]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将多路召回后的最终结果字典保存到本地</span></span><br><span class="line">    pickle.dump(final_recall_items_dict, <span class="built_in">open</span>(os.path.join(save_path, <span class="string">&#x27;final_recall_items_dict.pkl&#x27;</span>),<span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_recall_items_dict_rank</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里直接对多路召回的权重给了一个相同的值，其实可以根据前面召回的情况来调整参数的值</span></span><br><span class="line">weight_dict = &#123;<span class="string">&#x27;itemcf_sim_itemcf_recall&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">               <span class="string">&#x27;embedding_sim_item_recall&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">               <span class="string">&#x27;youtubednn_recall&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line">               <span class="string">&#x27;youtubednn_usercf_recall&#x27;</span>: <span class="number">1.0</span>, </span><br><span class="line">               <span class="string">&#x27;cold_start_recall&#x27;</span>: <span class="number">1.0</span>&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 最终合并之后每个用户召回150个商品进行排序</span></span><br><span class="line">final_recall_items_dict_rank = combine_recall_results(user_multi_recall_dict, weight_dict, topk=<span class="number">150</span>)</span><br></pre></td></tr></table></figure>

<pre><code>  0%|          | 0/5 [00:00&lt;?, ?it/s]

多路召回合并...
itemcf_sim_itemcf_recall...


 20%|██        | 1/5 [00:08&lt;00:34,  8.66s/it]

embedding_sim_item_recall...


 40%|████      | 2/5 [00:16&lt;00:24,  8.29s/it]

youtubednn_recall...
youtubednn_usercf_recall...


 80%|████████  | 4/5 [00:23&lt;00:06,  6.98s/it]

cold_start_recall...


100%|██████████| 5/5 [00:42&lt;00:00,  8.40s/it]</code></pre>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上述实现了如下召回策略：</p>
<ol>
<li>基于关联规则的itemcf</li>
<li>基于关联规则的usercf</li>
<li>youtubednn召回</li>
<li>冷启动召回</li>
</ol>
<p>对于上述实现的召回策略其实都不是最优的结果，我们只是做了个简单的尝试，其中还有很多地方可以优化，包括已经实现的这些召回策略的参数或者新加一些，修改一些关联规则都可以。当然还可以尝试更多的召回策略，比如对新闻进行热度召回等等。</p>
]]></content>
  </entry>
  <entry>
    <title>天池推荐系统入门赛——排序模型+模型融合</title>
    <url>/2020/12/02/%E5%A4%A9%E6%B1%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%85%A5%E9%97%A8%E8%B5%9B%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B+%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<h1 id="排序模型"><a href="#排序模型" class="headerlink" title="排序模型"></a>排序模型</h1><p>通过召回的操作， 我们已经进行了问题规模的缩减， 对于每个用户， 选择出了N篇文章作为了候选集，并基于召回的候选集构建了与用户历史相关的特征，以及用户本身的属性特征，文章本省的属性特征，以及用户与文章之间的特征，下面就是使用机器学习模型来对构造好的特征进行学习，然后对测试集进行预测，得到测试集中的每个候选集用户点击的概率，返回点击概率最大的topk个文章，作为最终的结果。</p>
<p>排序阶段选择了三个比较有代表性的排序模型，它们分别是：</p>
<ol>
<li>LGB的排序模型</li>
<li>LGB的分类模型</li>
<li>深度学习的分类模型DIN</li>
</ol>
<p>得到了最终的排序模型输出的结果之后，还选择了两种比较经典的模型集成的方法：</p>
<ol>
<li>输出结果加权融合</li>
<li>Staking（将模型的输出结果再使用一个简单模型进行预测）</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> gc, os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="读取排序特征"><a href="#读取排序特征" class="headerlink" title="读取排序特征"></a>读取排序特征</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_path = <span class="string">&#x27;./data_raw/&#x27;</span></span><br><span class="line">save_path = <span class="string">&#x27;./temp_results/&#x27;</span></span><br><span class="line">offline = <span class="literal">False</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重新读取数据的时候，发现click_article_id是一个浮点数，所以将其转换成int类型</span></span><br><span class="line">trn_user_item_feats_df = pd.read_csv(save_path + <span class="string">&#x27;trn_user_item_feats_df.csv&#x27;</span>)</span><br><span class="line">trn_user_item_feats_df[<span class="string">&#x27;click_article_id&#x27;</span>] = trn_user_item_feats_df[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    val_user_item_feats_df = pd.read_csv(save_path + <span class="string">&#x27;val_user_item_feats_df.csv&#x27;</span>)</span><br><span class="line">    val_user_item_feats_df[<span class="string">&#x27;click_article_id&#x27;</span>] = val_user_item_feats_df[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">tst_user_item_feats_df = pd.read_csv(save_path + <span class="string">&#x27;tst_user_item_feats_df.csv&#x27;</span>)</span><br><span class="line">tst_user_item_feats_df[<span class="string">&#x27;click_article_id&#x27;</span>] = tst_user_item_feats_df[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做特征的时候为了方便，给测试集也打上了一个无效的标签，这里直接删掉就行</span></span><br><span class="line"><span class="keyword">del</span> tst_user_item_feats_df[<span class="string">&#x27;label&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="返回排序后的结果"><a href="#返回排序后的结果" class="headerlink" title="返回排序后的结果"></a>返回排序后的结果</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submit</span>(<span class="params">recall_df, topk=<span class="number">5</span>, model_name=<span class="literal">None</span></span>):</span></span><br><span class="line">    recall_df = recall_df.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">    recall_df[<span class="string">&#x27;rank&#x27;</span>] = recall_df.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 判断是不是每个用户都有5篇文章及以上</span></span><br><span class="line">    tmp = recall_df.groupby(<span class="string">&#x27;user_id&#x27;</span>).apply(<span class="keyword">lambda</span> x: x[<span class="string">&#x27;rank&#x27;</span>].<span class="built_in">max</span>())</span><br><span class="line">    <span class="keyword">assert</span> tmp.<span class="built_in">min</span>() &gt;= topk</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">del</span> recall_df[<span class="string">&#x27;pred_score&#x27;</span>]</span><br><span class="line">    submit = recall_df[recall_df[<span class="string">&#x27;rank&#x27;</span>] &lt;= topk].set_index([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;rank&#x27;</span>]).unstack(-<span class="number">1</span>).reset_index()</span><br><span class="line">    </span><br><span class="line">    submit.columns = [<span class="built_in">int</span>(col) <span class="keyword">if</span> <span class="built_in">isinstance</span>(col, <span class="built_in">int</span>) <span class="keyword">else</span> col <span class="keyword">for</span> col <span class="keyword">in</span> submit.columns.droplevel(<span class="number">0</span>)]</span><br><span class="line">    <span class="comment"># 按照提交格式定义列名</span></span><br><span class="line">    submit = submit.rename(columns=&#123;<span class="string">&#x27;&#x27;</span>: <span class="string">&#x27;user_id&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;article_1&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;article_2&#x27;</span>, </span><br><span class="line">                                                  <span class="number">3</span>: <span class="string">&#x27;article_3&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;article_4&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;article_5&#x27;</span>&#125;)</span><br><span class="line">    </span><br><span class="line">    save_name = save_path + model_name + <span class="string">&#x27;_&#x27;</span> + datetime.today().strftime(<span class="string">&#x27;%m-%d&#x27;</span>) + <span class="string">&#x27;.csv&#x27;</span></span><br><span class="line">    submit.to_csv(save_name, index=<span class="literal">False</span>, header=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排序结果归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm_sim</span>(<span class="params">sim_df, weight=<span class="number">0.0</span></span>):</span></span><br><span class="line">    <span class="comment"># print(sim_df.head())</span></span><br><span class="line">    min_sim = sim_df.<span class="built_in">min</span>()</span><br><span class="line">    max_sim = sim_df.<span class="built_in">max</span>()</span><br><span class="line">    <span class="keyword">if</span> max_sim == min_sim:</span><br><span class="line">        sim_df = sim_df.apply(<span class="keyword">lambda</span> sim: <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sim_df = sim_df.apply(<span class="keyword">lambda</span> sim: <span class="number">1.0</span> * (sim - min_sim) / (max_sim - min_sim))</span><br><span class="line"></span><br><span class="line">    sim_df = sim_df.apply(<span class="keyword">lambda</span> sim: sim + weight)  <span class="comment"># plus one</span></span><br><span class="line">    <span class="keyword">return</span> sim_df</span><br></pre></td></tr></table></figure>

<h2 id="LGB排序模型"><a href="#LGB排序模型" class="headerlink" title="LGB排序模型"></a>LGB排序模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 防止中间出错之后重新读取数据</span></span><br><span class="line">trn_user_item_feats_df_rank_model = trn_user_item_feats_df.copy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    val_user_item_feats_df_rank_model = val_user_item_feats_df.copy()</span><br><span class="line">    </span><br><span class="line">tst_user_item_feats_df_rank_model = tst_user_item_feats_df.copy()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义特征列</span></span><br><span class="line">lgb_cols = [<span class="string">&#x27;sim0&#x27;</span>, <span class="string">&#x27;time_diff0&#x27;</span>, <span class="string">&#x27;word_diff0&#x27;</span>,<span class="string">&#x27;sim_max&#x27;</span>, <span class="string">&#x27;sim_min&#x27;</span>, <span class="string">&#x27;sim_sum&#x27;</span>, </span><br><span class="line">            <span class="string">&#x27;sim_mean&#x27;</span>, <span class="string">&#x27;score&#x27;</span>,<span class="string">&#x27;click_size&#x27;</span>, <span class="string">&#x27;time_diff_mean&#x27;</span>, <span class="string">&#x27;active_level&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;click_environment&#x27;</span>,<span class="string">&#x27;click_deviceGroup&#x27;</span>, <span class="string">&#x27;click_os&#x27;</span>, <span class="string">&#x27;click_country&#x27;</span>, </span><br><span class="line">            <span class="string">&#x27;click_region&#x27;</span>,<span class="string">&#x27;click_referrer_type&#x27;</span>, <span class="string">&#x27;user_time_hob1&#x27;</span>, <span class="string">&#x27;user_time_hob2&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;words_hbo&#x27;</span>, <span class="string">&#x27;category_id&#x27;</span>, <span class="string">&#x27;created_at_ts&#x27;</span>,<span class="string">&#x27;words_count&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排序模型分组</span></span><br><span class="line">trn_user_item_feats_df_rank_model.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">g_train = trn_user_item_feats_df_rank_model.groupby([<span class="string">&#x27;user_id&#x27;</span>], as_index=<span class="literal">False</span>).count()[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    val_user_item_feats_df_rank_model.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">    g_val = val_user_item_feats_df_rank_model.groupby([<span class="string">&#x27;user_id&#x27;</span>], as_index=<span class="literal">False</span>).count()[<span class="string">&quot;label&quot;</span>].values</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排序模型定义</span></span><br><span class="line">lgb_ranker = lgb.LGBMRanker(boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, num_leaves=<span class="number">31</span>, reg_alpha=<span class="number">0.0</span>, reg_lambda=<span class="number">1</span>,</span><br><span class="line">                            max_depth=-<span class="number">1</span>, n_estimators=<span class="number">100</span>, subsample=<span class="number">0.7</span>, colsample_bytree=<span class="number">0.7</span>, subsample_freq=<span class="number">1</span>,</span><br><span class="line">                            learning_rate=<span class="number">0.01</span>, min_child_weight=<span class="number">50</span>, random_state=<span class="number">2018</span>, n_jobs= <span class="number">16</span>)  </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排序模型训练</span></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    lgb_ranker.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model[<span class="string">&#x27;label&#x27;</span>], group=g_train,</span><br><span class="line">                eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model[<span class="string">&#x27;label&#x27;</span>])], </span><br><span class="line">                eval_group= [g_val], eval_at=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], eval_metric=[<span class="string">&#x27;ndcg&#x27;</span>, ], early_stopping_rounds=<span class="number">50</span>, )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    lgb_ranker.fit(trn_user_item_feats_df[lgb_cols], trn_user_item_feats_df[<span class="string">&#x27;label&#x27;</span>], group=g_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">tst_user_item_feats_df[<span class="string">&#x27;pred_score&#x27;</span>] = lgb_ranker.predict(tst_user_item_feats_df[lgb_cols], num_iteration=lgb_ranker.best_iteration_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将这里的排序结果保存一份，用户后面的模型融合</span></span><br><span class="line">tst_user_item_feats_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;lgb_ranker_score.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测结果重新排序, 及生成提交结果</span></span><br><span class="line">rank_results = tst_user_item_feats_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]]</span><br><span class="line">rank_results[<span class="string">&#x27;click_article_id&#x27;</span>] = rank_results[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line">submit(rank_results, topk=<span class="number">5</span>, model_name=<span class="string">&#x27;lgb_ranker&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分</span></span><br><span class="line"><span class="comment">#  这一部分与前面的单独训练和验证是分开的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_kfold_users</span>(<span class="params">trn_df, n=<span class="number">5</span></span>):</span></span><br><span class="line">    user_ids = trn_df[<span class="string">&#x27;user_id&#x27;</span>].unique()</span><br><span class="line">    user_set = [user_ids[i::n] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    <span class="keyword">return</span> user_set</span><br><span class="line"></span><br><span class="line">k_fold = <span class="number">5</span></span><br><span class="line">trn_df = trn_user_item_feats_df_rank_model</span><br><span class="line">user_set = get_kfold_users(trn_df, n=k_fold)</span><br><span class="line"></span><br><span class="line">score_list = []</span><br><span class="line">score_df = trn_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>,<span class="string">&#x27;label&#x27;</span>]]</span><br><span class="line">sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 五折交叉验证，并将中间结果保存用于staking</span></span><br><span class="line"><span class="keyword">for</span> n_fold, valid_user <span class="keyword">in</span> <span class="built_in">enumerate</span>(user_set):</span><br><span class="line">    train_idx = trn_df[~trn_df[<span class="string">&#x27;user_id&#x27;</span>].isin(valid_user)] <span class="comment"># add slide user</span></span><br><span class="line">    valid_idx = trn_df[trn_df[<span class="string">&#x27;user_id&#x27;</span>].isin(valid_user)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练集与验证集的用户分组</span></span><br><span class="line">    train_idx.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">    g_train = train_idx.groupby([<span class="string">&#x27;user_id&#x27;</span>], as_index=<span class="literal">False</span>).count()[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line">    </span><br><span class="line">    valid_idx.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line">    g_val = valid_idx.groupby([<span class="string">&#x27;user_id&#x27;</span>], as_index=<span class="literal">False</span>).count()[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    lgb_ranker = lgb.LGBMRanker(boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, num_leaves=<span class="number">31</span>, reg_alpha=<span class="number">0.0</span>, reg_lambda=<span class="number">1</span>,</span><br><span class="line">                            max_depth=-<span class="number">1</span>, n_estimators=<span class="number">100</span>, subsample=<span class="number">0.7</span>, colsample_bytree=<span class="number">0.7</span>, subsample_freq=<span class="number">1</span>,</span><br><span class="line">                            learning_rate=<span class="number">0.01</span>, min_child_weight=<span class="number">50</span>, random_state=<span class="number">2018</span>, n_jobs= <span class="number">16</span>)  </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    lgb_ranker.fit(train_idx[lgb_cols], train_idx[<span class="string">&#x27;label&#x27;</span>], group=g_train,</span><br><span class="line">                   eval_set=[(valid_idx[lgb_cols], valid_idx[<span class="string">&#x27;label&#x27;</span>])], eval_group= [g_val], </span><br><span class="line">                   eval_at=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], eval_metric=[<span class="string">&#x27;ndcg&#x27;</span>, ], early_stopping_rounds=<span class="number">50</span>, )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测验证集结果</span></span><br><span class="line">    valid_idx[<span class="string">&#x27;pred_score&#x27;</span>] = lgb_ranker.predict(valid_idx[lgb_cols], num_iteration=lgb_ranker.best_iteration_)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输出结果进行归一化</span></span><br><span class="line">    valid_idx[<span class="string">&#x27;pred_score&#x27;</span>] = valid_idx[[<span class="string">&#x27;pred_score&#x27;</span>]].transform(<span class="keyword">lambda</span> x: norm_sim(x))</span><br><span class="line">    </span><br><span class="line">    valid_idx.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">    valid_idx[<span class="string">&#x27;pred_rank&#x27;</span>] = valid_idx.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将验证集的预测结果放到一个列表中，后面进行拼接</span></span><br><span class="line">    score_list.append(valid_idx[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> offline:</span><br><span class="line">        sub_preds += lgb_ranker.predict(tst_user_item_feats_df_rank_model[lgb_cols], lgb_ranker.best_iteration_)</span><br><span class="line">    </span><br><span class="line">score_df_ = pd.concat(score_list, axis=<span class="number">0</span>)</span><br><span class="line">score_df = score_df.merge(score_df_, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>])</span><br><span class="line"><span class="comment"># 保存训练集交叉验证产生的新特征</span></span><br><span class="line">score_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;trn_lgb_ranker_feats.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征</span></span><br><span class="line">tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_score&#x27;</span>] = sub_preds / k_fold</span><br><span class="line">tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_score&#x27;</span>] = tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_score&#x27;</span>].transform(<span class="keyword">lambda</span> x: norm_sim(x))</span><br><span class="line">tst_user_item_feats_df_rank_model.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_rank&#x27;</span>] = tst_user_item_feats_df_rank_model.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存测试集交叉验证的新特征</span></span><br><span class="line">tst_user_item_feats_df_rank_model[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;tst_lgb_ranker_feats.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测结果重新排序, 及生成提交结果</span></span><br><span class="line"><span class="comment"># 单模型生成提交结果</span></span><br><span class="line">rank_results = tst_user_item_feats_df_rank_model[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]]</span><br><span class="line">rank_results[<span class="string">&#x27;click_article_id&#x27;</span>] = rank_results[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line">submit(rank_results, topk=<span class="number">5</span>, model_name=<span class="string">&#x27;lgb_ranker&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="LGB分类模型"><a href="#LGB分类模型" class="headerlink" title="LGB分类模型"></a>LGB分类模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型及参数的定义</span></span><br><span class="line">lgb_Classfication = lgb.LGBMClassifier(boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, num_leaves=<span class="number">31</span>, reg_alpha=<span class="number">0.0</span>, reg_lambda=<span class="number">1</span>,</span><br><span class="line">                            max_depth=-<span class="number">1</span>, n_estimators=<span class="number">500</span>, subsample=<span class="number">0.7</span>, colsample_bytree=<span class="number">0.7</span>, subsample_freq=<span class="number">1</span>,</span><br><span class="line">                            learning_rate=<span class="number">0.01</span>, min_child_weight=<span class="number">50</span>, random_state=<span class="number">2018</span>, n_jobs= <span class="number">16</span>, verbose=<span class="number">10</span>)  </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model[<span class="string">&#x27;label&#x27;</span>],</span><br><span class="line">                    eval_set=[(val_user_item_feats_df_rank_model[lgb_cols], val_user_item_feats_df_rank_model[<span class="string">&#x27;label&#x27;</span>])], </span><br><span class="line">                    eval_metric=[<span class="string">&#x27;auc&#x27;</span>, ],early_stopping_rounds=<span class="number">50</span>, )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    lgb_Classfication.fit(trn_user_item_feats_df_rank_model[lgb_cols], trn_user_item_feats_df_rank_model[<span class="string">&#x27;label&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">tst_user_item_feats_df[<span class="string">&#x27;pred_score&#x27;</span>] = lgb_Classfication.predict_proba(tst_user_item_feats_df[lgb_cols])[:,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将这里的排序结果保存一份，用户后面的模型融合</span></span><br><span class="line">tst_user_item_feats_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;lgb_cls_score.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测结果重新排序, 及生成提交结果</span></span><br><span class="line">rank_results = tst_user_item_feats_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]]</span><br><span class="line">rank_results[<span class="string">&#x27;click_article_id&#x27;</span>] = rank_results[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line">submit(rank_results, topk=<span class="number">5</span>, model_name=<span class="string">&#x27;lgb_cls&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分</span></span><br><span class="line"><span class="comment">#  这一部分与前面的单独训练和验证是分开的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_kfold_users</span>(<span class="params">trn_df, n=<span class="number">5</span></span>):</span></span><br><span class="line">    user_ids = trn_df[<span class="string">&#x27;user_id&#x27;</span>].unique()</span><br><span class="line">    user_set = [user_ids[i::n] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    <span class="keyword">return</span> user_set</span><br><span class="line"></span><br><span class="line">k_fold = <span class="number">5</span></span><br><span class="line">trn_df = trn_user_item_feats_df_rank_model</span><br><span class="line">user_set = get_kfold_users(trn_df, n=k_fold)</span><br><span class="line"></span><br><span class="line">score_list = []</span><br><span class="line">score_df = trn_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]]</span><br><span class="line">sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 五折交叉验证，并将中间结果保存用于staking</span></span><br><span class="line"><span class="keyword">for</span> n_fold, valid_user <span class="keyword">in</span> <span class="built_in">enumerate</span>(user_set):</span><br><span class="line">    train_idx = trn_df[~trn_df[<span class="string">&#x27;user_id&#x27;</span>].isin(valid_user)] <span class="comment"># add slide user</span></span><br><span class="line">    valid_idx = trn_df[trn_df[<span class="string">&#x27;user_id&#x27;</span>].isin(valid_user)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型及参数的定义</span></span><br><span class="line">    lgb_Classfication = lgb.LGBMClassifier(boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, num_leaves=<span class="number">31</span>, reg_alpha=<span class="number">0.0</span>, reg_lambda=<span class="number">1</span>,</span><br><span class="line">                            max_depth=-<span class="number">1</span>, n_estimators=<span class="number">100</span>, subsample=<span class="number">0.7</span>, colsample_bytree=<span class="number">0.7</span>, subsample_freq=<span class="number">1</span>,</span><br><span class="line">                            learning_rate=<span class="number">0.01</span>, min_child_weight=<span class="number">50</span>, random_state=<span class="number">2018</span>, n_jobs= <span class="number">16</span>, verbose=<span class="number">10</span>)  </span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    lgb_Classfication.fit(train_idx[lgb_cols], train_idx[<span class="string">&#x27;label&#x27;</span>],eval_set=[(valid_idx[lgb_cols], valid_idx[<span class="string">&#x27;label&#x27;</span>])], </span><br><span class="line">                          eval_metric=[<span class="string">&#x27;auc&#x27;</span>, ],early_stopping_rounds=<span class="number">50</span>, )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测验证集结果</span></span><br><span class="line">    valid_idx[<span class="string">&#x27;pred_score&#x27;</span>] = lgb_Classfication.predict_proba(valid_idx[lgb_cols], </span><br><span class="line">                                                              num_iteration=lgb_Classfication.best_iteration_)[:,<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输出结果进行归一化 分类模型输出的值本身就是一个概率值不需要进行归一化</span></span><br><span class="line">    <span class="comment"># valid_idx[&#x27;pred_score&#x27;] = valid_idx[[&#x27;pred_score&#x27;]].transform(lambda x: norm_sim(x))</span></span><br><span class="line">    </span><br><span class="line">    valid_idx.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">    valid_idx[<span class="string">&#x27;pred_rank&#x27;</span>] = valid_idx.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将验证集的预测结果放到一个列表中，后面进行拼接</span></span><br><span class="line">    score_list.append(valid_idx[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> offline:</span><br><span class="line">        sub_preds += lgb_Classfication.predict_proba(tst_user_item_feats_df_rank_model[lgb_cols], </span><br><span class="line">                                                     num_iteration=lgb_Classfication.best_iteration_)[:,<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">score_df_ = pd.concat(score_list, axis=<span class="number">0</span>)</span><br><span class="line">score_df = score_df.merge(score_df_, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>])</span><br><span class="line"><span class="comment"># 保存训练集交叉验证产生的新特征</span></span><br><span class="line">score_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;trn_lgb_cls_feats.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征</span></span><br><span class="line">tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_score&#x27;</span>] = sub_preds / k_fold</span><br><span class="line">tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_score&#x27;</span>] = tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_score&#x27;</span>].transform(<span class="keyword">lambda</span> x: norm_sim(x))</span><br><span class="line">tst_user_item_feats_df_rank_model.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">tst_user_item_feats_df_rank_model[<span class="string">&#x27;pred_rank&#x27;</span>] = tst_user_item_feats_df_rank_model.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存测试集交叉验证的新特征</span></span><br><span class="line">tst_user_item_feats_df_rank_model[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;tst_lgb_cls_feats.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测结果重新排序, 及生成提交结果</span></span><br><span class="line">rank_results = tst_user_item_feats_df_rank_model[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]]</span><br><span class="line">rank_results[<span class="string">&#x27;click_article_id&#x27;</span>] = rank_results[<span class="string">&#x27;click_article_id&#x27;</span>].astype(<span class="built_in">int</span>)</span><br><span class="line">submit(rank_results, topk=<span class="number">5</span>, model_name=<span class="string">&#x27;lgb_cls&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="DIN模型"><a href="#DIN模型" class="headerlink" title="DIN模型"></a>DIN模型</h2><h3 id="用户的历史点击行为列表"><a href="#用户的历史点击行为列表" class="headerlink" title="用户的历史点击行为列表"></a>用户的历史点击行为列表</h3><p>这个是为后面的DIN模型服务的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    all_data = pd.read_csv(<span class="string">&#x27;./data_raw/train_click_log.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trn_data = pd.read_csv(<span class="string">&#x27;./data_raw/train_click_log.csv&#x27;</span>)</span><br><span class="line">    tst_data = pd.read_csv(<span class="string">&#x27;./data_raw/testA_click_log.csv&#x27;</span>)</span><br><span class="line">    all_data = trn_data.append(tst_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hist_click =all_data[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>]].groupby(<span class="string">&#x27;user_id&#x27;</span>).agg(&#123;<span class="built_in">list</span>&#125;).reset_index()</span><br><span class="line">his_behavior_df = pd.DataFrame()</span><br><span class="line">his_behavior_df[<span class="string">&#x27;user_id&#x27;</span>] = hist_click[<span class="string">&#x27;user_id&#x27;</span>]</span><br><span class="line">his_behavior_df[<span class="string">&#x27;hist_click_article_id&#x27;</span>] = hist_click[<span class="string">&#x27;click_article_id&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_user_item_feats_df_din_model = trn_user_item_feats_df.copy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    val_user_item_feats_df_din_model = val_user_item_feats_df.copy()</span><br><span class="line"><span class="keyword">else</span>: </span><br><span class="line">    val_user_item_feats_df_din_model = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">tst_user_item_feats_df_din_model = tst_user_item_feats_df.copy()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trn_user_item_feats_df_din_model = trn_user_item_feats_df_din_model.merge(his_behavior_df, on=<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    val_user_item_feats_df_din_model = val_user_item_feats_df_din_model.merge(his_behavior_df, on=<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    val_user_item_feats_df_din_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">tst_user_item_feats_df_din_model = tst_user_item_feats_df_din_model.merge(his_behavior_df, on=<span class="string">&#x27;user_id&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="DIN模型简介"><a href="#DIN模型简介" class="headerlink" title="DIN模型简介"></a>DIN模型简介</h3><p>我们下面尝试使用DIN模型， DIN的全称是Deep Interest Network， 这是阿里2018年基于前面的深度学习模型无法表达用户多样化的兴趣而提出的一个模型， 它可以通过考虑【给定的候选广告】和【用户的历史行为】的相关性，来计算用户兴趣的表示向量。具体来说就是通过引入局部激活单元，通过软搜索历史行为的相关部分来关注相关的用户兴趣，并采用加权和来获得有关候选广告的用户兴趣的表示。与候选广告相关性较高的行为会获得较高的激活权重，并支配着用户兴趣。该表示向量在不同广告上有所不同，大大提高了模型的表达能力。所以该模型对于此次新闻推荐的任务也比较适合， 我们在这里通过当前的候选文章与用户历史点击文章的相关性来计算用户对于文章的兴趣。 该模型的结构如下：</p>
<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/abc/image-20201116201646983.png" alt="image-20201116201646983"></p>
<p>我们这里直接调包来使用这个模型， 关于这个模型的详细细节部分我们会在下一期的推荐系统组队学习中给出。下面说一下该模型如何具体使用：deepctr的函数原型如下：</p>
<blockquote>
<p>def DIN(dnn_feature_columns, history_feature_list, dnn_use_bn=False,<br>    dnn_hidden_units=(200, 80), dnn_activation=’relu’, att_hidden_size=(80, 40), att_activation=”dice”,<br>   att_weight_normalization=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, seed=1024,<br>    task=’binary’):</p>
<ul>
<li>dnn_feature_columns: 特征列， 包含数据所有特征的列表</li>
<li>history_feature_list: 用户历史行为列， 反应用户历史行为的特征的列表</li>
<li>dnn_use_bn: 是否使用BatchNormalization</li>
<li>dnn_hidden_units: 全连接层网络的层数和每一层神经元的个数， 一个列表或者元组</li>
<li>dnn_activation_relu: 全连接网络的激活单元类型</li>
<li>att_hidden_size: 注意力层的全连接网络的层数和每一层神经元的个数</li>
<li>att_activation: 注意力层的激活单元类型</li>
<li>att_weight_normalization: 是否归一化注意力得分</li>
<li>l2_reg_dnn: 全连接网络的正则化系数</li>
<li>l2_reg_embedding: embedding向量的正则化稀疏</li>
<li>dnn_dropout: 全连接网络的神经元的失活概率</li>
<li>task: 任务， 可以是分类， 也可是是回归</li>
</ul>
</blockquote>
<p>在具体使用的时候， 我们必须要传入特征列和历史行为列， 但是再传入之前， 我们需要进行一下特征列的预处理。具体如下：</p>
<ol>
<li>首先，我们要处理数据集， 得到数据， 由于我们是基于用户过去的行为去预测用户是否点击当前文章， 所以我们需要把数据的特征列划分成数值型特征， 离散型特征和历史行为特征列三部分， 对于每一部分， DIN模型的处理会有不同<ol>
<li>对于离散型特征， 在我们的数据集中就是那些类别型的特征， 比如user_id这种， 这种类别型特征， 我们首先要经过embedding处理得到每个特征的低维稠密型表示， 既然要经过embedding， 那么我们就需要为每一列的类别特征的取值建立一个字典，并指明embedding维度， 所以在使用deepctr的DIN模型准备数据的时候， 我们需要通过SparseFeat函数指明这些类别型特征, 这个函数的传入参数就是列名， 列的唯一取值(建立字典用)和embedding维度。</li>
<li>对于用户历史行为特征列， 比如文章id， 文章的类别等这种， 同样的我们需要先经过embedding处理， 只不过和上面不一样的地方是，对于这种特征， 我们在得到每个特征的embedding表示之后， 还需要通过一个Attention_layer计算用户的历史行为和当前候选文章的相关性以此得到当前用户的embedding向量， 这个向量就可以基于当前的候选文章与用户过去点击过得历史文章的相似性的程度来反应用户的兴趣， 并且随着用户的不同的历史点击来变化，去动态的模拟用户兴趣的变化过程。这类特征对于每个用户都是一个历史行为序列， 对于每个用户， 历史行为序列长度会不一样， 可能有的用户点击的历史文章多，有的点击的历史文章少， 所以我们还需要把这个长度统一起来， 在为DIN模型准备数据的时候， 我们首先要通过SparseFeat函数指明这些类别型特征， 然后还需要通过VarLenSparseFeat函数再进行序列填充， 使得每个用户的历史序列一样长， 所以这个函数参数中会有个maxlen，来指明序列的最大长度是多少。</li>
<li>对于连续型特征列， 我们只需要用DenseFeat函数来指明列名和维度即可。</li>
</ol>
</li>
<li>处理完特征列之后， 我们把相应的数据与列进行对应，就得到了最后的数据。</li>
</ol>
<p>下面根据具体的代码感受一下， 逻辑是这样， 首先我们需要写一个数据准备函数， 在这里面就是根据上面的具体步骤准备数据， 得到数据和特征列， 然后就是建立DIN模型并训练， 最后基于模型进行测试。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入deepctr</span></span><br><span class="line"><span class="keyword">from</span> deepctr.models <span class="keyword">import</span> DIN</span><br><span class="line"><span class="keyword">from</span> deepctr.feature_column <span class="keyword">import</span> SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> * </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_DEVICE_ORDER&quot;</span>] = <span class="string">&quot;PCI_BUS_ID&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;2&quot;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据准备函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_din_feats_columns</span>(<span class="params">df, dense_fea, sparse_fea, behavior_fea, his_behavior_fea, emb_dim=<span class="number">32</span>, max_len=<span class="number">100</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    数据准备函数:</span></span><br><span class="line"><span class="string">    df: 数据集</span></span><br><span class="line"><span class="string">    dense_fea: 数值型特征列</span></span><br><span class="line"><span class="string">    sparse_fea: 离散型特征列</span></span><br><span class="line"><span class="string">    behavior_fea: 用户的候选行为特征列</span></span><br><span class="line"><span class="string">    his_behavior_fea: 用户的历史行为特征列</span></span><br><span class="line"><span class="string">    embedding_dim: embedding的维度， 这里为了简单， 统一把离散型特征列采用一样的隐向量维度</span></span><br><span class="line"><span class="string">    max_len: 用户序列的最大长度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=df[feat].nunique() + <span class="number">1</span>, embedding_dim=emb_dim) <span class="keyword">for</span> feat <span class="keyword">in</span> sparse_fea]</span><br><span class="line">    </span><br><span class="line">    dense_feature_columns = [DenseFeat(feat, <span class="number">1</span>, ) <span class="keyword">for</span> feat <span class="keyword">in</span> dense_fea]</span><br><span class="line">    </span><br><span class="line">    var_feature_columns = [VarLenSparseFeat(SparseFeat(feat, vocabulary_size=df[<span class="string">&#x27;click_article_id&#x27;</span>].nunique() + <span class="number">1</span>,</span><br><span class="line">                                    embedding_dim=emb_dim, embedding_name=<span class="string">&#x27;click_article_id&#x27;</span>), maxlen=max_len) <span class="keyword">for</span> feat <span class="keyword">in</span> hist_behavior_fea]</span><br><span class="line">    </span><br><span class="line">    dnn_feature_columns = sparse_feature_columns + dense_feature_columns + var_feature_columns</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 建立x, x是一个字典的形式</span></span><br><span class="line">    x = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> get_feature_names(dnn_feature_columns):</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">in</span> his_behavior_fea:</span><br><span class="line">            <span class="comment"># 这是历史行为序列</span></span><br><span class="line">            his_list = [l <span class="keyword">for</span> l <span class="keyword">in</span> df[name]]</span><br><span class="line">            x[name] = pad_sequences(his_list, maxlen=max_len, padding=<span class="string">&#x27;post&#x27;</span>)      <span class="comment"># 二维数组</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x[name] = df[name].values</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x, dnn_feature_columns</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 把特征分开</span></span><br><span class="line">sparse_fea = [<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;category_id&#x27;</span>, <span class="string">&#x27;click_environment&#x27;</span>, <span class="string">&#x27;click_deviceGroup&#x27;</span>, </span><br><span class="line">              <span class="string">&#x27;click_os&#x27;</span>, <span class="string">&#x27;click_country&#x27;</span>, <span class="string">&#x27;click_region&#x27;</span>, <span class="string">&#x27;click_referrer_type&#x27;</span>, <span class="string">&#x27;is_cat_hab&#x27;</span>]</span><br><span class="line"></span><br><span class="line">behavior_fea = [<span class="string">&#x27;click_article_id&#x27;</span>]</span><br><span class="line"></span><br><span class="line">hist_behavior_fea = [<span class="string">&#x27;hist_click_article_id&#x27;</span>]</span><br><span class="line"></span><br><span class="line">dense_fea = [<span class="string">&#x27;sim0&#x27;</span>, <span class="string">&#x27;time_diff0&#x27;</span>, <span class="string">&#x27;word_diff0&#x27;</span>, <span class="string">&#x27;sim_max&#x27;</span>, <span class="string">&#x27;sim_min&#x27;</span>, <span class="string">&#x27;sim_sum&#x27;</span>, <span class="string">&#x27;sim_mean&#x27;</span>, <span class="string">&#x27;score&#x27;</span>,</span><br><span class="line">             <span class="string">&#x27;rank&#x27;</span>,<span class="string">&#x27;click_size&#x27;</span>,<span class="string">&#x27;time_diff_mean&#x27;</span>,<span class="string">&#x27;active_level&#x27;</span>,<span class="string">&#x27;user_time_hob1&#x27;</span>,<span class="string">&#x27;user_time_hob2&#x27;</span>,</span><br><span class="line">             <span class="string">&#x27;words_hbo&#x27;</span>,<span class="string">&#x27;words_count&#x27;</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dense特征进行归一化, 神经网络训练都需要将数值进行归一化处理</span></span><br><span class="line">mm = MinMaxScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是做一些特殊处理，当在其他的地方出现无效值的时候，不处理无法进行归一化，刚开始可以先把他注释掉，在运行了下面的代码</span></span><br><span class="line"><span class="comment"># 之后如果发现报错，应该先去想办法处理如何不出现inf之类的值</span></span><br><span class="line"><span class="comment"># trn_user_item_feats_df_din_model.replace([np.inf, -np.inf], 0, inplace=True)</span></span><br><span class="line"><span class="comment"># tst_user_item_feats_df_din_model.replace([np.inf, -np.inf], 0, inplace=True)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> dense_fea:</span><br><span class="line">    trn_user_item_feats_df_din_model[feat] = mm.fit_transform(trn_user_item_feats_df_din_model[[feat]])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> val_user_item_feats_df_din_model <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        val_user_item_feats_df_din_model[feat] = mm.fit_transform(val_user_item_feats_df_din_model[[feat]])</span><br><span class="line">    </span><br><span class="line">    tst_user_item_feats_df_din_model[feat] = mm.fit_transform(tst_user_item_feats_df_din_model[[feat]])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 准备训练数据</span></span><br><span class="line">x_trn, dnn_feature_columns = get_din_feats_columns(trn_user_item_feats_df_din_model, dense_fea, </span><br><span class="line">                                               sparse_fea, behavior_fea, hist_behavior_fea, max_len=<span class="number">50</span>)</span><br><span class="line">y_trn = trn_user_item_feats_df_din_model[<span class="string">&#x27;label&#x27;</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    <span class="comment"># 准备验证数据</span></span><br><span class="line">    x_val, dnn_feature_columns = get_din_feats_columns(val_user_item_feats_df_din_model, dense_fea, </span><br><span class="line">                                                   sparse_fea, behavior_fea, hist_behavior_fea, max_len=<span class="number">50</span>)</span><br><span class="line">    y_val = val_user_item_feats_df_din_model[<span class="string">&#x27;label&#x27;</span>].values</span><br><span class="line">    </span><br><span class="line">dense_fea = [x <span class="keyword">for</span> x <span class="keyword">in</span> dense_fea <span class="keyword">if</span> x != <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">x_tst, dnn_feature_columns = get_din_feats_columns(tst_user_item_feats_df_din_model, dense_fea, </span><br><span class="line">                                               sparse_fea, behavior_fea, hist_behavior_fea, max_len=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:From /home/ryluo/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">model = DIN(dnn_feature_columns, behavior_fea)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型结构</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型编译</span></span><br><span class="line">model.<span class="built_in">compile</span>(<span class="string">&#x27;adam&#x27;</span>, <span class="string">&#x27;binary_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;binary_crossentropy&#x27;</span>, tf.keras.metrics.AUC()])</span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:From /home/ryluo/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/ryluo/anaconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:255: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
user_id (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_article_id (InputLayer)   [(None, 1)]          0                                            
__________________________________________________________________________________________________
category_id (InputLayer)        [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_environment (InputLayer)  [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_deviceGroup (InputLayer)  [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_os (InputLayer)           [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_country (InputLayer)      [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_region (InputLayer)       [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_referrer_type (InputLayer [(None, 1)]          0                                            
__________________________________________________________________________________________________
is_cat_hab (InputLayer)         [(None, 1)]          0                                            
__________________________________________________________________________________________________
sparse_emb_user_id (Embedding)  (None, 1, 32)        1600032     user_id[0][0]                    
__________________________________________________________________________________________________
sparse_seq_emb_hist_click_artic multiple             525664      click_article_id[0][0]           
                                                                 hist_click_article_id[0][0]      
                                                                 click_article_id[0][0]           
__________________________________________________________________________________________________
sparse_emb_category_id (Embeddi (None, 1, 32)        7776        category_id[0][0]                
__________________________________________________________________________________________________
sparse_emb_click_environment (E (None, 1, 32)        128         click_environment[0][0]          
__________________________________________________________________________________________________
sparse_emb_click_deviceGroup (E (None, 1, 32)        160         click_deviceGroup[0][0]          
__________________________________________________________________________________________________
sparse_emb_click_os (Embedding) (None, 1, 32)        288         click_os[0][0]                   
__________________________________________________________________________________________________
sparse_emb_click_country (Embed (None, 1, 32)        384         click_country[0][0]              
__________________________________________________________________________________________________
sparse_emb_click_region (Embedd (None, 1, 32)        928         click_region[0][0]               
__________________________________________________________________________________________________
sparse_emb_click_referrer_type  (None, 1, 32)        256         click_referrer_type[0][0]        
__________________________________________________________________________________________________
sparse_emb_is_cat_hab (Embeddin (None, 1, 32)        64          is_cat_hab[0][0]                 
__________________________________________________________________________________________________
no_mask (NoMask)                (None, 1, 32)        0           sparse_emb_user_id[0][0]         
                                                                 sparse_seq_emb_hist_click_article
                                                                 sparse_emb_category_id[0][0]     
                                                                 sparse_emb_click_environment[0][0
                                                                 sparse_emb_click_deviceGroup[0][0
                                                                 sparse_emb_click_os[0][0]        
                                                                 sparse_emb_click_country[0][0]   
                                                                 sparse_emb_click_region[0][0]    
                                                                 sparse_emb_click_referrer_type[0]
                                                                 sparse_emb_is_cat_hab[0][0]      
__________________________________________________________________________________________________
hist_click_article_id (InputLay [(None, 50)]         0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1, 320)       0           no_mask[0][0]                    
                                                                 no_mask[1][0]                    
                                                                 no_mask[2][0]                    
                                                                 no_mask[3][0]                    
                                                                 no_mask[4][0]                    
                                                                 no_mask[5][0]                    
                                                                 no_mask[6][0]                    
                                                                 no_mask[7][0]                    
                                                                 no_mask[8][0]                    
                                                                 no_mask[9][0]                    
__________________________________________________________________________________________________
no_mask_1 (NoMask)              (None, 1, 320)       0           concatenate[0][0]                
__________________________________________________________________________________________________
attention_sequence_pooling_laye (None, 1, 32)        13961       sparse_seq_emb_hist_click_article
                                                                 sparse_seq_emb_hist_click_article
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 1, 352)       0           no_mask_1[0][0]                  
                                                                 attention_sequence_pooling_layer[
__________________________________________________________________________________________________
sim0 (InputLayer)               [(None, 1)]          0                                            
__________________________________________________________________________________________________
time_diff0 (InputLayer)         [(None, 1)]          0                                            
__________________________________________________________________________________________________
word_diff0 (InputLayer)         [(None, 1)]          0                                            
__________________________________________________________________________________________________
sim_max (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
sim_min (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
sim_sum (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
sim_mean (InputLayer)           [(None, 1)]          0                                            
__________________________________________________________________________________________________
score (InputLayer)              [(None, 1)]          0                                            
__________________________________________________________________________________________________
rank (InputLayer)               [(None, 1)]          0                                            
__________________________________________________________________________________________________
click_size (InputLayer)         [(None, 1)]          0                                            
__________________________________________________________________________________________________
time_diff_mean (InputLayer)     [(None, 1)]          0                                            
__________________________________________________________________________________________________
active_level (InputLayer)       [(None, 1)]          0                                            
__________________________________________________________________________________________________
user_time_hob1 (InputLayer)     [(None, 1)]          0                                            
__________________________________________________________________________________________________
user_time_hob2 (InputLayer)     [(None, 1)]          0                                            
__________________________________________________________________________________________________
words_hbo (InputLayer)          [(None, 1)]          0                                            
__________________________________________________________________________________________________
words_count (InputLayer)        [(None, 1)]          0                                            
__________________________________________________________________________________________________
flatten (Flatten)               (None, 352)          0           concatenate_1[0][0]              
__________________________________________________________________________________________________
no_mask_3 (NoMask)              (None, 1)            0           sim0[0][0]                       
                                                                 time_diff0[0][0]                 
                                                                 word_diff0[0][0]                 
                                                                 sim_max[0][0]                    
                                                                 sim_min[0][0]                    
                                                                 sim_sum[0][0]                    
                                                                 sim_mean[0][0]                   
                                                                 score[0][0]                      
                                                                 rank[0][0]                       
                                                                 click_size[0][0]                 
                                                                 time_diff_mean[0][0]             
                                                                 active_level[0][0]               
                                                                 user_time_hob1[0][0]             
                                                                 user_time_hob2[0][0]             
                                                                 words_hbo[0][0]                  
                                                                 words_count[0][0]                
__________________________________________________________________________________________________
no_mask_2 (NoMask)              (None, 352)          0           flatten[0][0]                    
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 16)           0           no_mask_3[0][0]                  
                                                                 no_mask_3[1][0]                  
                                                                 no_mask_3[2][0]                  
                                                                 no_mask_3[3][0]                  
                                                                 no_mask_3[4][0]                  
                                                                 no_mask_3[5][0]                  
                                                                 no_mask_3[6][0]                  
                                                                 no_mask_3[7][0]                  
                                                                 no_mask_3[8][0]                  
                                                                 no_mask_3[9][0]                  
                                                                 no_mask_3[10][0]                 
                                                                 no_mask_3[11][0]                 
                                                                 no_mask_3[12][0]                 
                                                                 no_mask_3[13][0]                 
                                                                 no_mask_3[14][0]                 
                                                                 no_mask_3[15][0]                 
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 352)          0           no_mask_2[0][0]                  
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 16)           0           concatenate_2[0][0]              
__________________________________________________________________________________________________
no_mask_4 (NoMask)              multiple             0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 368)          0           no_mask_4[0][0]                  
                                                                 no_mask_4[1][0]                  
__________________________________________________________________________________________________
dnn_1 (DNN)                     (None, 80)           89880       concatenate_3[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 1)            80          dnn_1[0][0]                      
__________________________________________________________________________________________________
prediction_layer (PredictionLay (None, 1)            1           dense[0][0]                      
==================================================================================================
Total params: 2,239,602
Trainable params: 2,239,362
Non-trainable params: 240
__________________________________________________________________________________________________</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="keyword">if</span> offline:</span><br><span class="line">    history = model.fit(x_trn, y_trn, verbose=<span class="number">1</span>, epochs=<span class="number">10</span>, validation_data=(x_val, y_val) , batch_size=<span class="number">256</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 也可以使用上面的语句用自己采样出来的验证集</span></span><br><span class="line">    <span class="comment"># history = model.fit(x_trn, y_trn, verbose=1, epochs=3, validation_split=0.3, batch_size=256)</span></span><br><span class="line">    history = model.fit(x_trn, y_trn, verbose=<span class="number">1</span>, epochs=<span class="number">2</span>, batch_size=<span class="number">256</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/2
290964/290964 [==============================] - 55s 189us/sample - loss: 0.4209 - binary_crossentropy: 0.4206 - auc: 0.7842
Epoch 2/2
290964/290964 [==============================] - 52s 178us/sample - loss: 0.3630 - binary_crossentropy: 0.3618 - auc: 0.8478</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">tst_user_item_feats_df_din_model[<span class="string">&#x27;pred_score&#x27;</span>] = model.predict(x_tst, verbose=<span class="number">1</span>, batch_size=<span class="number">256</span>)</span><br><span class="line">tst_user_item_feats_df_din_model[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;din_rank_score.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>500000/500000 [==============================] - 20s 39us/sample</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测结果重新排序, 及生成提交结果</span></span><br><span class="line">rank_results = tst_user_item_feats_df_din_model[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]]</span><br><span class="line">submit(rank_results, topk=<span class="number">5</span>, model_name=<span class="string">&#x27;din&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 五折交叉验证，这里的五折交叉是以用户为目标进行五折划分</span></span><br><span class="line"><span class="comment">#  这一部分与前面的单独训练和验证是分开的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_kfold_users</span>(<span class="params">trn_df, n=<span class="number">5</span></span>):</span></span><br><span class="line">    user_ids = trn_df[<span class="string">&#x27;user_id&#x27;</span>].unique()</span><br><span class="line">    user_set = [user_ids[i::n] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    <span class="keyword">return</span> user_set</span><br><span class="line"></span><br><span class="line">k_fold = <span class="number">5</span></span><br><span class="line">trn_df = trn_user_item_feats_df_din_model</span><br><span class="line">user_set = get_kfold_users(trn_df, n=k_fold)</span><br><span class="line"></span><br><span class="line">score_list = []</span><br><span class="line">score_df = trn_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]]</span><br><span class="line">sub_preds = np.zeros(tst_user_item_feats_df_rank_model.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">dense_fea = [x <span class="keyword">for</span> x <span class="keyword">in</span> dense_fea <span class="keyword">if</span> x != <span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">x_tst, dnn_feature_columns = get_din_feats_columns(tst_user_item_feats_df_din_model, dense_fea, </span><br><span class="line">                                                   sparse_fea, behavior_fea, hist_behavior_fea, max_len=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 五折交叉验证，并将中间结果保存用于staking</span></span><br><span class="line"><span class="keyword">for</span> n_fold, valid_user <span class="keyword">in</span> <span class="built_in">enumerate</span>(user_set):</span><br><span class="line">    train_idx = trn_df[~trn_df[<span class="string">&#x27;user_id&#x27;</span>].isin(valid_user)] <span class="comment"># add slide user</span></span><br><span class="line">    valid_idx = trn_df[trn_df[<span class="string">&#x27;user_id&#x27;</span>].isin(valid_user)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 准备训练数据</span></span><br><span class="line">    x_trn, dnn_feature_columns = get_din_feats_columns(train_idx, dense_fea, </span><br><span class="line">                                                       sparse_fea, behavior_fea, hist_behavior_fea, max_len=<span class="number">50</span>)</span><br><span class="line">    y_trn = train_idx[<span class="string">&#x27;label&#x27;</span>].values</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 准备验证数据</span></span><br><span class="line">    x_val, dnn_feature_columns = get_din_feats_columns(valid_idx, dense_fea, </span><br><span class="line">                                                   sparse_fea, behavior_fea, hist_behavior_fea, max_len=<span class="number">50</span>)</span><br><span class="line">    y_val = valid_idx[<span class="string">&#x27;label&#x27;</span>].values</span><br><span class="line">    </span><br><span class="line">    history = model.fit(x_trn, y_trn, verbose=<span class="number">1</span>, epochs=<span class="number">2</span>, validation_data=(x_val, y_val) , batch_size=<span class="number">256</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测验证集结果</span></span><br><span class="line">    valid_idx[<span class="string">&#x27;pred_score&#x27;</span>] = model.predict(x_val, verbose=<span class="number">1</span>, batch_size=<span class="number">256</span>)   </span><br><span class="line">    </span><br><span class="line">    valid_idx.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">    valid_idx[<span class="string">&#x27;pred_rank&#x27;</span>] = valid_idx.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将验证集的预测结果放到一个列表中，后面进行拼接</span></span><br><span class="line">    score_list.append(valid_idx[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是线上测试，需要计算每次交叉验证的结果相加，最后求平均</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> offline:</span><br><span class="line">        sub_preds += model.predict(x_tst, verbose=<span class="number">1</span>, batch_size=<span class="number">256</span>)[:, <span class="number">0</span>]   </span><br><span class="line">    </span><br><span class="line">score_df_ = pd.concat(score_list, axis=<span class="number">0</span>)</span><br><span class="line">score_df = score_df.merge(score_df_, how=<span class="string">&#x27;left&#x27;</span>, on=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>])</span><br><span class="line"><span class="comment"># 保存训练集交叉验证产生的新特征</span></span><br><span class="line">score_df[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;trn_din_cls_feats.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试集的预测结果，多次交叉验证求平均,将预测的score和对应的rank特征保存，可以用于后面的staking，这里还可以构造其他更多的特征</span></span><br><span class="line">tst_user_item_feats_df_din_model[<span class="string">&#x27;pred_score&#x27;</span>] = sub_preds / k_fold</span><br><span class="line">tst_user_item_feats_df_din_model[<span class="string">&#x27;pred_score&#x27;</span>] = tst_user_item_feats_df_din_model[<span class="string">&#x27;pred_score&#x27;</span>].transform(<span class="keyword">lambda</span> x: norm_sim(x))</span><br><span class="line">tst_user_item_feats_df_din_model.sort_values(by=[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>])</span><br><span class="line">tst_user_item_feats_df_din_model[<span class="string">&#x27;pred_rank&#x27;</span>] = tst_user_item_feats_df_din_model.groupby([<span class="string">&#x27;user_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].rank(ascending=<span class="literal">False</span>, method=<span class="string">&#x27;first&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存测试集交叉验证的新特征</span></span><br><span class="line">tst_user_item_feats_df_din_model[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]].to_csv(save_path + <span class="string">&#x27;tst_din_cls_feats.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>



<h1 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h1><h2 id="加权融合"><a href="#加权融合" class="headerlink" title="加权融合"></a>加权融合</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取多个模型的排序结果文件</span></span><br><span class="line">lgb_ranker = pd.read_csv(save_path + <span class="string">&#x27;lgb_ranker_score.csv&#x27;</span>)</span><br><span class="line">lgb_cls = pd.read_csv(save_path + <span class="string">&#x27;lgb_cls_score.csv&#x27;</span>)</span><br><span class="line">din_ranker = pd.read_csv(save_path + <span class="string">&#x27;din_rank_score.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里也可以换成交叉验证输出的测试结果进行加权融合</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rank_model = &#123;<span class="string">&#x27;lgb_ranker&#x27;</span>: lgb_ranker, </span><br><span class="line">              <span class="string">&#x27;lgb_cls&#x27;</span>: lgb_cls, </span><br><span class="line">              <span class="string">&#x27;din_ranker&#x27;</span>: din_ranker&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ensumble_predict_topk</span>(<span class="params">rank_model, topk=<span class="number">5</span></span>):</span></span><br><span class="line">    final_recall = rank_model[<span class="string">&#x27;lgb_cls&#x27;</span>].append(rank_model[<span class="string">&#x27;din_ranker&#x27;</span>])</span><br><span class="line">    rank_model[<span class="string">&#x27;lgb_ranker&#x27;</span>][<span class="string">&#x27;pred_score&#x27;</span>] = rank_model[<span class="string">&#x27;lgb_ranker&#x27;</span>][<span class="string">&#x27;pred_score&#x27;</span>].transform(<span class="keyword">lambda</span> x: norm_sim(x))</span><br><span class="line">    </span><br><span class="line">    final_recall = final_recall.append(rank_model[<span class="string">&#x27;lgb_ranker&#x27;</span>])</span><br><span class="line">    final_recall = final_recall.groupby([<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>])[<span class="string">&#x27;pred_score&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line">    </span><br><span class="line">    submit(final_recall, topk=topk, model_name=<span class="string">&#x27;ensemble_fuse&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_ensumble_predict_topk(rank_model)</span><br></pre></td></tr></table></figure>

<h2 id="Staking"><a href="#Staking" class="headerlink" title="Staking"></a>Staking</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取多个模型的交叉验证生成的结果文件</span></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">trn_lgb_ranker_feats = pd.read_csv(save_path + <span class="string">&#x27;trn_lgb_ranker_feats.csv&#x27;</span>)</span><br><span class="line">trn_lgb_cls_feats = pd.read_csv(save_path + <span class="string">&#x27;trn_lgb_cls_feats.csv&#x27;</span>)</span><br><span class="line">trn_din_cls_feats = pd.read_csv(save_path + <span class="string">&#x27;trn_din_cls_feats.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">tst_lgb_ranker_feats = pd.read_csv(save_path + <span class="string">&#x27;tst_lgb_ranker_feats.csv&#x27;</span>)</span><br><span class="line">tst_lgb_cls_feats = pd.read_csv(save_path + <span class="string">&#x27;tst_lgb_cls_feats.csv&#x27;</span>)</span><br><span class="line">tst_din_cls_feats = pd.read_csv(save_path + <span class="string">&#x27;tst_din_cls_feats.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将多个模型输出的特征进行拼接</span></span><br><span class="line"></span><br><span class="line">finall_trn_ranker_feats = trn_lgb_ranker_feats[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]]</span><br><span class="line">finall_tst_ranker_feats = tst_lgb_ranker_feats[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, trn_model <span class="keyword">in</span> <span class="built_in">enumerate</span>([trn_lgb_ranker_feats, trn_lgb_cls_feats, trn_din_cls_feats]):</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> [ <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]:</span><br><span class="line">        col_name = feat + <span class="string">&#x27;_&#x27;</span> + <span class="built_in">str</span>(idx)</span><br><span class="line">        finall_trn_ranker_feats[col_name] = trn_model[feat]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, tst_model <span class="keyword">in</span> <span class="built_in">enumerate</span>([tst_lgb_ranker_feats, tst_lgb_cls_feats, tst_din_cls_feats]):</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> [ <span class="string">&#x27;pred_score&#x27;</span>, <span class="string">&#x27;pred_rank&#x27;</span>]:</span><br><span class="line">        col_name = feat + <span class="string">&#x27;_&#x27;</span> + <span class="built_in">str</span>(idx)</span><br><span class="line">        finall_tst_ranker_feats[col_name] = tst_model[feat]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个逻辑回归模型再次拟合交叉验证产生的特征对测试集进行预测</span></span><br><span class="line"><span class="comment"># 这里需要注意的是，在做交叉验证的时候可以构造多一些与输出预测值相关的特征，来丰富这里简单模型的特征</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">feat_cols = [<span class="string">&#x27;pred_score_0&#x27;</span>, <span class="string">&#x27;pred_rank_0&#x27;</span>, <span class="string">&#x27;pred_score_1&#x27;</span>, <span class="string">&#x27;pred_rank_1&#x27;</span>, <span class="string">&#x27;pred_score_2&#x27;</span>, <span class="string">&#x27;pred_rank_2&#x27;</span>]</span><br><span class="line"></span><br><span class="line">trn_x = finall_trn_ranker_feats[feat_cols]</span><br><span class="line">trn_y = finall_trn_ranker_feats[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line">tst_x = finall_tst_ranker_feats[feat_cols]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">lr.fit(trn_x, trn_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">finall_tst_ranker_feats[<span class="string">&#x27;pred_score&#x27;</span>] = lr.predict_proba(tst_x)[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测结果重新排序, 及生成提交结果</span></span><br><span class="line">rank_results = finall_tst_ranker_feats[[<span class="string">&#x27;user_id&#x27;</span>, <span class="string">&#x27;click_article_id&#x27;</span>, <span class="string">&#x27;pred_score&#x27;</span>]]</span><br><span class="line">submit(rank_results, topk=<span class="number">5</span>, model_name=<span class="string">&#x27;ensumble_staking&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本章主要学习了三个排序模型，包括LGB的Rank， LGB的Classifier还有深度学习的DIN模型， 当然，对于这三个模型的原理部分，我们并没有给出详细的介绍， 请大家课下自己探索原理，也欢迎大家把自己的探索与所学分享出来，我们一块学习和进步。最后，我们进行了简单的模型融合策略，包括简单的加权和Stacking。</p>
]]></content>
  </entry>
</search>
